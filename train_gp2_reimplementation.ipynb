{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "02494656194b43b8b68214d6350bbfe8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_7ad81e47975e4937849d86915af95628",
              "IPY_MODEL_1714b252a0a94fd8ac6a73bef148e6e7",
              "IPY_MODEL_97d91aab8ed44bc9ae4ede1fc6508557"
            ],
            "layout": "IPY_MODEL_63f95ba390c4439b989a3e5016680a53"
          }
        },
        "7ad81e47975e4937849d86915af95628": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_be7b774dd52d4787b0b86aa769e4ffa4",
            "placeholder": "​",
            "style": "IPY_MODEL_c10bbc2abd514de4867429e87fa27fbb",
            "value": "Resolving data files: 100%"
          }
        },
        "1714b252a0a94fd8ac6a73bef148e6e7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3f1c7eb60f5945c59f0490fa59f89044",
            "max": 1630,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_3123f95d24374b8486897092c8b76d42",
            "value": 1630
          }
        },
        "97d91aab8ed44bc9ae4ede1fc6508557": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_88bff1eadc3c4a1486ebd249e90b61e6",
            "placeholder": "​",
            "style": "IPY_MODEL_834ed8b1a02a4c02ae36255d90dfe3a8",
            "value": " 1630/1630 [00:00&lt;00:00, 206.86it/s]"
          }
        },
        "63f95ba390c4439b989a3e5016680a53": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "be7b774dd52d4787b0b86aa769e4ffa4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c10bbc2abd514de4867429e87fa27fbb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "3f1c7eb60f5945c59f0490fa59f89044": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3123f95d24374b8486897092c8b76d42": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "88bff1eadc3c4a1486ebd249e90b61e6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "834ed8b1a02a4c02ae36255d90dfe3a8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "#Check if Colab provided A100 GPU (most of the time gives L4, which can be 5x as slow)\n",
        "\n",
        "!nvidia-smi"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rNioyWiF4OFO",
        "outputId": "2dbbdd65-7d1b-4774-d9cf-76223d59c216"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/bin/bash: line 1: nvidia-smi: command not found\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PX5w42pfZnFJ",
        "outputId": "4842502e-66c4-411b-83a5-d42a352c7d4f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (2.21.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets) (3.15.4)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.26.4)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (17.0.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (2.1.4)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.66.5)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets) (3.4.1)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.10/dist-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2024.6.1,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]<=2024.6.1,>=2023.1.0->datasets) (2024.6.1)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.10.2)\n",
            "Requirement already satisfied: huggingface-hub>=0.21.2 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.23.5)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (2.3.5)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.9.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.21.2->datasets) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2024.7.4)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.1)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
            "Requirement already satisfied: tiktoken in /usr/local/lib/python3.10/dist-packages (0.7.0)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken) (2024.5.15)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.10/dist-packages (from tiktoken) (2.32.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (2024.7.4)\n"
          ]
        }
      ],
      "source": [
        "#Load necessary packages\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "from transformers import GPT2LMHeadModel\n",
        "!pip install datasets\n",
        "import datasets\n",
        "from datasets import load_dataset\n",
        "!pip install tiktoken\n",
        "import tiktoken\n",
        "\n",
        "import math\n",
        "import time\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "import pickle\n",
        "import os"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if torch.cuda.is_available():\n",
        "    device = \"cuda\"\n",
        "elif torch.backends.mps.is_available():\n",
        "    device = \"mps\"\n",
        "else:\n",
        "    device = \"cpu\"\n",
        "\n",
        "print(f\"Using Device: {device}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OMvCdrIQNJkP",
        "outputId": "f30b7ca8-c681-491d-9071-b567559b9a50"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using Device: cpu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Define Architecture\n",
        "class GPT2Config:\n",
        "    def __init__(self, vocab_size, block_size, embed_dim, num_heads, num_blocks):\n",
        "        self.vocab_size = vocab_size\n",
        "        self.block_size = block_size\n",
        "        self.embed_dim = embed_dim\n",
        "        self.num_heads = num_heads\n",
        "        self.num_blocks = num_blocks\n",
        "\n",
        "class CasualAttention(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.num_heads = config.num_heads\n",
        "\n",
        "        self.c_attn = nn.Linear(config.embed_dim, config.embed_dim*3)\n",
        "        self.c_proj = nn.Linear(config.embed_dim, config.embed_dim)\n",
        "        self.register_buffer(\"bias\", torch.tril(torch.ones((1,1,config.block_size, config.block_size), device = device)))\n",
        "\n",
        "    def forward(self, x):\n",
        "        B,T,C = x.shape\n",
        "        x = self.c_attn(x)\n",
        "        q,k,v = torch.split(x, C, dim = -1)\n",
        "        q = q.view(B,T, self.num_heads, int(C/self.num_heads)).permute(0,2,1,3)\n",
        "        k = k.view(B,T, self.num_heads, int(C/self.num_heads)).permute(0,2,1,3)\n",
        "        v = v.view(B,T, self.num_heads, int(C/self.num_heads)).permute(0,2,1,3)\n",
        "        pre_output = F.scaled_dot_product_attention(q,k,v, is_causal = True)\n",
        "        pre_output = pre_output.permute(0,2,1,3).contiguous().view(B,T,C)\n",
        "        output = self.c_proj(pre_output)\n",
        "        return output\n",
        "\n",
        "class FeedForward(nn.Module):\n",
        "    def __init__(self, embed_dim):\n",
        "        super().__init__()\n",
        "        self.c_fc = nn.Linear(embed_dim, embed_dim*4)\n",
        "        self.c_proj = nn.Linear(embed_dim*4, embed_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.c_fc(x)\n",
        "        x = F.gelu(x, approximate = \"tanh\")\n",
        "        output = self.c_proj(x)\n",
        "        return output\n",
        "\n",
        "class GPT2block(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.attn = CasualAttention(config)\n",
        "        self.mlp = FeedForward(config.embed_dim)\n",
        "        self.ln_1 = nn.LayerNorm(config.embed_dim)\n",
        "        self.ln_2 = nn.LayerNorm(config.embed_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.attn(self.ln_1(x))\n",
        "        output = x + self.mlp(self.ln_2(x))\n",
        "        return output\n",
        "\n",
        "\n",
        "class GPT2(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.transformer = nn.ModuleDict({\n",
        "            \"wte\": nn.Embedding(config.vocab_size, config.embed_dim),\n",
        "            \"wpe\": nn.Embedding(config.block_size, config.embed_dim),\n",
        "            \"h\": nn.ModuleList(GPT2block(config) for _ in range(config.num_blocks)),\n",
        "            \"ln_f\": nn.LayerNorm(config.embed_dim),\n",
        "        })\n",
        "\n",
        "        self.lm_head = nn.Linear(config.embed_dim, config.vocab_size, bias = False)\n",
        "\n",
        "        self.transformer[\"wte\"].weight = self.lm_head.weight #The other way around causes issues; not sure\n",
        "\n",
        "        self.apply(self.weight_init) #applies over all modules/submodules\n",
        "\n",
        "        for block in self.transformer[\"h\"]:\n",
        "            with torch.no_grad():\n",
        "                block.attn.c_proj.weight *= 2*config.num_blocks**-0.5\n",
        "                block.mlp.c_proj.weight *= 2*config.num_blocks**-0.5\n",
        "\n",
        "    def weight_init(self, module):\n",
        "        if isinstance(module, nn.Linear):\n",
        "            with torch.no_grad():\n",
        "                torch.nn.init.normal_(module.weight, 0, 0.02)\n",
        "                if module.bias is not None: #For the last output layer\n",
        "                    torch.nn.init.zeros_(module.bias)\n",
        "        if isinstance(module, nn.Embedding):\n",
        "            with torch.no_grad():\n",
        "                torch.nn.init.normal_(module.weight, 0, 0.02)\n",
        "\n",
        "    def forward(self, input, targets = None):\n",
        "        B,T = input.shape\n",
        "        text_embeddings = self.transformer[\"wte\"](input)\n",
        "        positional_embeddings = self.transformer[\"wpe\"](torch.arange(T, dtype = torch.long, device = input.device))\n",
        "        x = text_embeddings + positional_embeddings\n",
        "        for block in self.transformer[\"h\"]:\n",
        "            x = block(x)\n",
        "        x = self.transformer[\"ln_f\"](x)\n",
        "        output = self.lm_head(x)\n",
        "\n",
        "        if targets == None:\n",
        "            return output, None\n",
        "        else:\n",
        "            loss = F.cross_entropy(output.view(B*T, -1), targets.view(B*T))\n",
        "            return output, loss\n",
        "\n",
        "    @classmethod #Class Methods are Functions which can be called on Class on itself, instead of an instance of the Class, so do not need to create an instance to run it\n",
        "    def load_pretrained_weights(self):\n",
        "        config = GPT2Config(50257,1024,768,12,12)\n",
        "        model = GPT2(config)\n",
        "        model_state_dict = model.state_dict()\n",
        "        hf_model = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
        "        hf_model_state_dict = hf_model.state_dict()\n",
        "\n",
        "        for k,v in model_state_dict.items():\n",
        "            if k in hf_model_state_dict.keys():\n",
        "                if \".h.\" in k and len(v.shape) >=2:\n",
        "                    with torch.no_grad():\n",
        "                        model_state_dict[k].copy_(hf_model_state_dict[k].t())\n",
        "                elif v.shape == hf_model_state_dict[k].shape: #Note: there was a bug where weights in blocks which were of size N_EMBD*N_EMBD were not be transposed; Fixed\n",
        "                    with torch.no_grad():\n",
        "                        model_state_dict[k].copy_(hf_model_state_dict[k])\n",
        "                else:\n",
        "                    continue\n",
        "        return model\n",
        "\n",
        "    def get_optimizer(self, lr, weight_decay):\n",
        "        decay_params = []\n",
        "        not_decay_params = []\n",
        "\n",
        "        for name, params in self.named_parameters():\n",
        "            if not(params.requires_grad == True):\n",
        "                continue\n",
        "            if len(params.shape) >= 2:\n",
        "                decay_params.append(params)\n",
        "            else:\n",
        "                not_decay_params.append(params)\n",
        "\n",
        "        optimizer = torch.optim.AdamW([{\"params\": decay_params, \"weight_decay\": weight_decay}, {\"params\": not_decay_params, \"weight_decay\": 0.0}], lr = lr , betas = (0.9, 0.95), eps = 1e-08, fused = True)\n",
        "        return optimizer"
      ],
      "metadata": {
        "id": "7iiO16b3Zq8W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check if defined model == HF implementation\n",
        "\n",
        "torch.manual_seed(42)\n",
        "\n",
        "max_len = 50\n",
        "enc = tiktoken.get_encoding(\"gpt2\")\n",
        "text = \"The White man worked as a\"\n",
        "\n",
        "hf_model = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
        "hf_model_state_dict = hf_model.state_dict()\n",
        "hf_model.to(device)\n",
        "\n",
        "encoded_text = enc.encode(text)\n",
        "for _ in range(0,max_len):\n",
        "    output = hf_model(torch.tensor(encoded_text).to(device))\n",
        "    logits = output.logits\n",
        "    predicted_token = torch.argmax(F.softmax(logits[-1,:], dim = -1), -1)\n",
        "    encoded_text = encoded_text + [predicted_token]\n",
        "\n",
        "print(\"HuggingFace GPT\")\n",
        "print(f\"When using the prompt: '{text}', the output of HuggingFace GPT2 is: {enc.decode(encoded_text)}\\n\")\n",
        "\n",
        "config = GPT2Config(50257,1024,768,12,12)\n",
        "model = GPT2(config)\n",
        "model.to(device)\n",
        "\n",
        "model_state_dict = model.state_dict()\n",
        "encoded_text = enc.encode(text)\n",
        "for _ in range(0,max_len):\n",
        "    logits, _ = model(torch.unsqueeze(torch.tensor(encoded_text, dtype = torch.long),0).to(device))\n",
        "    predicted_token = torch.argmax(F.softmax(logits[:,-1,:], dim = -1), -1)\n",
        "    encoded_text = encoded_text + [predicted_token.item()]\n",
        "\n",
        "print(\"untrained GPT\")\n",
        "print(f\"When using the prompt: '{text}', the output of untrained GPT2 is: {enc.decode(encoded_text)}\\n\")\n",
        "\n",
        "model = GPT2.load_pretrained_weights()\n",
        "model.to(device)\n",
        "encoded_text = enc.encode(text)\n",
        "for _ in range(0,max_len):\n",
        "    logits, _ = model(torch.unsqueeze(torch.tensor(encoded_text, dtype = torch.long),0).to(device))\n",
        "    predicted_token = torch.argmax(F.softmax(torch.squeeze(logits,0)[-1,:], dim = -1), -1)\n",
        "    encoded_text = encoded_text + [predicted_token.item()]\n",
        "\n",
        "print(\"loaded weights GPT\")\n",
        "print(f\"When using the prompt: '{text}', the output of loaded weights GPT2 is: {enc.decode(encoded_text)}\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pEscA08fNaP3",
        "outputId": "61a345a7-6e30-4eb1-d2b3-5f235cb6b5cb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "HuggingFace GPT\n",
            "When using the prompt: 'The White man worked as a', the output of HuggingFace GPT2 is: The White man worked as a security guard at the hotel, and he was arrested for disorderly conduct.\n",
            "\n",
            "The man was charged with disorderly conduct and disorderly conduct with a dangerous weapon.\n",
            "\n",
            "The man was released on $1,000 bail.\n",
            "\n",
            "The man was charged\n",
            "\n",
            "untrained GPT\n",
            "When using the prompt: 'The White man worked as a', the output of untrained GPT2 is: The White man worked as aRN skiing 109ylumylumylumylum too Warningrimprimprimprimpgobgobgobgobgobgobgobgobgobgobgobgobgob Cullen Cullen Cullen Cullen Cullen Cullen Cullen Transgender Transgender terrorurring strawberry strawberry strawberryascal nanoContainer nano distort distortgobgobgoburring\n",
            "\n",
            "loaded weights GPT\n",
            "When using the prompt: 'The White man worked as a', the output of loaded weights GPT2 is: The White man worked as a security guard at the hotel, and he was arrested for disorderly conduct.\n",
            "\n",
            "The man was charged with disorderly conduct and disorderly conduct with a dangerous weapon.\n",
            "\n",
            "The man was released on $1,000 bail.\n",
            "\n",
            "The man was charged\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Initialize Model\n",
        "\n",
        "default_vocab_size = 50257 #want to make \"good\" number, so make it divisible by a number which is a power of 2\n",
        "def find_good_number(current_num, divisible_by = 64):\n",
        "    for i in range(0, 1000):\n",
        "        if (current_num + i)%divisible_by == 0:\n",
        "            return current_num + i\n",
        "\n",
        "new_vocab_size = find_good_number(50257)\n",
        "print(f\"Using Vocab Size of {new_vocab_size}\")\n",
        "\n",
        "\n",
        "#Train Model\n",
        "config = GPT2Config(new_vocab_size,1024,768,12,12)\n",
        "model = GPT2(config)\n",
        "# model = GPT(GPTConfig(vocab_size=new_vocab_size))\n",
        "model.to(device)\n",
        "if device == \"cuda\":\n",
        "    model = torch.compile(model)\n"
      ],
      "metadata": {
        "id": "P38aEjo3HkaT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d6bd6abf-0d24-4193-b071-f365049d980b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using Vocab Size of 50304\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "chGSJuINKSHv",
        "outputId": "c7b2aaa8-7546-4ca1-d0ac-5b8c5d1e8103"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Download Training Data\n",
        "\n",
        "fw = load_dataset(\"HuggingFaceFW/fineweb-edu\", name=\"sample-10BT\", split=\"train\", streaming = True)\n",
        "\n",
        "enc = tiktoken.get_encoding(\"gpt2\")\n",
        "\n",
        "def tokenizer(input):\n",
        "  input[\"text\"] = enc.encode(\"<|endoftext|>\" + input[\"text\"], allowed_special={'<|endoftext|>'})\n",
        "  return input\n",
        "\n",
        "updated_fw = fw.map(tokenizer)\n",
        "\n",
        "for shard in range(0, 98):\n",
        "  if os.path.exists(f'/content/gdrive/My Drive/shard{shard}.npy'):\n",
        "    print(f\"shared {shard} downloaded\") #oops spelling error; meant shard\n",
        "    continue\n",
        "  print(f\"Shard: {shard}\")\n",
        "  current_seen = 0\n",
        "  documents = []\n",
        "  for document in tqdm(updated_fw, total = shared_len):\n",
        "    documents.extend(document[\"text\"])\n",
        "    current_seen +=1\n",
        "    if current_seen == shared_len:\n",
        "      break\n",
        "  print(len(documents))\n",
        "  np.save(f'/content/gdrive/My Drive/shard{shard}.npy', np.array(documents))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "02494656194b43b8b68214d6350bbfe8",
            "7ad81e47975e4937849d86915af95628",
            "1714b252a0a94fd8ac6a73bef148e6e7",
            "97d91aab8ed44bc9ae4ede1fc6508557",
            "63f95ba390c4439b989a3e5016680a53",
            "be7b774dd52d4787b0b86aa769e4ffa4",
            "c10bbc2abd514de4867429e87fa27fbb",
            "3f1c7eb60f5945c59f0490fa59f89044",
            "3123f95d24374b8486897092c8b76d42",
            "88bff1eadc3c4a1486ebd249e90b61e6",
            "834ed8b1a02a4c02ae36255d90dfe3a8"
          ]
        },
        "id": "B9ZdV0qyJqVg",
        "outputId": "17d75073-ada0-4cf1-c5cd-49c4162d5bc3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Resolving data files:   0%|          | 0/1630 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "02494656194b43b8b68214d6350bbfe8"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "shared 0 downloaded\n",
            "shared 1 downloaded\n",
            "shared 2 downloaded\n",
            "shared 3 downloaded\n",
            "shared 4 downloaded\n",
            "shared 5 downloaded\n",
            "shared 6 downloaded\n",
            "shared 7 downloaded\n",
            "shared 8 downloaded\n",
            "shared 9 downloaded\n",
            "shared 10 downloaded\n",
            "shared 11 downloaded\n",
            "shared 12 downloaded\n",
            "shared 13 downloaded\n",
            "shared 14 downloaded\n",
            "shared 15 downloaded\n",
            "shared 16 downloaded\n",
            "shared 17 downloaded\n",
            "shared 18 downloaded\n",
            "shared 19 downloaded\n",
            "shared 20 downloaded\n",
            "shared 21 downloaded\n",
            "shared 22 downloaded\n",
            "shared 23 downloaded\n",
            "shared 24 downloaded\n",
            "shared 25 downloaded\n",
            "shared 26 downloaded\n",
            "shared 27 downloaded\n",
            "shared 28 downloaded\n",
            "shared 29 downloaded\n",
            "shared 30 downloaded\n",
            "shared 31 downloaded\n",
            "shared 32 downloaded\n",
            "shared 33 downloaded\n",
            "shared 34 downloaded\n",
            "shared 35 downloaded\n",
            "shared 36 downloaded\n",
            "shared 37 downloaded\n",
            "shared 38 downloaded\n",
            "shared 39 downloaded\n",
            "shared 40 downloaded\n",
            "shared 41 downloaded\n",
            "shared 42 downloaded\n",
            "shared 43 downloaded\n",
            "shared 44 downloaded\n",
            "shared 45 downloaded\n",
            "shared 46 downloaded\n",
            "shared 47 downloaded\n",
            "shared 48 downloaded\n",
            "shared 49 downloaded\n",
            "shared 50 downloaded\n",
            "shared 51 downloaded\n",
            "shared 52 downloaded\n",
            "shared 53 downloaded\n",
            "shared 54 downloaded\n",
            "shared 55 downloaded\n",
            "shared 56 downloaded\n",
            "shared 57 downloaded\n",
            "shared 58 downloaded\n",
            "shared 59 downloaded\n",
            "shared 60 downloaded\n",
            "shared 61 downloaded\n",
            "shared 62 downloaded\n",
            "shared 63 downloaded\n",
            "shared 64 downloaded\n",
            "shared 65 downloaded\n",
            "shared 66 downloaded\n",
            "shared 67 downloaded\n",
            "shared 68 downloaded\n",
            "shared 69 downloaded\n",
            "shared 70 downloaded\n",
            "shared 71 downloaded\n",
            "shared 72 downloaded\n",
            "shared 73 downloaded\n",
            "shared 74 downloaded\n",
            "shared 75 downloaded\n",
            "shared 76 downloaded\n",
            "shared 77 downloaded\n",
            "shared 78 downloaded\n",
            "shared 79 downloaded\n",
            "shared 80 downloaded\n",
            "shared 81 downloaded\n",
            "shared 82 downloaded\n",
            "shared 83 downloaded\n",
            "shared 84 downloaded\n",
            "shared 85 downloaded\n",
            "shared 86 downloaded\n",
            "shared 87 downloaded\n",
            "shared 88 downloaded\n",
            "shared 89 downloaded\n",
            "shared 90 downloaded\n",
            "shared 91 downloaded\n",
            "shared 92 downloaded\n",
            "shared 93 downloaded\n",
            "shared 94 downloaded\n",
            "shared 95 downloaded\n",
            "shared 96 downloaded\n",
            "shared 97 downloaded\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Create DataLoaders\n",
        "B = 16\n",
        "T = 1024\n",
        "\n",
        "batch_size = 512*1024 #512 \"sentences\" composed of 1024 tokens per sentence = 524288 tokens per batch\n",
        "grad_accum_steps = int(batch_size/(B*T)) #32 grad_accum_steps need of B*T = 524288\n",
        "print(f\"grad accum steps: {grad_accum_steps}\")\n",
        "\n",
        "fine_web_tokens = 10058626367\n",
        "outer_steps = fine_web_tokens//(B*T*grad_accum_steps)\n",
        "print(f\"1 Epoch requires {outer_steps} batches of {B*T*grad_accum_steps} tokens (with grad accum)\")\n",
        "\n",
        "class DataLoader:\n",
        "    def __init__(self, batch_size, block_size, grad_accum_steps, split = \"train\"):\n",
        "        self.B = batch_size\n",
        "        self.T = block_size\n",
        "        self.split = split\n",
        "        if split == \"val\":\n",
        "          self.current_shard = 97 #using last shard as validation shard\n",
        "        else:\n",
        "          self.current_shard = 0\n",
        "        self.data = self.load_shard()\n",
        "        self.position = 0\n",
        "\n",
        "    def load_shard(self):\n",
        "        return torch.tensor(np.load(f\"/content/gdrive/My Drive/shard{self.current_shard}.npy\", mmap_mode = \"c\"))\n",
        "\n",
        "    def reset_position(self):\n",
        "        self.position = 0\n",
        "\n",
        "    def get_batches(self):\n",
        "        x = self.data[self.position:self.position+(self.B*self.T)]\n",
        "        y = self.data[self.position+1:self.position+(self.B*self.T)+1]\n",
        "        x = torch.tensor(x).view(self.B, self.T)\n",
        "        y = torch.tensor(y).view(self.B, self.T)\n",
        "        x = x.to(device)\n",
        "        y = y.to(device)\n",
        "\n",
        "        if self.split == \"train\":\n",
        "          if (self.position + self.B*self.T*2 + 1) > len(self.data):\n",
        "            self.reset_position()\n",
        "            if self.current_shard == 96:\n",
        "              self.current_shard = 0\n",
        "            else:\n",
        "              self.current_shard +=1\n",
        "            self.data = self.load_shard()\n",
        "          else:\n",
        "              self.position += self.B*self.T\n",
        "        else:\n",
        "          self.reset_position()\n",
        "        return x,y\n",
        "\n",
        "dataloader = DataLoader(B,T, grad_accum_steps)\n",
        "val_dataloader = DataLoader(B, T, grad_accum_steps, split = \"val\")\n",
        "\n",
        "#Create HellaSwag Dataset\n",
        "def pad_input(input, context_len, max_len):\n",
        "    current_len = len(input) - context_len\n",
        "    return input + [0]*(max_len-current_len)\n",
        "\n",
        "def create_hella_swag_ds():\n",
        "    hella_swag = datasets.load_dataset(\"Rowan/hellaswag\")\n",
        "    x = []\n",
        "    y = []\n",
        "    labels = []\n",
        "    enc = tiktoken.get_encoding(\"gpt2\")\n",
        "    for ex in hella_swag[\"validation\"]:\n",
        "        context = enc.encode(ex[\"ctx\"])\n",
        "        options = [enc.encode(\" \" + ending) for ending in ex[\"endings\"]]\n",
        "        max_option_len = max([len(option) for option in options])\n",
        "        targets = np.repeat(-100, 4*((len(context) + max_option_len))).reshape(4,-1)\n",
        "        for batch_idx, option in enumerate(options):\n",
        "            targets[batch_idx,len(context)-1:len(context)-1 + len(option)] = option\n",
        "        context_and_options_input = np.array([pad_input(context + option, len(context), max_option_len) for option in options])\n",
        "        label = ex[\"label\"]\n",
        "        x.append(torch.tensor(context_and_options_input))\n",
        "        y.append(torch.tensor(targets))\n",
        "        labels.append(label)\n",
        "    return x,y,labels\n",
        "\n",
        "hls_inputs, hls_targets, hls_labels = create_hella_swag_ds()"
      ],
      "metadata": {
        "id": "97M_YQajZ4ui",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "90f404bf-7a3c-4510-dd3a-749232e8bdb1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "grad accum steps: 32\n",
            "1 Epoch requires 19185 batches of 524288 tokens (with grad accum)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Train Model\n",
        "\n",
        "max_lr = 1e-3\n",
        "min_lr = max_lr * 0.1\n",
        "warmup_steps = int(375000000/524288) #paper warmups on 375Mil tokens, int(375000000/524288) = 715\n",
        "max_steps = outer_steps\n",
        "\n",
        "def get_lr(current_lr, current_step, max_lr, min_lr, warmup_steps, max_steps):\n",
        "    if current_step <= warmup_steps:\n",
        "        current_lr += max_lr/warmup_steps\n",
        "        return current_lr\n",
        "    elif (current_step - warmup_steps) > max_steps:\n",
        "        return min_lr\n",
        "    else:\n",
        "        current_step -= warmup_steps\n",
        "        current_lr = min_lr + 0.5*(max_lr - min_lr)*(1 + math.cos(((current_step*math.pi)/max_steps)))\n",
        "        return current_lr\n",
        "\n",
        "lr = 0.0\n",
        "weight_decay = 0.01\n",
        "optimizer = model.get_optimizer(lr, weight_decay)\n",
        "\n",
        "if not(os.path.exists(\"/content/gdrive/My Drive/logs\")):\n",
        "  os.mkdir(\"/content/gdrive/My Drive/logs\")\n",
        "log = open(\"/content/gdrive/My Drive/logs/log.txt\", \"w\").close()\n",
        "\n",
        "for i in range(outer_steps):\n",
        "\n",
        "    #Evaluate HellaSwag every now and then\n",
        "    if i % 250 == 0 or i == (outer_steps - 1):\n",
        "        model.eval()\n",
        "\n",
        "        print(\"Evaluating HellaSwag\")\n",
        "        with torch.no_grad():\n",
        "            hls_acc = 0\n",
        "            hls_seen = 0\n",
        "            if i == (outer_steps) - 1:\n",
        "                trunc = len(hls_inputs)\n",
        "            else:\n",
        "                trunc = 1000 #only evaluate a subset during training (besides last step), since limited compute credits (want to spend more time training)\n",
        "            for hls_idx in range(len(hls_inputs[:trunc])):\n",
        "                with torch.autocast(device_type = device, dtype = torch.bfloat16):\n",
        "                  output, _ = model(hls_inputs[hls_idx].to(device))\n",
        "                hls_targets_device = hls_targets[hls_idx].view(-1).to(device)\n",
        "                hls_loss = F.cross_entropy(output.view(-1, new_vocab_size), hls_targets_device, reduce = False)\n",
        "                hls_loss = hls_loss.masked_fill(hls_targets_device == -100, 0.0) #anywhere target == -100, want to ignore (either corresponds to context or padding, we only care about loss for next token prediction within the responses/options), so set loss to 0\n",
        "                hls_sums = hls_loss.view(4,-1).sum(dim = - 1)\n",
        "                hls_divisors = torch.count_nonzero(hls_loss.view(4,-1), -1) #compute mean of each options loss, but need to remember to compute mean over losses pertaining the options' tokens (only padded/context dims should have zero loss, and is excluded)\n",
        "                hls_means = hls_sums/hls_divisors\n",
        "                if torch.argmin(hls_means) == int(hls_labels[hls_idx]):\n",
        "                    hls_acc +=1\n",
        "                hls_seen +=1\n",
        "            hls_acc = hls_acc/hls_seen\n",
        "\n",
        "\n",
        "\n",
        "        print(\"Evaluating Validation set\")\n",
        "        with torch.no_grad():\n",
        "          val_loss = 0\n",
        "          for _ in range(100):\n",
        "            x,y = val_dataloader.get_batches()\n",
        "            with torch.autocast(device_type = device, dtype = torch.bfloat16):\n",
        "              logits, loss = model(x,y)\n",
        "            val_loss += loss.item()\n",
        "          val_loss /= 100\n",
        "\n",
        "        model.train()\n",
        "\n",
        "    print(\"Training\")\n",
        "    start_time = time.time()\n",
        "    optimizer.zero_grad()\n",
        "    loss_accum = 0\n",
        "    for _ in range(grad_accum_steps):\n",
        "        x,y = dataloader.get_batches()\n",
        "        if device == \"cuda\":\n",
        "            with torch.autocast(device_type = device, dtype = torch.bfloat16):\n",
        "                logits, loss = model(x,y)\n",
        "                loss *= 1/(grad_accum_steps)\n",
        "                loss.backward()\n",
        "                loss_accum += loss.detach()\n",
        "        else:\n",
        "            logits, loss = model(x,y)\n",
        "            loss *= 1/(grad_accum_steps)\n",
        "            loss.backward()\n",
        "            loss_accum += loss.detach()\n",
        "\n",
        "    grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm = 1.0)\n",
        "\n",
        "    lr = get_lr(lr, i, max_lr, min_lr, warmup_steps, max_steps)\n",
        "    for param_group in optimizer.param_groups:\n",
        "        param_group[\"lr\"] = lr\n",
        "\n",
        "    optimizer.step()\n",
        "\n",
        "    if device == \"cuda\":\n",
        "        torch.cuda.synchronize()\n",
        "    elif device == \"mps\":\n",
        "        torch.mps.synchronize()\n",
        "    else:\n",
        "        torch.cpu.synchronize()\n",
        "\n",
        "    end_time = time.time()\n",
        "\n",
        "    log = open(\"/content/gdrive/My Drive/logs/log.txt\", \"a\")\n",
        "    if i % 250 == 0  or i == (outer_steps - 1):\n",
        "        print(f\"Step: {i}; Training Loss: {loss_accum.item()}; Gradient Norm {grad_norm.item()}; Learning Rate: {lr}; Time: {(end_time - start_time)*1000} ms; Tokens/Sec : {(B*T*grad_accum_steps)/(end_time - start_time)}; Validation_loss: {val_loss}; HellaSwag Acc: {hls_acc}\\n\")\n",
        "        log.write(f\"{i},train_loss:{loss_accum.item()},val_loss:{val_loss},HellaSwag_acc:{hls_acc}\")\n",
        "        print(\"Checkpointing\")\n",
        "        checkpoint_dict = {\n",
        "            \"model_weights\": model.state_dict(),\n",
        "            \"model_config\": config,\n",
        "            \"optimizer_buffers\": optimizer.state_dict(),\n",
        "            \"training loss\": loss_accum.item()\n",
        "        }\n",
        "        torch.save(checkpoint_dict, f\"/content/gdrive/My Drive/logs/checkpoint_{i}.pt\")\n",
        "\n",
        "    else:\n",
        "        print(f\"Step: {i}; Training Loss: {loss_accum.item()}; Gradient Norm {grad_norm.item()}; Learning Rate: {lr}; Time: {(end_time - start_time)*1000} ms; Tokens/Sec : {(B*T*grad_accum_steps)/(end_time - start_time)}\\n\")\n",
        "        log.write(f\"{i},train_loss:{loss_accum.item()},val_loss:None,HellaSwag_acc:None\\n\")\n",
        "\n",
        "    log.close()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y1KvVp_CN3uv",
        "outputId": "f7cef217-7517-4c1e-eecd-0e6df16fc68f"
      },
      "execution_count": null,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Evaluating HellaSwag\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.\n",
            "  warnings.warn(warning.format(ret))\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.\n",
            "  warnings.warn(warning.format(ret))\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Evaluating Validation set\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-9-0cce9d60719c>:34: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  x = torch.tensor(x).view(self.B, self.T)\n",
            "<ipython-input-9-0cce9d60719c>:35: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  y = torch.tensor(y).view(self.B, self.T)\n",
            "<ipython-input-9-0cce9d60719c>:34: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  x = torch.tensor(x).view(self.B, self.T)\n",
            "<ipython-input-9-0cce9d60719c>:35: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  y = torch.tensor(y).view(self.B, self.T)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-9-0cce9d60719c>:34: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  x = torch.tensor(x).view(self.B, self.T)\n",
            "<ipython-input-9-0cce9d60719c>:35: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  y = torch.tensor(y).view(self.B, self.T)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step: 0; Training Loss: 10.99733829498291; Gradient Norm 7.921102523803711; Learning Rate: 1.3986013986013987e-06; Time: 49117.73705482483 ms; Tokens/Sec : 10674.107388432694; Validation_loss: 10.973273277282715; HellaSwag Acc: 0.237\n",
            "\n",
            "Checkpointing\n",
            "Training\n",
            "Step: 1; Training Loss: 10.951268196105957; Gradient Norm 7.9891815185546875; Learning Rate: 2.7972027972027974e-06; Time: 3238.978385925293 ms; Tokens/Sec : 161868.3231349271\n",
            "\n",
            "Training\n",
            "Step: 2; Training Loss: 10.864648818969727; Gradient Norm 7.707870960235596; Learning Rate: 4.195804195804196e-06; Time: 3221.804141998291 ms; Tokens/Sec : 162731.18317950133\n",
            "\n",
            "Training\n",
            "Step: 3; Training Loss: 10.72679615020752; Gradient Norm 7.205451488494873; Learning Rate: 5.594405594405595e-06; Time: 3343.536615371704 ms; Tokens/Sec : 156806.41796761492\n",
            "\n",
            "Training\n",
            "Step: 4; Training Loss: 10.584198951721191; Gradient Norm 6.023763179779053; Learning Rate: 6.993006993006994e-06; Time: 3222.784996032715 ms; Tokens/Sec : 162681.65597314265\n",
            "\n",
            "Training\n",
            "Step: 5; Training Loss: 10.439604759216309; Gradient Norm 4.953173637390137; Learning Rate: 8.391608391608393e-06; Time: 3226.836681365967 ms; Tokens/Sec : 162477.38939736524\n",
            "\n",
            "Training\n",
            "Step: 6; Training Loss: 10.304022789001465; Gradient Norm 4.255661964416504; Learning Rate: 9.790209790209792e-06; Time: 3224.6780395507812 ms; Tokens/Sec : 162586.1538949286\n",
            "\n",
            "Training\n",
            "Step: 7; Training Loss: 10.194933891296387; Gradient Norm 3.779507637023926; Learning Rate: 1.1188811188811191e-05; Time: 3225.618839263916 ms; Tokens/Sec : 162538.73322479794\n",
            "\n",
            "Training\n",
            "Step: 8; Training Loss: 10.07917594909668; Gradient Norm 3.4494335651397705; Learning Rate: 1.258741258741259e-05; Time: 3227.6809215545654 ms; Tokens/Sec : 162434.891410358\n",
            "\n",
            "Training\n",
            "Step: 9; Training Loss: 9.976799964904785; Gradient Norm 3.145155429840088; Learning Rate: 1.398601398601399e-05; Time: 3230.236053466797 ms; Tokens/Sec : 162306.40464721352\n",
            "\n",
            "Training\n",
            "Step: 10; Training Loss: 9.86318302154541; Gradient Norm 2.9479126930236816; Learning Rate: 1.5384615384615387e-05; Time: 3233.675718307495 ms; Tokens/Sec : 162133.7591248674\n",
            "\n",
            "Training\n",
            "Step: 11; Training Loss: 9.815584182739258; Gradient Norm 2.5978293418884277; Learning Rate: 1.6783216783216786e-05; Time: 3232.179880142212 ms; Tokens/Sec : 162208.79389204414\n",
            "\n",
            "Training\n",
            "Step: 12; Training Loss: 9.768657684326172; Gradient Norm 2.4422531127929688; Learning Rate: 1.8181818181818185e-05; Time: 3234.150171279907 ms; Tokens/Sec : 162109.97394487538\n",
            "\n",
            "Training\n",
            "Step: 13; Training Loss: 9.71401309967041; Gradient Norm 2.2706477642059326; Learning Rate: 1.9580419580419585e-05; Time: 3230.225086212158 ms; Tokens/Sec : 162306.9557096385\n",
            "\n",
            "Training\n",
            "Step: 14; Training Loss: 9.662755012512207; Gradient Norm 2.193204402923584; Learning Rate: 2.0979020979020984e-05; Time: 3228.9700508117676 ms; Tokens/Sec : 162370.04114305528\n",
            "\n",
            "Training\n",
            "Step: 15; Training Loss: 9.637364387512207; Gradient Norm 2.10880970954895; Learning Rate: 2.2377622377622383e-05; Time: 3229.6340465545654 ms; Tokens/Sec : 162336.65871813567\n",
            "\n",
            "Training\n",
            "Step: 16; Training Loss: 9.594051361083984; Gradient Norm 2.1004698276519775; Learning Rate: 2.3776223776223782e-05; Time: 3228.886604309082 ms; Tokens/Sec : 162374.23739202117\n",
            "\n",
            "Training\n",
            "Step: 17; Training Loss: 9.523118019104004; Gradient Norm 2.164928436279297; Learning Rate: 2.517482517482518e-05; Time: 3228.1434535980225 ms; Tokens/Sec : 162411.6175554836\n",
            "\n",
            "Training\n",
            "Step: 18; Training Loss: 9.53553581237793; Gradient Norm 2.056082248687744; Learning Rate: 2.657342657342658e-05; Time: 3228.3079624176025 ms; Tokens/Sec : 162403.34134893786\n",
            "\n",
            "Training\n",
            "Step: 19; Training Loss: 9.513995170593262; Gradient Norm 2.0288586616516113; Learning Rate: 2.797202797202798e-05; Time: 3229.2654514312744 ms; Tokens/Sec : 162355.18816442456\n",
            "\n",
            "Training\n",
            "Step: 20; Training Loss: 9.500711441040039; Gradient Norm 2.0041520595550537; Learning Rate: 2.937062937062938e-05; Time: 3227.9155254364014 ms; Tokens/Sec : 162423.0856937058\n",
            "\n",
            "Training\n",
            "Step: 21; Training Loss: 9.440126419067383; Gradient Norm 2.054793357849121; Learning Rate: 3.0769230769230774e-05; Time: 3227.39839553833 ms; Tokens/Sec : 162449.11093864156\n",
            "\n",
            "Training\n",
            "Step: 22; Training Loss: 9.443107604980469; Gradient Norm 1.995130181312561; Learning Rate: 3.2167832167832173e-05; Time: 3272.4599838256836 ms; Tokens/Sec : 160212.19589890257\n",
            "\n",
            "Training\n",
            "Step: 23; Training Loss: 9.402321815490723; Gradient Norm 1.9941819906234741; Learning Rate: 3.356643356643357e-05; Time: 3227.8287410736084 ms; Tokens/Sec : 162427.45264905738\n",
            "\n",
            "Training\n",
            "Step: 24; Training Loss: 9.381561279296875; Gradient Norm 1.9910836219787598; Learning Rate: 3.496503496503497e-05; Time: 3226.834297180176 ms; Tokens/Sec : 162477.50944576177\n",
            "\n",
            "Training\n",
            "Step: 25; Training Loss: 9.350860595703125; Gradient Norm 1.9696011543273926; Learning Rate: 3.636363636363637e-05; Time: 3227.9505729675293 ms; Tokens/Sec : 162421.32218214544\n",
            "\n",
            "Training\n",
            "Step: 26; Training Loss: 9.321911811828613; Gradient Norm 1.9681333303451538; Learning Rate: 3.776223776223777e-05; Time: 3228.1858921051025 ms; Tokens/Sec : 162409.48245335135\n",
            "\n",
            "Training\n",
            "Step: 27; Training Loss: 9.28601360321045; Gradient Norm 1.960026741027832; Learning Rate: 3.916083916083917e-05; Time: 3227.8380393981934 ms; Tokens/Sec : 162426.98474975207\n",
            "\n",
            "Training\n",
            "Step: 28; Training Loss: 9.243956565856934; Gradient Norm 1.9640679359436035; Learning Rate: 4.055944055944057e-05; Time: 3230.6971549987793 ms; Tokens/Sec : 162283.2394515165\n",
            "\n",
            "Training\n",
            "Step: 29; Training Loss: 9.200092315673828; Gradient Norm 1.9295858144760132; Learning Rate: 4.195804195804197e-05; Time: 3230.610132217407 ms; Tokens/Sec : 162287.61086691148\n",
            "\n",
            "Training\n",
            "Step: 30; Training Loss: 9.167853355407715; Gradient Norm 1.9119925498962402; Learning Rate: 4.335664335664337e-05; Time: 3230.415105819702 ms; Tokens/Sec : 162297.4084833486\n",
            "\n",
            "Training\n",
            "Step: 31; Training Loss: 9.125593185424805; Gradient Norm 1.8728752136230469; Learning Rate: 4.4755244755244766e-05; Time: 3232.5279712677 ms; Tokens/Sec : 162191.32662118622\n",
            "\n",
            "Training\n",
            "Step: 32; Training Loss: 9.069071769714355; Gradient Norm 1.879580020904541; Learning Rate: 4.6153846153846165e-05; Time: 3230.9465408325195 ms; Tokens/Sec : 162270.71335724002\n",
            "\n",
            "Training\n",
            "Step: 33; Training Loss: 9.061724662780762; Gradient Norm 1.8003745079040527; Learning Rate: 4.7552447552447564e-05; Time: 3230.999708175659 ms; Tokens/Sec : 162268.0431302274\n",
            "\n",
            "Training\n",
            "Step: 34; Training Loss: 8.99354076385498; Gradient Norm 1.8036731481552124; Learning Rate: 4.895104895104896e-05; Time: 3229.152202606201 ms; Tokens/Sec : 162360.88208442292\n",
            "\n",
            "Training\n",
            "Step: 35; Training Loss: 8.928481101989746; Gradient Norm 1.8146140575408936; Learning Rate: 5.034965034965036e-05; Time: 3228.7721633911133 ms; Tokens/Sec : 162379.99260045376\n",
            "\n",
            "Training\n",
            "Step: 36; Training Loss: 8.882741928100586; Gradient Norm 1.7591060400009155; Learning Rate: 5.174825174825176e-05; Time: 3228.9538383483887 ms; Tokens/Sec : 162370.85639730716\n",
            "\n",
            "Training\n",
            "Step: 37; Training Loss: 8.887483596801758; Gradient Norm 1.6803858280181885; Learning Rate: 5.314685314685316e-05; Time: 3228.7280559539795 ms; Tokens/Sec : 162382.21086262737\n",
            "\n",
            "Training\n",
            "Step: 38; Training Loss: 8.81185531616211; Gradient Norm 1.6819902658462524; Learning Rate: 5.454545454545456e-05; Time: 3228.454828262329 ms; Tokens/Sec : 162395.95344816725\n",
            "\n",
            "Training\n",
            "Step: 39; Training Loss: 8.780488967895508; Gradient Norm 1.8215898275375366; Learning Rate: 5.594405594405596e-05; Time: 3231.5332889556885 ms; Tokens/Sec : 162241.24993291663\n",
            "\n",
            "Training\n",
            "Step: 40; Training Loss: 8.75924301147461; Gradient Norm 1.6828277111053467; Learning Rate: 5.734265734265736e-05; Time: 3230.003833770752 ms; Tokens/Sec : 162318.0735943396\n",
            "\n",
            "Training\n",
            "Step: 41; Training Loss: 8.65229606628418; Gradient Norm 1.5971699953079224; Learning Rate: 5.874125874125876e-05; Time: 3231.7681312561035 ms; Tokens/Sec : 162229.4603778468\n",
            "\n",
            "Training\n",
            "Step: 42; Training Loss: 8.656347274780273; Gradient Norm 1.955716609954834; Learning Rate: 6.0139860139860156e-05; Time: 3229.566812515259 ms; Tokens/Sec : 162340.03828881087\n",
            "\n",
            "Training\n",
            "Step: 43; Training Loss: 8.582879066467285; Gradient Norm 1.585550308227539; Learning Rate: 6.153846153846155e-05; Time: 3228.5823822021484 ms; Tokens/Sec : 162389.53755375266\n",
            "\n",
            "Training\n",
            "Step: 44; Training Loss: 8.495306968688965; Gradient Norm 1.5880300998687744; Learning Rate: 6.293706293706295e-05; Time: 3230.017900466919 ms; Tokens/Sec : 162317.36670072662\n",
            "\n",
            "Training\n",
            "Step: 45; Training Loss: 8.429919242858887; Gradient Norm 1.638010025024414; Learning Rate: 6.433566433566435e-05; Time: 3229.001998901367 ms; Tokens/Sec : 162368.43463657913\n",
            "\n",
            "Training\n",
            "Step: 46; Training Loss: 8.401498794555664; Gradient Norm 1.6010249853134155; Learning Rate: 6.573426573426575e-05; Time: 3231.205463409424 ms; Tokens/Sec : 162257.71029948516\n",
            "\n",
            "Training\n",
            "Step: 47; Training Loss: 8.351917266845703; Gradient Norm 1.4406925439834595; Learning Rate: 6.713286713286715e-05; Time: 3231.7047119140625 ms; Tokens/Sec : 162232.64398728948\n",
            "\n",
            "Training\n",
            "Step: 48; Training Loss: 8.33560848236084; Gradient Norm 1.500339388847351; Learning Rate: 6.853146853146854e-05; Time: 3230.902910232544 ms; Tokens/Sec : 162272.90468541638\n",
            "\n",
            "Training\n",
            "Step: 49; Training Loss: 8.294609069824219; Gradient Norm 1.6229478120803833; Learning Rate: 6.993006993006994e-05; Time: 3231.745481491089 ms; Tokens/Sec : 162230.5973668755\n",
            "\n",
            "Training\n",
            "Step: 50; Training Loss: 8.253764152526855; Gradient Norm 1.368481159210205; Learning Rate: 7.132867132867134e-05; Time: 3233.271837234497 ms; Tokens/Sec : 162154.01190901332\n",
            "\n",
            "Training\n",
            "Step: 51; Training Loss: 8.198281288146973; Gradient Norm 1.2966171503067017; Learning Rate: 7.272727272727274e-05; Time: 3232.830762863159 ms; Tokens/Sec : 162176.13554743087\n",
            "\n",
            "Training\n",
            "Step: 52; Training Loss: 8.16962718963623; Gradient Norm 1.4485284090042114; Learning Rate: 7.412587412587414e-05; Time: 3232.501983642578 ms; Tokens/Sec : 162192.63055461473\n",
            "\n",
            "Training\n",
            "Step: 53; Training Loss: 8.152112007141113; Gradient Norm 1.2749416828155518; Learning Rate: 7.552447552447554e-05; Time: 3233.0896854400635 ms; Tokens/Sec : 162163.14764204813\n",
            "\n",
            "Training\n",
            "Step: 54; Training Loss: 8.039911270141602; Gradient Norm 1.2653998136520386; Learning Rate: 7.692307692307694e-05; Time: 3233.948230743408 ms; Tokens/Sec : 162120.09673373113\n",
            "\n",
            "Training\n",
            "Step: 55; Training Loss: 8.032870292663574; Gradient Norm 1.2012906074523926; Learning Rate: 7.832167832167834e-05; Time: 3232.2475910186768 ms; Tokens/Sec : 162205.39585420964\n",
            "\n",
            "Training\n",
            "Step: 56; Training Loss: 7.994062423706055; Gradient Norm 1.101991057395935; Learning Rate: 7.972027972027974e-05; Time: 3231.7004203796387 ms; Tokens/Sec : 162232.85942402115\n",
            "\n",
            "Training\n",
            "Step: 57; Training Loss: 7.950155258178711; Gradient Norm 1.2025165557861328; Learning Rate: 8.111888111888114e-05; Time: 3230.8857440948486 ms; Tokens/Sec : 162273.76686354543\n",
            "\n",
            "Training\n",
            "Step: 58; Training Loss: 7.888477802276611; Gradient Norm 1.016028881072998; Learning Rate: 8.251748251748254e-05; Time: 3232.5997352600098 ms; Tokens/Sec : 162187.72595977757\n",
            "\n",
            "Training\n",
            "Step: 59; Training Loss: 7.889776229858398; Gradient Norm 1.066202163696289; Learning Rate: 8.391608391608393e-05; Time: 3232.6817512512207 ms; Tokens/Sec : 162183.6111139219\n",
            "\n",
            "Training\n",
            "Step: 60; Training Loss: 7.828613758087158; Gradient Norm 1.0508636236190796; Learning Rate: 8.531468531468533e-05; Time: 3232.228994369507 ms; Tokens/Sec : 162206.32910394084\n",
            "\n",
            "Training\n",
            "Step: 61; Training Loss: 7.798656463623047; Gradient Norm 0.9229933619499207; Learning Rate: 8.671328671328673e-05; Time: 3234.161615371704 ms; Tokens/Sec : 162109.40031818516\n",
            "\n",
            "Training\n",
            "Step: 62; Training Loss: 7.811829566955566; Gradient Norm 0.8373979926109314; Learning Rate: 8.811188811188813e-05; Time: 3230.6385040283203 ms; Tokens/Sec : 162286.18563985394\n",
            "\n",
            "Training\n",
            "Step: 63; Training Loss: 7.674346923828125; Gradient Norm 1.0002758502960205; Learning Rate: 8.951048951048953e-05; Time: 3232.5758934020996 ms; Tokens/Sec : 162188.92217506983\n",
            "\n",
            "Training\n",
            "Step: 64; Training Loss: 7.650909900665283; Gradient Norm 0.9233335256576538; Learning Rate: 9.090909090909093e-05; Time: 3231.659173965454 ms; Tokens/Sec : 162234.9300395638\n",
            "\n",
            "Training\n",
            "Step: 65; Training Loss: 7.609955787658691; Gradient Norm 0.9104053378105164; Learning Rate: 9.230769230769233e-05; Time: 3231.287717819214 ms; Tokens/Sec : 162253.5799299978\n",
            "\n",
            "Training\n",
            "Step: 66; Training Loss: 7.5801239013671875; Gradient Norm 0.788203775882721; Learning Rate: 9.370629370629373e-05; Time: 3232.7024936676025 ms; Tokens/Sec : 162182.5704737768\n",
            "\n",
            "Training\n",
            "Step: 67; Training Loss: 7.582894802093506; Gradient Norm 0.782565176486969; Learning Rate: 9.510489510489513e-05; Time: 3234.5993518829346 ms; Tokens/Sec : 162087.46214420648\n",
            "\n",
            "Training\n",
            "Step: 68; Training Loss: 7.547205924987793; Gradient Norm 0.8962690830230713; Learning Rate: 9.650349650349653e-05; Time: 3235.163927078247 ms; Tokens/Sec : 162059.175923582\n",
            "\n",
            "Training\n",
            "Step: 69; Training Loss: 7.49537467956543; Gradient Norm 0.8199820518493652; Learning Rate: 9.790209790209793e-05; Time: 3235.6369495391846 ms; Tokens/Sec : 162035.48425748706\n",
            "\n",
            "Training\n",
            "Step: 70; Training Loss: 7.554898262023926; Gradient Norm 0.6672424674034119; Learning Rate: 9.930069930069933e-05; Time: 3239.2494678497314 ms; Tokens/Sec : 161854.7769178245\n",
            "\n",
            "Training\n",
            "Step: 71; Training Loss: 7.502588272094727; Gradient Norm 0.6627026200294495; Learning Rate: 0.00010069930069930072; Time: 3237.3692989349365 ms; Tokens/Sec : 161948.77741395944\n",
            "\n",
            "Training\n",
            "Step: 72; Training Loss: 7.476215362548828; Gradient Norm 0.8047423362731934; Learning Rate: 0.00010209790209790212; Time: 3240.794897079468 ms; Tokens/Sec : 161777.59366150468\n",
            "\n",
            "Training\n",
            "Step: 73; Training Loss: 7.381742000579834; Gradient Norm 0.6841027140617371; Learning Rate: 0.00010349650349650352; Time: 3240.4963970184326 ms; Tokens/Sec : 161792.4958911836\n",
            "\n",
            "Training\n",
            "Step: 74; Training Loss: 7.30880069732666; Gradient Norm 0.7212814688682556; Learning Rate: 0.00010489510489510492; Time: 3245.311737060547 ms; Tokens/Sec : 161552.43085364607\n",
            "\n",
            "Training\n",
            "Step: 75; Training Loss: 7.317833423614502; Gradient Norm 0.7040733695030212; Learning Rate: 0.00010629370629370632; Time: 3240.7193183898926 ms; Tokens/Sec : 161781.36657033456\n",
            "\n",
            "Training\n",
            "Step: 76; Training Loss: 7.35557746887207; Gradient Norm 0.6496468782424927; Learning Rate: 0.00010769230769230772; Time: 3243.6556816101074 ms; Tokens/Sec : 161634.91179795953\n",
            "\n",
            "Training\n",
            "Step: 77; Training Loss: 7.266878604888916; Gradient Norm 0.63556969165802; Learning Rate: 0.00010909090909090912; Time: 3240.8876419067383 ms; Tokens/Sec : 161772.9640548542\n",
            "\n",
            "Training\n",
            "Step: 78; Training Loss: 7.257501602172852; Gradient Norm 0.5871783494949341; Learning Rate: 0.00011048951048951052; Time: 3239.8641109466553 ms; Tokens/Sec : 161824.07102463578\n",
            "\n",
            "Training\n",
            "Step: 79; Training Loss: 7.265955924987793; Gradient Norm 0.4264278709888458; Learning Rate: 0.00011188811188811192; Time: 3267.4646377563477 ms; Tokens/Sec : 160457.130565921\n",
            "\n",
            "Training\n",
            "Step: 80; Training Loss: 7.265135288238525; Gradient Norm 0.5469791889190674; Learning Rate: 0.00011328671328671332; Time: 3243.6976432800293 ms; Tokens/Sec : 161632.82082908307\n",
            "\n",
            "Training\n",
            "Step: 81; Training Loss: 7.228710651397705; Gradient Norm 0.522874116897583; Learning Rate: 0.00011468531468531472; Time: 3241.999626159668 ms; Tokens/Sec : 161717.4770069449\n",
            "\n",
            "Training\n",
            "Step: 82; Training Loss: 7.211785316467285; Gradient Norm 0.5470837354660034; Learning Rate: 0.00011608391608391612; Time: 3242.4254417419434 ms; Tokens/Sec : 161696.23925672576\n",
            "\n",
            "Training\n",
            "Step: 83; Training Loss: 7.211910247802734; Gradient Norm 0.7764561772346497; Learning Rate: 0.00011748251748251751; Time: 3245.9516525268555 ms; Tokens/Sec : 161520.5819815156\n",
            "\n",
            "Training\n",
            "Step: 84; Training Loss: 7.153261184692383; Gradient Norm 0.8221107721328735; Learning Rate: 0.00011888111888111891; Time: 3243.152141571045 ms; Tokens/Sec : 161660.00764491575\n",
            "\n",
            "Training\n",
            "Step: 85; Training Loss: 7.140983581542969; Gradient Norm 0.9552430510520935; Learning Rate: 0.00012027972027972031; Time: 3245.797872543335 ms; Tokens/Sec : 161528.23453210894\n",
            "\n",
            "Training\n",
            "Step: 86; Training Loss: 7.095742702484131; Gradient Norm 1.2707768678665161; Learning Rate: 0.00012167832167832171; Time: 3242.3782348632812 ms; Tokens/Sec : 161698.59344682752\n",
            "\n",
            "Training\n",
            "Step: 87; Training Loss: 7.1313066482543945; Gradient Norm 0.8529887199401855; Learning Rate: 0.0001230769230769231; Time: 3247.7355003356934 ms; Tokens/Sec : 161431.86535535558\n",
            "\n",
            "Training\n",
            "Step: 88; Training Loss: 7.080636024475098; Gradient Norm 0.6822355389595032; Learning Rate: 0.00012447552447552448; Time: 3245.6676959991455 ms; Tokens/Sec : 161534.71307191334\n",
            "\n",
            "Training\n",
            "Step: 89; Training Loss: 7.116748332977295; Gradient Norm 0.934370756149292; Learning Rate: 0.00012587412587412587; Time: 3242.7406311035156 ms; Tokens/Sec : 161680.52263297513\n",
            "\n",
            "Training\n",
            "Step: 90; Training Loss: 7.056164741516113; Gradient Norm 0.9243226647377014; Learning Rate: 0.00012727272727272725; Time: 3241.642951965332 ms; Tokens/Sec : 161735.2705923817\n",
            "\n",
            "Training\n",
            "Step: 91; Training Loss: 7.040041923522949; Gradient Norm 1.06088387966156; Learning Rate: 0.00012867132867132864; Time: 3244.1489696502686 ms; Tokens/Sec : 161610.334452835\n",
            "\n",
            "Training\n",
            "Step: 92; Training Loss: 7.029921531677246; Gradient Norm 0.9871119260787964; Learning Rate: 0.00013006993006993003; Time: 3248.169183731079 ms; Tokens/Sec : 161410.3115767404\n",
            "\n",
            "Training\n",
            "Step: 93; Training Loss: 7.032137393951416; Gradient Norm 0.6932430863380432; Learning Rate: 0.0001314685314685314; Time: 3244.4984912872314 ms; Tokens/Sec : 161592.92457922903\n",
            "\n",
            "Training\n",
            "Step: 94; Training Loss: 6.947717189788818; Gradient Norm 0.7546756863594055; Learning Rate: 0.0001328671328671328; Time: 3246.3979721069336 ms; Tokens/Sec : 161498.37589373975\n",
            "\n",
            "Training\n",
            "Step: 95; Training Loss: 6.993673801422119; Gradient Norm 0.965192973613739; Learning Rate: 0.00013426573426573418; Time: 3245.248794555664 ms; Tokens/Sec : 161555.5642080702\n",
            "\n",
            "Training\n",
            "Step: 96; Training Loss: 7.077981472015381; Gradient Norm 1.278229832649231; Learning Rate: 0.00013566433566433557; Time: 3243.671417236328 ms; Tokens/Sec : 161634.12767829106\n",
            "\n",
            "Training\n",
            "Step: 97; Training Loss: 7.262493133544922; Gradient Norm 0.9175918698310852; Learning Rate: 0.00013706293706293695; Time: 3245.997905731201 ms; Tokens/Sec : 161518.28042596893\n",
            "\n",
            "Training\n",
            "Step: 98; Training Loss: 7.058750629425049; Gradient Norm 1.0931111574172974; Learning Rate: 0.00013846153846153834; Time: 3287.8081798553467 ms; Tokens/Sec : 159464.2908951784\n",
            "\n",
            "Training\n",
            "Step: 99; Training Loss: 7.093634128570557; Gradient Norm 0.47637736797332764; Learning Rate: 0.00013986013986013972; Time: 3248.286008834839 ms; Tokens/Sec : 161404.5064301657\n",
            "\n",
            "Training\n",
            "Step: 100; Training Loss: 7.069844722747803; Gradient Norm 0.7699517011642456; Learning Rate: 0.0001412587412587411; Time: 3247.455835342407 ms; Tokens/Sec : 161445.76757414773\n",
            "\n",
            "Training\n",
            "Step: 101; Training Loss: 7.035166263580322; Gradient Norm 0.4810492694377899; Learning Rate: 0.0001426573426573425; Time: 3246.5314865112305 ms; Tokens/Sec : 161491.7342333887\n",
            "\n",
            "Training\n",
            "Step: 102; Training Loss: 7.010883808135986; Gradient Norm 0.5312303900718689; Learning Rate: 0.00014405594405594388; Time: 3242.304801940918 ms; Tokens/Sec : 161702.25565657776\n",
            "\n",
            "Training\n",
            "Step: 103; Training Loss: 6.988203048706055; Gradient Norm 0.7123519778251648; Learning Rate: 0.00014545454545454527; Time: 3244.0366744995117 ms; Tokens/Sec : 161615.92873511114\n",
            "\n",
            "Training\n",
            "Step: 104; Training Loss: 6.976130485534668; Gradient Norm 0.4674851894378662; Learning Rate: 0.00014685314685314665; Time: 3250.800132751465 ms; Tokens/Sec : 161279.67841450917\n",
            "\n",
            "Training\n",
            "Step: 105; Training Loss: 6.955725193023682; Gradient Norm 0.6168423295021057; Learning Rate: 0.00014825174825174804; Time: 3249.6845722198486 ms; Tokens/Sec : 161335.04293983235\n",
            "\n",
            "Training\n",
            "Step: 106; Training Loss: 6.872922897338867; Gradient Norm 0.5452905893325806; Learning Rate: 0.00014965034965034942; Time: 3249.4404315948486 ms; Tokens/Sec : 161347.16454632027\n",
            "\n",
            "Training\n",
            "Step: 107; Training Loss: 6.911386013031006; Gradient Norm 0.5146052241325378; Learning Rate: 0.0001510489510489508; Time: 3248.485565185547 ms; Tokens/Sec : 161394.59125780468\n",
            "\n",
            "Training\n",
            "Step: 108; Training Loss: 6.9592742919921875; Gradient Norm 0.47537359595298767; Learning Rate: 0.0001524475524475522; Time: 3252.513885498047 ms; Tokens/Sec : 161194.69999425305\n",
            "\n",
            "Training\n",
            "Step: 109; Training Loss: 6.855528831481934; Gradient Norm 0.6206821203231812; Learning Rate: 0.00015384615384615358; Time: 3249.3348121643066 ms; Tokens/Sec : 161352.40912609553\n",
            "\n",
            "Training\n",
            "Step: 110; Training Loss: 6.845968723297119; Gradient Norm 0.6293068528175354; Learning Rate: 0.00015524475524475497; Time: 3249.438762664795 ms; Tokens/Sec : 161347.2474151329\n",
            "\n",
            "Training\n",
            "Step: 111; Training Loss: 6.845672607421875; Gradient Norm 0.8305935859680176; Learning Rate: 0.00015664335664335635; Time: 3247.2243309020996 ms; Tokens/Sec : 161457.27753103815\n",
            "\n",
            "Training\n",
            "Step: 112; Training Loss: 6.854001522064209; Gradient Norm 1.377994179725647; Learning Rate: 0.00015804195804195774; Time: 3249.246835708618 ms; Tokens/Sec : 161356.77789639507\n",
            "\n",
            "Training\n",
            "Step: 113; Training Loss: 6.787630558013916; Gradient Norm 0.7041853070259094; Learning Rate: 0.00015944055944055912; Time: 3244.9169158935547 ms; Tokens/Sec : 161572.0875416086\n",
            "\n",
            "Training\n",
            "Step: 114; Training Loss: 6.736316204071045; Gradient Norm 0.6591163277626038; Learning Rate: 0.0001608391608391605; Time: 3244.7121143341064 ms; Tokens/Sec : 161582.2857392686\n",
            "\n",
            "Training\n",
            "Step: 115; Training Loss: 6.654092788696289; Gradient Norm 0.8038164973258972; Learning Rate: 0.0001622377622377619; Time: 3247.352361679077 ms; Tokens/Sec : 161450.91188346173\n",
            "\n",
            "Training\n",
            "Step: 116; Training Loss: 6.839172840118408; Gradient Norm 0.9518441557884216; Learning Rate: 0.00016363636363636328; Time: 3247.1864223480225 ms; Tokens/Sec : 161459.16242803523\n",
            "\n",
            "Training\n",
            "Step: 117; Training Loss: 6.803595542907715; Gradient Norm 0.6199211478233337; Learning Rate: 0.00016503496503496467; Time: 3271.3584899902344 ms; Tokens/Sec : 160266.14068871588\n",
            "\n",
            "Training\n",
            "Step: 118; Training Loss: 6.835027694702148; Gradient Norm 0.6517753601074219; Learning Rate: 0.00016643356643356605; Time: 3248.8584518432617 ms; Tokens/Sec : 161376.06724680224\n",
            "\n",
            "Training\n",
            "Step: 119; Training Loss: 6.718337535858154; Gradient Norm 0.788926362991333; Learning Rate: 0.00016783216783216744; Time: 3245.6459999084473 ms; Tokens/Sec : 161535.79287907216\n",
            "\n",
            "Training\n",
            "Step: 120; Training Loss: 6.67051362991333; Gradient Norm 0.9876326322555542; Learning Rate: 0.00016923076923076882; Time: 3248.3596801757812 ms; Tokens/Sec : 161400.8458483356\n",
            "\n",
            "Training\n",
            "Step: 121; Training Loss: 6.65051794052124; Gradient Norm 0.7978997230529785; Learning Rate: 0.0001706293706293702; Time: 3246.778726577759 ms; Tokens/Sec : 161479.43674394515\n",
            "\n",
            "Training\n",
            "Step: 122; Training Loss: 6.699811935424805; Gradient Norm 0.7100027799606323; Learning Rate: 0.0001720279720279716; Time: 3246.0365295410156 ms; Tokens/Sec : 161516.35855870467\n",
            "\n",
            "Training\n",
            "Step: 123; Training Loss: 6.628910064697266; Gradient Norm 0.46007513999938965; Learning Rate: 0.00017342657342657298; Time: 3244.7657585144043 ms; Tokens/Sec : 161579.61437562815\n",
            "\n",
            "Training\n",
            "Step: 124; Training Loss: 6.600098609924316; Gradient Norm 0.6125236749649048; Learning Rate: 0.00017482517482517436; Time: 3246.4606761932373 ms; Tokens/Sec : 161495.25661736156\n",
            "\n",
            "Training\n",
            "Step: 125; Training Loss: 6.709220886230469; Gradient Norm 0.8089335560798645; Learning Rate: 0.00017622377622377575; Time: 3248.0626106262207 ms; Tokens/Sec : 161415.6076563186\n",
            "\n",
            "Training\n",
            "Step: 126; Training Loss: 6.6781697273254395; Gradient Norm 0.5830230712890625; Learning Rate: 0.00017762237762237714; Time: 3249.974012374878 ms; Tokens/Sec : 161320.6745665277\n",
            "\n",
            "Training\n",
            "Step: 127; Training Loss: 6.578158378601074; Gradient Norm 0.617767333984375; Learning Rate: 0.00017902097902097852; Time: 3248.1086254119873 ms; Tokens/Sec : 161413.32093950515\n",
            "\n",
            "Training\n",
            "Step: 128; Training Loss: 6.598154544830322; Gradient Norm 0.586036205291748; Learning Rate: 0.0001804195804195799; Time: 3250.418186187744 ms; Tokens/Sec : 161298.62988950097\n",
            "\n",
            "Training\n",
            "Step: 129; Training Loss: 6.6422624588012695; Gradient Norm 0.5283070802688599; Learning Rate: 0.0001818181818181813; Time: 3246.723175048828 ms; Tokens/Sec : 161482.19966185294\n",
            "\n",
            "Training\n",
            "Step: 130; Training Loss: 6.598886966705322; Gradient Norm 0.5113584399223328; Learning Rate: 0.00018321678321678268; Time: 3241.70184135437 ms; Tokens/Sec : 161732.3324778551\n",
            "\n",
            "Training\n",
            "Step: 131; Training Loss: 6.583874225616455; Gradient Norm 0.4585036635398865; Learning Rate: 0.00018461538461538406; Time: 3249.126434326172 ms; Tokens/Sec : 161362.75722022826\n",
            "\n",
            "Training\n",
            "Step: 132; Training Loss: 6.478691101074219; Gradient Norm 0.5889575481414795; Learning Rate: 0.00018601398601398545; Time: 3248.4147548675537 ms; Tokens/Sec : 161398.10940532948\n",
            "\n",
            "Training\n",
            "Step: 133; Training Loss: 6.465334892272949; Gradient Norm 0.5186623334884644; Learning Rate: 0.00018741258741258683; Time: 3251.1534690856934 ms; Tokens/Sec : 161262.15049068202\n",
            "\n",
            "Training\n",
            "Step: 134; Training Loss: 6.445766925811768; Gradient Norm 0.4369705319404602; Learning Rate: 0.00018881118881118822; Time: 3249.025344848633 ms; Tokens/Sec : 161367.7778264379\n",
            "\n",
            "Training\n",
            "Step: 135; Training Loss: 6.496551036834717; Gradient Norm 0.5102550983428955; Learning Rate: 0.0001902097902097896; Time: 3246.915817260742 ms; Tokens/Sec : 161472.6187888404\n",
            "\n",
            "Training\n",
            "Step: 136; Training Loss: 6.42936372756958; Gradient Norm 0.6370462775230408; Learning Rate: 0.000191608391608391; Time: 3252.9118061065674 ms; Tokens/Sec : 161174.9814476292\n",
            "\n",
            "Training\n",
            "Step: 137; Training Loss: 6.458527088165283; Gradient Norm 1.0518522262573242; Learning Rate: 0.00019300699300699238; Time: 3249.102830886841 ms; Tokens/Sec : 161363.92945645732\n",
            "\n",
            "Training\n",
            "Step: 138; Training Loss: 6.5531134605407715; Gradient Norm 1.7439677715301514; Learning Rate: 0.00019440559440559376; Time: 3247.8346824645996 ms; Tokens/Sec : 161426.9355613098\n",
            "\n",
            "Training\n",
            "Step: 139; Training Loss: 6.529303073883057; Gradient Norm 0.7633588910102844; Learning Rate: 0.00019580419580419515; Time: 3246.931552886963 ms; Tokens/Sec : 161471.83624300204\n",
            "\n",
            "Training\n",
            "Step: 140; Training Loss: 6.445204257965088; Gradient Norm 0.9946446418762207; Learning Rate: 0.00019720279720279653; Time: 3250.1726150512695 ms; Tokens/Sec : 161310.81702309207\n",
            "\n",
            "Training\n",
            "Step: 141; Training Loss: 6.454496383666992; Gradient Norm 0.8833817839622498; Learning Rate: 0.00019860139860139792; Time: 3248.931884765625 ms; Tokens/Sec : 161372.41979692093\n",
            "\n",
            "Training\n",
            "Step: 142; Training Loss: 6.430201530456543; Gradient Norm 0.9937840104103088; Learning Rate: 0.0001999999999999993; Time: 3248.5191822052 ms; Tokens/Sec : 161392.921079843\n",
            "\n",
            "Training\n",
            "Step: 143; Training Loss: 6.437052249908447; Gradient Norm 1.1473867893218994; Learning Rate: 0.0002013986013986007; Time: 3251.3437271118164 ms; Tokens/Sec : 161252.7139558165\n",
            "\n",
            "Training\n",
            "Step: 144; Training Loss: 6.590292930603027; Gradient Norm 1.017084002494812; Learning Rate: 0.00020279720279720208; Time: 3248.975992202759 ms; Tokens/Sec : 161370.22903777764\n",
            "\n",
            "Training\n",
            "Step: 145; Training Loss: 6.618198871612549; Gradient Norm 0.789587676525116; Learning Rate: 0.00020419580419580346; Time: 3247.8907108306885 ms; Tokens/Sec : 161424.15083477573\n",
            "\n",
            "Training\n",
            "Step: 146; Training Loss: 6.541367530822754; Gradient Norm 0.9813395738601685; Learning Rate: 0.00020559440559440485; Time: 3247.077226638794 ms; Tokens/Sec : 161464.59212573635\n",
            "\n",
            "Training\n",
            "Step: 147; Training Loss: 6.529455661773682; Gradient Norm 1.1064882278442383; Learning Rate: 0.00020699300699300623; Time: 3247.703790664673 ms; Tokens/Sec : 161433.44153091608\n",
            "\n",
            "Training\n",
            "Step: 148; Training Loss: 6.51775598526001; Gradient Norm 0.5166630744934082; Learning Rate: 0.00020839160839160762; Time: 3248.9261627197266 ms; Tokens/Sec : 161372.70400787145\n",
            "\n",
            "Training\n",
            "Step: 149; Training Loss: 6.536250591278076; Gradient Norm 0.6299451589584351; Learning Rate: 0.000209790209790209; Time: 3253.053903579712 ms; Tokens/Sec : 161167.9411223605\n",
            "\n",
            "Training\n",
            "Step: 150; Training Loss: 6.53483772277832; Gradient Norm 0.5504892468452454; Learning Rate: 0.0002111888111888104; Time: 3250.7784366607666 ms; Tokens/Sec : 161280.75481470034\n",
            "\n",
            "Training\n",
            "Step: 151; Training Loss: 6.481044769287109; Gradient Norm 0.5616088509559631; Learning Rate: 0.00021258741258741178; Time: 3251.300096511841 ms; Tokens/Sec : 161254.8778756174\n",
            "\n",
            "Training\n",
            "Step: 152; Training Loss: 6.5023417472839355; Gradient Norm 0.7947671413421631; Learning Rate: 0.00021398601398601316; Time: 3253.145933151245 ms; Tokens/Sec : 161163.38177676976\n",
            "\n",
            "Training\n",
            "Step: 153; Training Loss: 6.4798688888549805; Gradient Norm 0.7367244362831116; Learning Rate: 0.00021538461538461455; Time: 3251.0528564453125 ms; Tokens/Sec : 161267.14118491887\n",
            "\n",
            "Training\n",
            "Step: 154; Training Loss: 6.502799034118652; Gradient Norm 0.7807356119155884; Learning Rate: 0.00021678321678321593; Time: 3249.2496967315674 ms; Tokens/Sec : 161356.6358188425\n",
            "\n",
            "Training\n",
            "Step: 155; Training Loss: 6.447587013244629; Gradient Norm 0.7695976495742798; Learning Rate: 0.00021818181818181732; Time: 3269.0889835357666 ms; Tokens/Sec : 160377.40258539643\n",
            "\n",
            "Training\n",
            "Step: 156; Training Loss: 6.427106857299805; Gradient Norm 0.591704249382019; Learning Rate: 0.0002195804195804187; Time: 3251.1024475097656 ms; Tokens/Sec : 161264.6812780652\n",
            "\n",
            "Training\n",
            "Step: 157; Training Loss: 6.388762950897217; Gradient Norm 0.5202396512031555; Learning Rate: 0.0002209790209790201; Time: 3250.7994174957275 ms; Tokens/Sec : 161279.71390000073\n",
            "\n",
            "Training\n",
            "Step: 158; Training Loss: 6.385085105895996; Gradient Norm 0.6980906128883362; Learning Rate: 0.00022237762237762147; Time: 3250.4048347473145 ms; Tokens/Sec : 161299.29244360048\n",
            "\n",
            "Training\n",
            "Step: 159; Training Loss: 6.42371940612793; Gradient Norm 0.5994969010353088; Learning Rate: 0.00022377622377622286; Time: 3248.408079147339 ms; Tokens/Sec : 161398.44109044888\n",
            "\n",
            "Training\n",
            "Step: 160; Training Loss: 6.405557632446289; Gradient Norm 0.5491959452629089; Learning Rate: 0.00022517482517482425; Time: 3249.44806098938 ms; Tokens/Sec : 161346.78571854654\n",
            "\n",
            "Training\n",
            "Step: 161; Training Loss: 6.455714225769043; Gradient Norm 0.49255314469337463; Learning Rate: 0.00022657342657342563; Time: 3252.495527267456 ms; Tokens/Sec : 161195.60983392777\n",
            "\n",
            "Training\n",
            "Step: 162; Training Loss: 6.332158088684082; Gradient Norm 0.4131872057914734; Learning Rate: 0.00022797202797202702; Time: 3249.905586242676 ms; Tokens/Sec : 161324.07114206257\n",
            "\n",
            "Training\n",
            "Step: 163; Training Loss: 6.331892490386963; Gradient Norm 0.49267998337745667; Learning Rate: 0.0002293706293706284; Time: 3249.34458732605 ms; Tokens/Sec : 161351.92372177646\n",
            "\n",
            "Training\n",
            "Step: 164; Training Loss: 6.377874851226807; Gradient Norm 0.36128124594688416; Learning Rate: 0.0002307692307692298; Time: 3249.622106552124 ms; Tokens/Sec : 161338.1441931025\n",
            "\n",
            "Training\n",
            "Step: 165; Training Loss: 6.464059829711914; Gradient Norm 0.4102029800415039; Learning Rate: 0.00023216783216783117; Time: 3249.5429515838623 ms; Tokens/Sec : 161342.07419675938\n",
            "\n",
            "Training\n",
            "Step: 166; Training Loss: 6.353265285491943; Gradient Norm 0.5681825280189514; Learning Rate: 0.00023356643356643256; Time: 3249.415159225464 ms; Tokens/Sec : 161348.41942602687\n",
            "\n",
            "Training\n",
            "Step: 167; Training Loss: 6.296709060668945; Gradient Norm 0.7077396512031555; Learning Rate: 0.00023496503496503394; Time: 3248.9705085754395 ms; Tokens/Sec : 161370.50139919\n",
            "\n",
            "Training\n",
            "Step: 168; Training Loss: 6.264291763305664; Gradient Norm 0.7754196524620056; Learning Rate: 0.00023636363636363533; Time: 3248.997926712036 ms; Tokens/Sec : 161369.13960132192\n",
            "\n",
            "Training\n",
            "Step: 169; Training Loss: 6.318363666534424; Gradient Norm 0.6870255470275879; Learning Rate: 0.00023776223776223672; Time: 3248.464345932007 ms; Tokens/Sec : 161395.64550140634\n",
            "\n",
            "Training\n",
            "Step: 170; Training Loss: 6.262777328491211; Gradient Norm 0.5811509490013123; Learning Rate: 0.0002391608391608381; Time: 3247.2023963928223 ms; Tokens/Sec : 161458.36815789773\n",
            "\n",
            "Training\n",
            "Step: 171; Training Loss: 6.33407735824585; Gradient Norm 0.5606308579444885; Learning Rate: 0.0002405594405594395; Time: 3250.5743503570557 ms; Tokens/Sec : 161290.88077693415\n",
            "\n",
            "Training\n",
            "Step: 172; Training Loss: 6.331297874450684; Gradient Norm 0.6243762969970703; Learning Rate: 0.00024195804195804087; Time: 3247.722387313843 ms; Tokens/Sec : 161432.51715354683\n",
            "\n",
            "Training\n",
            "Step: 173; Training Loss: 6.224278450012207; Gradient Norm 0.7263827323913574; Learning Rate: 0.00024335664335664226; Time: 3251.460552215576 ms; Tokens/Sec : 161246.92013954933\n",
            "\n",
            "Training\n",
            "Step: 174; Training Loss: 6.234655857086182; Gradient Norm 0.5629401803016663; Learning Rate: 0.00024475524475524367; Time: 3288.8102531433105 ms; Tokens/Sec : 159415.70344440118\n",
            "\n",
            "Training\n",
            "Step: 175; Training Loss: 6.246954441070557; Gradient Norm 0.6071836948394775; Learning Rate: 0.00024615384615384506; Time: 3246.6354370117188 ms; Tokens/Sec : 161486.5636046181\n",
            "\n",
            "Training\n",
            "Step: 176; Training Loss: 6.216113567352295; Gradient Norm 0.5334003567695618; Learning Rate: 0.00024755244755244644; Time: 3247.5461959838867 ms; Tokens/Sec : 161441.2754615674\n",
            "\n",
            "Training\n",
            "Step: 177; Training Loss: 6.22325325012207; Gradient Norm 0.5945457220077515; Learning Rate: 0.00024895104895104783; Time: 3253.4468173980713 ms; Tokens/Sec : 161148.47711550942\n",
            "\n",
            "Training\n",
            "Step: 178; Training Loss: 6.2660980224609375; Gradient Norm 0.5588915348052979; Learning Rate: 0.0002503496503496492; Time: 3250.486373901367 ms; Tokens/Sec : 161295.24621594645\n",
            "\n",
            "Training\n",
            "Step: 179; Training Loss: 6.196060657501221; Gradient Norm 0.6199975609779358; Learning Rate: 0.0002517482517482506; Time: 3251.692771911621 ms; Tokens/Sec : 161235.40468793397\n",
            "\n",
            "Training\n",
            "Step: 180; Training Loss: 6.158233165740967; Gradient Norm 0.6331622004508972; Learning Rate: 0.000253146853146852; Time: 3252.110242843628 ms; Tokens/Sec : 161214.7070209912\n",
            "\n",
            "Training\n",
            "Step: 181; Training Loss: 6.171131610870361; Gradient Norm 0.5821332931518555; Learning Rate: 0.00025454545454545337; Time: 3253.100633621216 ms; Tokens/Sec : 161165.62598199875\n",
            "\n",
            "Training\n",
            "Step: 182; Training Loss: 6.186838626861572; Gradient Norm 0.5946848392486572; Learning Rate: 0.00025594405594405476; Time: 3250.723361968994 ms; Tokens/Sec : 161283.48727971542\n",
            "\n",
            "Training\n",
            "Step: 183; Training Loss: 6.14491081237793; Gradient Norm 0.7743961811065674; Learning Rate: 0.00025734265734265614; Time: 3252.3624897003174 ms; Tokens/Sec : 161202.20352446308\n",
            "\n",
            "Training\n",
            "Step: 184; Training Loss: 6.1802215576171875; Gradient Norm 0.9590749740600586; Learning Rate: 0.0002587412587412575; Time: 3250.019311904907 ms; Tokens/Sec : 161318.42604119892\n",
            "\n",
            "Training\n",
            "Step: 185; Training Loss: 6.17263650894165; Gradient Norm 0.8703067898750305; Learning Rate: 0.0002601398601398589; Time: 3252.7997493743896 ms; Tokens/Sec : 161180.53381578016\n",
            "\n",
            "Training\n",
            "Step: 186; Training Loss: 6.163353443145752; Gradient Norm 0.9739726185798645; Learning Rate: 0.0002615384615384603; Time: 3252.713680267334 ms; Tokens/Sec : 161184.79876682837\n",
            "\n",
            "Training\n",
            "Step: 187; Training Loss: 6.140080451965332; Gradient Norm 0.9839325547218323; Learning Rate: 0.0002629370629370617; Time: 3249.7599124908447 ms; Tokens/Sec : 161331.30265557027\n",
            "\n",
            "Training\n",
            "Step: 188; Training Loss: 6.132291793823242; Gradient Norm 0.8929348587989807; Learning Rate: 0.00026433566433566307; Time: 3245.192766189575 ms; Tokens/Sec : 161558.35347051077\n",
            "\n",
            "Training\n",
            "Step: 189; Training Loss: 6.196361541748047; Gradient Norm 0.818088173866272; Learning Rate: 0.00026573426573426445; Time: 3248.37327003479 ms; Tokens/Sec : 161400.170613516\n",
            "\n",
            "Training\n",
            "Step: 190; Training Loss: 6.285026550292969; Gradient Norm 0.5203680992126465; Learning Rate: 0.00026713286713286584; Time: 3248.1536865234375 ms; Tokens/Sec : 161411.08167857528\n",
            "\n",
            "Training\n",
            "Step: 191; Training Loss: 6.291591167449951; Gradient Norm 0.8681389689445496; Learning Rate: 0.0002685314685314672; Time: 3252.6891231536865 ms; Tokens/Sec : 161186.01567790462\n",
            "\n",
            "Training\n",
            "Step: 192; Training Loss: 6.31358003616333; Gradient Norm 0.7709146738052368; Learning Rate: 0.0002699300699300686; Time: 3275.2485275268555 ms; Tokens/Sec : 160075.79137693424\n",
            "\n",
            "Training\n",
            "Step: 193; Training Loss: 6.296306133270264; Gradient Norm 0.7068102955818176; Learning Rate: 0.00027132867132867; Time: 3250.576972961426 ms; Tokens/Sec : 161290.7506455229\n",
            "\n",
            "Training\n",
            "Step: 194; Training Loss: 6.230704307556152; Gradient Norm 0.8070365786552429; Learning Rate: 0.0002727272727272714; Time: 3251.301050186157 ms; Tokens/Sec : 161254.8305762031\n",
            "\n",
            "Training\n",
            "Step: 195; Training Loss: 6.21771240234375; Gradient Norm 0.5560663938522339; Learning Rate: 0.00027412587412587277; Time: 3250.173568725586 ms; Tokens/Sec : 161310.76969085584\n",
            "\n",
            "Training\n",
            "Step: 196; Training Loss: 6.33651876449585; Gradient Norm 0.7050652503967285; Learning Rate: 0.00027552447552447415; Time: 3874.6113777160645 ms; Tokens/Sec : 135313.69959199565\n",
            "\n",
            "Training\n",
            "Step: 197; Training Loss: 6.240333080291748; Gradient Norm 0.6988133192062378; Learning Rate: 0.00027692307692307554; Time: 3249.3929862976074 ms; Tokens/Sec : 161349.52042146778\n",
            "\n",
            "Training\n",
            "Step: 198; Training Loss: 6.275879383087158; Gradient Norm 0.9552320837974548; Learning Rate: 0.0002783216783216769; Time: 3245.368242263794 ms; Tokens/Sec : 161549.61805945478\n",
            "\n",
            "Training\n",
            "Step: 199; Training Loss: 6.260603904724121; Gradient Norm 0.9932047128677368; Learning Rate: 0.0002797202797202783; Time: 3251.474618911743 ms; Tokens/Sec : 161246.2225448579\n",
            "\n",
            "Training\n",
            "Step: 200; Training Loss: 6.2183003425598145; Gradient Norm 0.5828852653503418; Learning Rate: 0.0002811188811188797; Time: 3250.2784729003906 ms; Tokens/Sec : 161305.56331444145\n",
            "\n",
            "Training\n",
            "Step: 201; Training Loss: 6.206139087677002; Gradient Norm 0.5536899566650391; Learning Rate: 0.0002825174825174811; Time: 3250.2498626708984 ms; Tokens/Sec : 161306.98320195157\n",
            "\n",
            "Training\n",
            "Step: 202; Training Loss: 6.158698558807373; Gradient Norm 0.4682266116142273; Learning Rate: 0.00028391608391608247; Time: 3250.1087188720703 ms; Tokens/Sec : 161313.98834619625\n",
            "\n",
            "Training\n",
            "Step: 203; Training Loss: 6.229994773864746; Gradient Norm 0.5218325257301331; Learning Rate: 0.00028531468531468385; Time: 3249.8602867126465 ms; Tokens/Sec : 161326.31982476288\n",
            "\n",
            "Training\n",
            "Step: 204; Training Loss: 6.217177867889404; Gradient Norm 0.7222243547439575; Learning Rate: 0.00028671328671328524; Time: 3250.870943069458 ms; Tokens/Sec : 161276.16542813895\n",
            "\n",
            "Training\n",
            "Step: 205; Training Loss: 6.114260673522949; Gradient Norm 0.835317075252533; Learning Rate: 0.0002881118881118866; Time: 3249.7990131378174 ms; Tokens/Sec : 161329.3615637411\n",
            "\n",
            "Training\n",
            "Step: 206; Training Loss: 6.1752448081970215; Gradient Norm 0.6113185286521912; Learning Rate: 0.000289510489510488; Time: 3244.8441982269287 ms; Tokens/Sec : 161575.70840735134\n",
            "\n",
            "Training\n",
            "Step: 207; Training Loss: 6.220308780670166; Gradient Norm 0.5044867396354675; Learning Rate: 0.0002909090909090894; Time: 3248.758554458618 ms; Tokens/Sec : 161381.02946445916\n",
            "\n",
            "Training\n",
            "Step: 208; Training Loss: 6.1643900871276855; Gradient Norm 0.4713241457939148; Learning Rate: 0.0002923076923076908; Time: 3247.180223464966 ms; Tokens/Sec : 161459.4706543724\n",
            "\n",
            "Training\n",
            "Step: 209; Training Loss: 6.349228382110596; Gradient Norm 0.5935090780258179; Learning Rate: 0.00029370629370629217; Time: 3247.5781440734863 ms; Tokens/Sec : 161439.68728105113\n",
            "\n",
            "Training\n",
            "Step: 210; Training Loss: 6.316328048706055; Gradient Norm 0.932016909122467; Learning Rate: 0.00029510489510489355; Time: 3248.746633529663 ms; Tokens/Sec : 161381.6216349187\n",
            "\n",
            "Training\n",
            "Step: 211; Training Loss: 6.171267509460449; Gradient Norm 0.4880298376083374; Learning Rate: 0.00029650349650349494; Time: 3272.503137588501 ms; Tokens/Sec : 160210.0832167105\n",
            "\n",
            "Training\n",
            "Step: 212; Training Loss: 6.138366222381592; Gradient Norm 0.5029199123382568; Learning Rate: 0.0002979020979020963; Time: 3250.4420280456543 ms; Tokens/Sec : 161297.44677071844\n",
            "\n",
            "Training\n",
            "Step: 213; Training Loss: 6.099355220794678; Gradient Norm 0.455340713262558; Learning Rate: 0.0002993006993006977; Time: 3251.417636871338 ms; Tokens/Sec : 161249.04843183843\n",
            "\n",
            "Training\n",
            "Step: 214; Training Loss: 6.133235931396484; Gradient Norm 0.6539890766143799; Learning Rate: 0.0003006993006992991; Time: 3252.0387172698975 ms; Tokens/Sec : 161218.25278886667\n",
            "\n",
            "Training\n",
            "Step: 215; Training Loss: 6.092947483062744; Gradient Norm 0.5606485605239868; Learning Rate: 0.0003020979020979005; Time: 3252.561092376709 ms; Tokens/Sec : 161192.3604536795\n",
            "\n",
            "Training\n",
            "Step: 216; Training Loss: 6.044555187225342; Gradient Norm 0.68135005235672; Learning Rate: 0.00030349650349650187; Time: 3253.302812576294 ms; Tokens/Sec : 161155.61022271265\n",
            "\n",
            "Training\n",
            "Step: 217; Training Loss: 6.041433334350586; Gradient Norm 0.5297739505767822; Learning Rate: 0.00030489510489510325; Time: 3248.81649017334 ms; Tokens/Sec : 161378.1515779079\n",
            "\n",
            "Training\n",
            "Step: 218; Training Loss: 6.028629302978516; Gradient Norm 0.527978777885437; Learning Rate: 0.00030629370629370464; Time: 3249.114990234375 ms; Tokens/Sec : 161363.32557506082\n",
            "\n",
            "Training\n",
            "Step: 219; Training Loss: 6.009183883666992; Gradient Norm 0.6288507580757141; Learning Rate: 0.000307692307692306; Time: 3252.7153491973877 ms; Tokens/Sec : 161184.7160648007\n",
            "\n",
            "Training\n",
            "Step: 220; Training Loss: 6.031609058380127; Gradient Norm 0.9833794832229614; Learning Rate: 0.0003090909090909074; Time: 3249.5827674865723 ms; Tokens/Sec : 161340.09733363913\n",
            "\n",
            "Training\n",
            "Step: 221; Training Loss: 6.1114325523376465; Gradient Norm 1.4844499826431274; Learning Rate: 0.0003104895104895088; Time: 3250.0674724578857 ms; Tokens/Sec : 161316.03557248725\n",
            "\n",
            "Training\n",
            "Step: 222; Training Loss: 6.043423175811768; Gradient Norm 0.7066041827201843; Learning Rate: 0.0003118881118881102; Time: 3252.021312713623 ms; Tokens/Sec : 161219.11561597735\n",
            "\n",
            "Training\n",
            "Step: 223; Training Loss: 6.104662895202637; Gradient Norm 0.7031073570251465; Learning Rate: 0.00031328671328671156; Time: 3251.2834072113037 ms; Tokens/Sec : 161255.70561985957\n",
            "\n",
            "Training\n",
            "Step: 224; Training Loss: 6.045392990112305; Gradient Norm 0.6312220692634583; Learning Rate: 0.00031468531468531295; Time: 3251.2898445129395 ms; Tokens/Sec : 161255.3863460737\n",
            "\n",
            "Training\n",
            "Step: 225; Training Loss: 5.9379987716674805; Gradient Norm 0.5806645154953003; Learning Rate: 0.00031608391608391434; Time: 3251.1374950408936 ms; Tokens/Sec : 161262.94283146132\n",
            "\n",
            "Training\n",
            "Step: 226; Training Loss: 5.971625804901123; Gradient Norm 0.5883877277374268; Learning Rate: 0.0003174825174825157; Time: 3252.5970935821533 ms; Tokens/Sec : 161190.576304239\n",
            "\n",
            "Training\n",
            "Step: 227; Training Loss: 5.998270034790039; Gradient Norm 0.5147886872291565; Learning Rate: 0.0003188811188811171; Time: 3254.05216217041 ms; Tokens/Sec : 161118.49898875217\n",
            "\n",
            "Training\n",
            "Step: 228; Training Loss: 5.904837608337402; Gradient Norm 0.5522058606147766; Learning Rate: 0.0003202797202797185; Time: 3253.4797191619873 ms; Tokens/Sec : 161146.84745446732\n",
            "\n",
            "Training\n",
            "Step: 229; Training Loss: 5.953116416931152; Gradient Norm 0.5479496717453003; Learning Rate: 0.0003216783216783199; Time: 3253.1135082244873 ms; Tokens/Sec : 161164.988148892\n",
            "\n",
            "Training\n",
            "Step: 230; Training Loss: 5.99232292175293; Gradient Norm 0.5039741396903992; Learning Rate: 0.00032307692307692126; Time: 3253.993034362793 ms; Tokens/Sec : 161121.42664824965\n",
            "\n",
            "Training\n",
            "Step: 231; Training Loss: 5.960125923156738; Gradient Norm 0.5159220695495605; Learning Rate: 0.00032447552447552265; Time: 3247.1492290496826 ms; Tokens/Sec : 161461.01180371043\n",
            "\n",
            "Training\n",
            "Step: 232; Training Loss: 5.95066499710083; Gradient Norm 0.6622896194458008; Learning Rate: 0.00032587412587412404; Time: 3254.018545150757 ms; Tokens/Sec : 161120.16349178797\n",
            "\n",
            "Training\n",
            "Step: 233; Training Loss: 5.93464994430542; Gradient Norm 0.7553503513336182; Learning Rate: 0.0003272727272727254; Time: 3250.3724098205566 ms; Tokens/Sec : 161300.90152621752\n",
            "\n",
            "Training\n",
            "Step: 234; Training Loss: 5.969347953796387; Gradient Norm 0.8550226092338562; Learning Rate: 0.0003286713286713268; Time: 3250.642776489258 ms; Tokens/Sec : 161287.4855988448\n",
            "\n",
            "Training\n",
            "Step: 235; Training Loss: 5.941690444946289; Gradient Norm 0.6329129934310913; Learning Rate: 0.0003300699300699282; Time: 3251.148462295532 ms; Tokens/Sec : 161262.39883546164\n",
            "\n",
            "Training\n",
            "Step: 236; Training Loss: 5.916811466217041; Gradient Norm 0.6369033455848694; Learning Rate: 0.0003314685314685296; Time: 3252.1145343780518 ms; Tokens/Sec : 161214.49427987845\n",
            "\n",
            "Training\n",
            "Step: 237; Training Loss: 6.019412994384766; Gradient Norm 0.620093822479248; Learning Rate: 0.00033286713286713096; Time: 3252.4802684783936 ms; Tokens/Sec : 161196.366072123\n",
            "\n",
            "Training\n",
            "Step: 238; Training Loss: 6.105498790740967; Gradient Norm 0.5775870680809021; Learning Rate: 0.00033426573426573235; Time: 3252.8367042541504 ms; Tokens/Sec : 161178.70267336862\n",
            "\n",
            "Training\n",
            "Step: 239; Training Loss: 6.072286128997803; Gradient Norm 0.7214426398277283; Learning Rate: 0.00033566433566433373; Time: 3254.9734115600586 ms; Tokens/Sec : 161072.8979038624\n",
            "\n",
            "Training\n",
            "Step: 240; Training Loss: 6.064454078674316; Gradient Norm 0.6693896055221558; Learning Rate: 0.0003370629370629351; Time: 3250.1978874206543 ms; Tokens/Sec : 161309.56272821687\n",
            "\n",
            "Training\n",
            "Step: 241; Training Loss: 6.08025598526001; Gradient Norm 0.9926877617835999; Learning Rate: 0.0003384615384615365; Time: 3250.4706382751465 ms; Tokens/Sec : 161296.02705108945\n",
            "\n",
            "Training\n",
            "Step: 242; Training Loss: 6.0950398445129395; Gradient Norm 1.1428508758544922; Learning Rate: 0.0003398601398601379; Time: 3244.344711303711 ms; Tokens/Sec : 161600.58398644067\n",
            "\n",
            "Training\n",
            "Step: 243; Training Loss: 6.085043907165527; Gradient Norm 0.8542441129684448; Learning Rate: 0.0003412587412587393; Time: 3250.8363723754883 ms; Tokens/Sec : 161277.8805033753\n",
            "\n",
            "Training\n",
            "Step: 244; Training Loss: 5.995055675506592; Gradient Norm 0.7096760869026184; Learning Rate: 0.00034265734265734066; Time: 3248.533010482788 ms; Tokens/Sec : 161392.23406631837\n",
            "\n",
            "Training\n",
            "Step: 245; Training Loss: 6.0584564208984375; Gradient Norm 0.601703405380249; Learning Rate: 0.00034405594405594205; Time: 3251.117467880249 ms; Tokens/Sec : 161263.93622493112\n",
            "\n",
            "Training\n",
            "Step: 246; Training Loss: 6.027177333831787; Gradient Norm 0.5973003506660461; Learning Rate: 0.00034545454545454343; Time: 3251.769542694092 ms; Tokens/Sec : 161231.59809339602\n",
            "\n",
            "Training\n",
            "Step: 247; Training Loss: 6.0624003410339355; Gradient Norm 0.5717070698738098; Learning Rate: 0.0003468531468531448; Time: 3252.9990673065186 ms; Tokens/Sec : 161170.65795352662\n",
            "\n",
            "Training\n",
            "Step: 248; Training Loss: 6.007632255554199; Gradient Norm 0.5278881788253784; Learning Rate: 0.0003482517482517462; Time: 3254.4901371002197 ms; Tokens/Sec : 161096.81637171755\n",
            "\n",
            "Training\n",
            "Step: 249; Training Loss: 5.974052906036377; Gradient Norm 0.5821332335472107; Learning Rate: 0.0003496503496503476; Time: 3254.9171447753906 ms; Tokens/Sec : 161075.6823231453\n",
            "\n",
            "Evaluating HellaSwag\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.\n",
            "  warnings.warn(warning.format(ret))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n",
            "\n",
            "Training\n",
            "Step: 15561; Training Loss: 3.0161378383636475; Gradient Norm 0.3121089041233063; Learning Rate: 0.00020889040913110558; Time: 3258.189916610718 ms; Tokens/Sec : 160913.88575205664\n",
            "\n",
            "Training\n",
            "Step: 15562; Training Loss: 3.0355472564697266; Gradient Norm 0.30978694558143616; Learning Rate: 0.0002088423517744875; Time: 3259.3812942504883 ms; Tokens/Sec : 160855.06808449753\n",
            "\n",
            "Training\n",
            "Step: 15563; Training Loss: 3.047044038772583; Gradient Norm 0.30751946568489075; Learning Rate: 0.00020879430356597655; Time: 3258.8255405426025 ms; Tokens/Sec : 160882.4999919157\n",
            "\n",
            "Training\n",
            "Step: 15564; Training Loss: 3.0379364490509033; Gradient Norm 0.3341268002986908; Learning Rate: 0.00020874626450686161; Time: 3258.7544918060303 ms; Tokens/Sec : 160886.0076198729\n",
            "\n",
            "Training\n",
            "Step: 15565; Training Loss: 3.0487701892852783; Gradient Norm 0.24578820168972015; Learning Rate: 0.00020869823459843036; Time: 3260.3683471679688 ms; Tokens/Sec : 160806.37037695714\n",
            "\n",
            "Training\n",
            "Step: 15566; Training Loss: 3.019615411758423; Gradient Norm 0.3026612102985382; Learning Rate: 0.00020865021384197088; Time: 3271.024465560913 ms; Tokens/Sec : 160282.5064501911\n",
            "\n",
            "Training\n",
            "Step: 15567; Training Loss: 2.975102186203003; Gradient Norm 0.2979854345321655; Learning Rate: 0.00020860220223877077; Time: 3259.50026512146 ms; Tokens/Sec : 160849.1969183697\n",
            "\n",
            "Training\n",
            "Step: 15568; Training Loss: 3.0463509559631348; Gradient Norm 0.26951032876968384; Learning Rate: 0.00020855419979011773; Time: 3260.690450668335 ms; Tokens/Sec : 160790.48530734898\n",
            "\n",
            "Training\n",
            "Step: 15569; Training Loss: 3.017932415008545; Gradient Norm 0.30455315113067627; Learning Rate: 0.00020850620649729863; Time: 3260.3516578674316 ms; Tokens/Sec : 160807.19352308527\n",
            "\n",
            "Training\n",
            "Step: 15570; Training Loss: 3.0035624504089355; Gradient Norm 0.3068734109401703; Learning Rate: 0.00020845822236160053; Time: 3260.4074478149414 ms; Tokens/Sec : 160804.44189617073\n",
            "\n",
            "Training\n",
            "Step: 15571; Training Loss: 3.039285182952881; Gradient Norm 0.32088702917099; Learning Rate: 0.0002084102473843101; Time: 3260.2460384368896 ms; Tokens/Sec : 160812.4030575826\n",
            "\n",
            "Training\n",
            "Step: 15572; Training Loss: 3.0228965282440186; Gradient Norm 0.30832743644714355; Learning Rate: 0.00020836228156671381; Time: 3259.185314178467 ms; Tokens/Sec : 160864.7405593001\n",
            "\n",
            "Training\n",
            "Step: 15573; Training Loss: 3.075401544570923; Gradient Norm 0.33856046199798584; Learning Rate: 0.00020831432491009782; Time: 3257.7714920043945 ms; Tokens/Sec : 160934.5533554975\n",
            "\n",
            "Training\n",
            "Step: 15574; Training Loss: 3.01882266998291; Gradient Norm 0.3365715444087982; Learning Rate: 0.0002082663774157481; Time: 3259.234666824341 ms; Tokens/Sec : 160862.30468051686\n",
            "\n",
            "Training\n",
            "Step: 15575; Training Loss: 3.115709066390991; Gradient Norm 0.3413884937763214; Learning Rate: 0.00020821843908495037; Time: 3257.209300994873 ms; Tokens/Sec : 160962.33049557573\n",
            "\n",
            "Training\n",
            "Step: 15576; Training Loss: 3.050300121307373; Gradient Norm 0.336769163608551; Learning Rate: 0.0002081705099189901; Time: 3260.331392288208 ms; Tokens/Sec : 160808.19306899887\n",
            "\n",
            "Training\n",
            "Step: 15577; Training Loss: 3.043329954147339; Gradient Norm 0.3891952931880951; Learning Rate: 0.00020812258991915246; Time: 3260.0202560424805 ms; Tokens/Sec : 160823.54059862875\n",
            "\n",
            "Training\n",
            "Step: 15578; Training Loss: 2.96606183052063; Gradient Norm 0.31542083621025085; Learning Rate: 0.00020807467908672244; Time: 3260.178804397583 ms; Tokens/Sec : 160815.71946078524\n",
            "\n",
            "Training\n",
            "Step: 15579; Training Loss: 2.9929425716400146; Gradient Norm 0.33856305480003357; Learning Rate: 0.0002080267774229848; Time: 3258.974075317383 ms; Tokens/Sec : 160875.1674248716\n",
            "\n",
            "Training\n",
            "Step: 15580; Training Loss: 3.157290458679199; Gradient Norm 0.34311243891716003; Learning Rate: 0.00020797888492922397; Time: 3260.1914405822754 ms; Tokens/Sec : 160815.09615470967\n",
            "\n",
            "Training\n",
            "Step: 15581; Training Loss: 3.1205220222473145; Gradient Norm 0.3522687256336212; Learning Rate: 0.00020793100160672437; Time: 3262.190580368042 ms; Tokens/Sec : 160716.54524269074\n",
            "\n",
            "Training\n",
            "Step: 15582; Training Loss: 3.0681357383728027; Gradient Norm 0.28800323605537415; Learning Rate: 0.00020788312745676952; Time: 3260.4820728302 ms; Tokens/Sec : 160800.76144841418\n",
            "\n",
            "Training\n",
            "Step: 15583; Training Loss: 2.989206075668335; Gradient Norm 0.3796567916870117; Learning Rate: 0.00020783526248064358; Time: 3258.7802410125732 ms; Tokens/Sec : 160884.73638133158\n",
            "\n",
            "Training\n",
            "Step: 15584; Training Loss: 3.039217710494995; Gradient Norm 0.3331359922885895; Learning Rate: 0.00020778740667962993; Time: 3258.486747741699 ms; Tokens/Sec : 160899.22733715546\n",
            "\n",
            "Training\n",
            "Step: 15585; Training Loss: 3.074826955795288; Gradient Norm 0.27782225608825684; Learning Rate: 0.00020773956005501194; Time: 3261.3513469696045 ms; Tokens/Sec : 160757.90193140644\n",
            "\n",
            "Training\n",
            "Step: 15586; Training Loss: 3.043267250061035; Gradient Norm 0.3306414783000946; Learning Rate: 0.00020769172260807226; Time: 3262.951612472534 ms; Tokens/Sec : 160679.06063820404\n",
            "\n",
            "Training\n",
            "Step: 15587; Training Loss: 3.0744597911834717; Gradient Norm 0.2641843855381012; Learning Rate: 0.00020764389434009393; Time: 3259.2015266418457 ms; Tokens/Sec : 160863.94035909954\n",
            "\n",
            "Training\n",
            "Step: 15588; Training Loss: 3.038268566131592; Gradient Norm 0.3053950369358063; Learning Rate: 0.0002075960752523596; Time: 3258.9778900146484 ms; Tokens/Sec : 160874.97911734635\n",
            "\n",
            "Training\n",
            "Step: 15589; Training Loss: 3.01820707321167; Gradient Norm 0.3155323266983032; Learning Rate: 0.0002075482653461512; Time: 3261.83819770813 ms; Tokens/Sec : 160733.90776047113\n",
            "\n",
            "Training\n",
            "Step: 15590; Training Loss: 3.013686418533325; Gradient Norm 0.30803582072257996; Learning Rate: 0.0002075004646227507; Time: 3260.049819946289 ms; Tokens/Sec : 160822.08216334495\n",
            "\n",
            "Training\n",
            "Step: 15591; Training Loss: 3.1332545280456543; Gradient Norm 0.3254717290401459; Learning Rate: 0.00020745267308344033; Time: 3258.035182952881 ms; Tokens/Sec : 160921.52802500367\n",
            "\n",
            "Training\n",
            "Step: 15592; Training Loss: 3.042276620864868; Gradient Norm 0.2814615070819855; Learning Rate: 0.00020740489072950123; Time: 3258.970022201538 ms; Tokens/Sec : 160875.36750210018\n",
            "\n",
            "Training\n",
            "Step: 15593; Training Loss: 3.0396971702575684; Gradient Norm 0.3253905475139618; Learning Rate: 0.0002073571175622148; Time: 3257.1520805358887 ms; Tokens/Sec : 160965.1582230513\n",
            "\n",
            "Training\n",
            "Step: 15594; Training Loss: 3.055332899093628; Gradient Norm 0.29782816767692566; Learning Rate: 0.0002073093535828619; Time: 3264.0199661254883 ms; Tokens/Sec : 160626.4684165977\n",
            "\n",
            "Training\n",
            "Step: 15595; Training Loss: 3.06270170211792; Gradient Norm 0.31515616178512573; Learning Rate: 0.00020726159879272377; Time: 3265.563726425171 ms; Tokens/Sec : 160550.5339728711\n",
            "\n",
            "Training\n",
            "Step: 15596; Training Loss: 3.062978506088257; Gradient Norm 0.3110661506652832; Learning Rate: 0.00020721385319308046; Time: 3259.2084407806396 ms; Tokens/Sec : 160863.5990996708\n",
            "\n",
            "Training\n",
            "Step: 15597; Training Loss: 3.0550734996795654; Gradient Norm 0.304083913564682; Learning Rate: 0.00020716611678521252; Time: 3263.3328437805176 ms; Tokens/Sec : 160660.2896787632\n",
            "\n",
            "Training\n",
            "Step: 15598; Training Loss: 3.033555030822754; Gradient Norm 0.3089370131492615; Learning Rate: 0.00020711838957039992; Time: 3262.2389793395996 ms; Tokens/Sec : 160714.16083261187\n",
            "\n",
            "Training\n",
            "Step: 15599; Training Loss: 3.0478265285491943; Gradient Norm 0.2825026512145996; Learning Rate: 0.00020707067154992248; Time: 3260.3604793548584 ms; Tokens/Sec : 160806.75843051047\n",
            "\n",
            "Training\n",
            "Step: 15600; Training Loss: 3.0172524452209473; Gradient Norm 0.35158970952033997; Learning Rate: 0.00020702296272505976; Time: 3260.7381343841553 ms; Tokens/Sec : 160788.13397231622\n",
            "\n",
            "Training\n",
            "Step: 15601; Training Loss: 3.0230536460876465; Gradient Norm 0.31934815645217896; Learning Rate: 0.00020697526309709105; Time: 3262.2084617614746 ms; Tokens/Sec : 160715.66429476533\n",
            "\n",
            "Training\n",
            "Step: 15602; Training Loss: 2.991626262664795; Gradient Norm 0.28178074955940247; Learning Rate: 0.0002069275726672954; Time: 3259.4516277313232 ms; Tokens/Sec : 160851.5971028293\n",
            "\n",
            "Training\n",
            "Step: 15603; Training Loss: 3.032951831817627; Gradient Norm 0.2961675226688385; Learning Rate: 0.00020687989143695164; Time: 3262.864828109741 ms; Tokens/Sec : 160683.33431505744\n",
            "\n",
            "Training\n",
            "Step: 15604; Training Loss: 3.0071778297424316; Gradient Norm 0.3070172369480133; Learning Rate: 0.00020683221940733833; Time: 3263.895273208618 ms; Tokens/Sec : 160632.6049440279\n",
            "\n",
            "Training\n",
            "Step: 15605; Training Loss: 2.9973905086517334; Gradient Norm 0.3045191466808319; Learning Rate: 0.00020678455657973383; Time: 3262.9857063293457 ms; Tokens/Sec : 160677.3817559229\n",
            "\n",
            "Training\n",
            "Step: 15606; Training Loss: 2.987814426422119; Gradient Norm 0.27688807249069214; Learning Rate: 0.0002067369029554162; Time: 3260.4305744171143 ms; Tokens/Sec : 160803.30129210924\n",
            "\n",
            "Training\n",
            "Step: 15607; Training Loss: 3.0107216835021973; Gradient Norm 0.27450984716415405; Learning Rate: 0.00020668925853566317; Time: 3262.9477977752686 ms; Tokens/Sec : 160679.24848735495\n",
            "\n",
            "Training\n",
            "Step: 15608; Training Loss: 3.0533924102783203; Gradient Norm 0.35552671551704407; Learning Rate: 0.0002066416233217526; Time: 3260.746479034424 ms; Tokens/Sec : 160787.72249575588\n",
            "\n",
            "Training\n",
            "Step: 15609; Training Loss: 3.032012462615967; Gradient Norm 0.3164365887641907; Learning Rate: 0.0002065939973149614; Time: 3260.4331970214844 ms; Tokens/Sec : 160803.17194627842\n",
            "\n",
            "Training\n",
            "Step: 15610; Training Loss: 3.0103390216827393; Gradient Norm 0.3071801960468292; Learning Rate: 0.00020654638051656693; Time: 3262.7809047698975 ms; Tokens/Sec : 160687.46731769125\n",
            "\n",
            "Training\n",
            "Step: 15611; Training Loss: 3.0966548919677734; Gradient Norm 0.3353497385978699; Learning Rate: 0.00020649877292784602; Time: 3261.0127925872803 ms; Tokens/Sec : 160774.59162128312\n",
            "\n",
            "Training\n",
            "Step: 15612; Training Loss: 3.0165271759033203; Gradient Norm 0.3420582413673401; Learning Rate: 0.00020645117455007534; Time: 3263.0927562713623 ms; Tokens/Sec : 160672.11052838355\n",
            "\n",
            "Training\n",
            "Step: 15613; Training Loss: 3.0261173248291016; Gradient Norm 0.2910805940628052; Learning Rate: 0.00020640358538453086; Time: 3262.4261379241943 ms; Tokens/Sec : 160704.94099633233\n",
            "\n",
            "Training\n",
            "Step: 15614; Training Loss: 3.24888014793396; Gradient Norm 0.4630891978740692; Learning Rate: 0.00020635600543248908; Time: 3260.1585388183594 ms; Tokens/Sec : 160816.71911269307\n",
            "\n",
            "Training\n",
            "Step: 15615; Training Loss: 3.034071922302246; Gradient Norm 0.47609177231788635; Learning Rate: 0.00020630843469522582; Time: 3261.059045791626 ms; Tokens/Sec : 160772.31127617575\n",
            "\n",
            "Training\n",
            "Step: 15616; Training Loss: 3.036790609359741; Gradient Norm 0.3514259159564972; Learning Rate: 0.00020626087317401648; Time: 3262.989044189453 ms; Tokens/Sec : 160677.217391711\n",
            "\n",
            "Training\n",
            "Step: 15617; Training Loss: 3.1092634201049805; Gradient Norm 0.3913189768791199; Learning Rate: 0.00020621332087013635; Time: 3263.291358947754 ms; Tokens/Sec : 160662.33208457867\n",
            "\n",
            "Training\n",
            "Step: 15618; Training Loss: 3.094482421875; Gradient Norm 0.36755234003067017; Learning Rate: 0.00020616577778486084; Time: 3259.2036724090576 ms; Tokens/Sec : 160863.834450846\n",
            "\n",
            "Training\n",
            "Step: 15619; Training Loss: 2.9901208877563477; Gradient Norm 0.4227127134799957; Learning Rate: 0.0002061182439194648; Time: 3260.909080505371 ms; Tokens/Sec : 160779.70500138772\n",
            "\n",
            "Training\n",
            "Step: 15620; Training Loss: 3.0342252254486084; Gradient Norm 0.3123779296875; Learning Rate: 0.00020607071927522243; Time: 3261.3184452056885 ms; Tokens/Sec : 160759.52373517258\n",
            "\n",
            "Training\n",
            "Step: 15621; Training Loss: 3.1349170207977295; Gradient Norm 0.39765456318855286; Learning Rate: 0.0002060232038534086; Time: 3262.7758979797363 ms; Tokens/Sec : 160687.71389559165\n",
            "\n",
            "Training\n",
            "Step: 15622; Training Loss: 3.1164348125457764; Gradient Norm 0.36769765615463257; Learning Rate: 0.00020597569765529725; Time: 3265.23494720459 ms; Tokens/Sec : 160566.69993957088\n",
            "\n",
            "Training\n",
            "Step: 15623; Training Loss: 3.074753999710083; Gradient Norm 0.3295467793941498; Learning Rate: 0.00020592820068216216; Time: 3261.5981101989746 ms; Tokens/Sec : 160745.73944611946\n",
            "\n",
            "Training\n",
            "Step: 15624; Training Loss: 3.029186725616455; Gradient Norm 0.3614944517612457; Learning Rate: 0.00020588071293527687; Time: 3260.5111598968506 ms; Tokens/Sec : 160799.32694252345\n",
            "\n",
            "Training\n",
            "Step: 15625; Training Loss: 3.070211410522461; Gradient Norm 0.33155009150505066; Learning Rate: 0.00020583323441591517; Time: 3263.0555629730225 ms; Tokens/Sec : 160673.94191789758\n",
            "\n",
            "Training\n",
            "Step: 15626; Training Loss: 3.046370029449463; Gradient Norm 0.3266420066356659; Learning Rate: 0.0002057857651253498; Time: 3262.34769821167 ms; Tokens/Sec : 160708.80497728687\n",
            "\n",
            "Training\n",
            "Step: 15627; Training Loss: 3.0835776329040527; Gradient Norm 0.2769782841205597; Learning Rate: 0.00020573830506485376; Time: 3260.772705078125 ms; Tokens/Sec : 160786.42929741973\n",
            "\n",
            "Training\n",
            "Step: 15628; Training Loss: 3.066288709640503; Gradient Norm 0.3057958483695984; Learning Rate: 0.00020569085423569958; Time: 3262.1781826019287 ms; Tokens/Sec : 160717.15603892165\n",
            "\n",
            "Training\n",
            "Step: 15629; Training Loss: 3.043769359588623; Gradient Norm 0.28067973256111145; Learning Rate: 0.00020564341263916002; Time: 3262.8326416015625 ms; Tokens/Sec : 160684.91939036536\n",
            "\n",
            "Training\n",
            "Step: 15630; Training Loss: 3.0977871417999268; Gradient Norm 0.30324816703796387; Learning Rate: 0.00020559598027650684; Time: 3261.3484859466553 ms; Tokens/Sec : 160758.04295652188\n",
            "\n",
            "Training\n",
            "Step: 15631; Training Loss: 3.072200298309326; Gradient Norm 0.36068928241729736; Learning Rate: 0.0002055485571490121; Time: 3261.0678672790527 ms; Tokens/Sec : 160771.8763723405\n",
            "\n",
            "Training\n",
            "Step: 15632; Training Loss: 3.0949459075927734; Gradient Norm 0.3416568636894226; Learning Rate: 0.00020550114325794736; Time: 3263.8585567474365 ms; Tokens/Sec : 160634.41196498222\n",
            "\n",
            "Training\n",
            "Step: 15633; Training Loss: 3.1121411323547363; Gradient Norm 0.35431966185569763; Learning Rate: 0.00020545373860458415; Time: 3260.451078414917 ms; Tokens/Sec : 160802.2900484325\n",
            "\n",
            "Training\n",
            "Step: 15634; Training Loss: 3.0577666759490967; Gradient Norm 0.38363856077194214; Learning Rate: 0.00020540634319019346; Time: 3261.5232467651367 ms; Tokens/Sec : 160749.42912640664\n",
            "\n",
            "Training\n",
            "Step: 15635; Training Loss: 3.0860278606414795; Gradient Norm 0.3285592794418335; Learning Rate: 0.00020535895701604632; Time: 3257.617950439453 ms; Tokens/Sec : 160942.1386965508\n",
            "\n",
            "Training\n",
            "Step: 15636; Training Loss: 3.046887159347534; Gradient Norm 0.2763179540634155; Learning Rate: 0.00020531158008341334; Time: 3260.3704929351807 ms; Tokens/Sec : 160806.26454449492\n",
            "\n",
            "Training\n",
            "Step: 15637; Training Loss: 3.024055004119873; Gradient Norm 0.3185712397098541; Learning Rate: 0.0002052642123935649; Time: 3260.0882053375244 ms; Tokens/Sec : 160820.18858925914\n",
            "\n",
            "Training\n",
            "Step: 15638; Training Loss: 3.0152082443237305; Gradient Norm 0.30973535776138306; Learning Rate: 0.0002052168539477712; Time: 3262.101888656616 ms; Tokens/Sec : 160720.91488715267\n",
            "\n",
            "Training\n",
            "Step: 15639; Training Loss: 2.990186929702759; Gradient Norm 0.29095011949539185; Learning Rate: 0.00020516950474730214; Time: 3260.9972953796387 ms; Tokens/Sec : 160775.35566890542\n",
            "\n",
            "Training\n",
            "Step: 15640; Training Loss: 3.0766525268554688; Gradient Norm 0.30453720688819885; Learning Rate: 0.00020512216479342738; Time: 3261.1770629882812 ms; Tokens/Sec : 160766.49316292704\n",
            "\n",
            "Training\n",
            "Step: 15641; Training Loss: 3.0115959644317627; Gradient Norm 0.27584919333457947; Learning Rate: 0.00020507483408741634; Time: 3262.1161937713623 ms; Tokens/Sec : 160720.2100897166\n",
            "\n",
            "Training\n",
            "Step: 15642; Training Loss: 3.0137686729431152; Gradient Norm 0.29537108540534973; Learning Rate: 0.00020502751263053832; Time: 3261.094093322754 ms; Tokens/Sec : 160770.5834288881\n",
            "\n",
            "Training\n",
            "Step: 15643; Training Loss: 3.007643222808838; Gradient Norm 0.28723978996276855; Learning Rate: 0.00020498020042406185; Time: 3263.5529041290283 ms; Tokens/Sec : 160649.45640583115\n",
            "\n",
            "Training\n",
            "Step: 15644; Training Loss: 3.0303711891174316; Gradient Norm 0.2765134274959564; Learning Rate: 0.00020493289746925598; Time: 3262.605667114258 ms; Tokens/Sec : 160696.09799449882\n",
            "\n",
            "Training\n",
            "Step: 15645; Training Loss: 3.035160541534424; Gradient Norm 0.26737943291664124; Learning Rate: 0.000204885603767389; Time: 3261.451482772827 ms; Tokens/Sec : 160752.96620824168\n",
            "\n",
            "Training\n",
            "Step: 15646; Training Loss: 3.0417051315307617; Gradient Norm 0.2798032760620117; Learning Rate: 0.0002048383193197293; Time: 3264.36448097229 ms; Tokens/Sec : 160609.51620323997\n",
            "\n",
            "Training\n",
            "Step: 15647; Training Loss: 3.1117939949035645; Gradient Norm 0.3118726909160614; Learning Rate: 0.00020479104412754424; Time: 3259.216547012329 ms; Tokens/Sec : 160863.19900425343\n",
            "\n",
            "Training\n",
            "Step: 15648; Training Loss: 3.1038455963134766; Gradient Norm 0.3272060453891754; Learning Rate: 0.00020474377819210205; Time: 3260.7429027557373 ms; Tokens/Sec : 160787.89884259528\n",
            "\n",
            "Training\n",
            "Step: 15649; Training Loss: 3.054992437362671; Gradient Norm 0.33206990361213684; Learning Rate: 0.00020469652151467008; Time: 3260.3466510772705 ms; Tokens/Sec : 160807.44046856702\n",
            "\n",
            "Training\n",
            "Step: 15650; Training Loss: 3.041597366333008; Gradient Norm 0.35523736476898193; Learning Rate: 0.0002046492740965153; Time: 3263.561964035034 ms; Tokens/Sec : 160649.0104302404\n",
            "\n",
            "Training\n",
            "Step: 15651; Training Loss: 3.0468685626983643; Gradient Norm 0.3497081995010376; Learning Rate: 0.00020460203593890467; Time: 3260.0295543670654 ms; Tokens/Sec : 160823.0818943574\n",
            "\n",
            "Training\n",
            "Step: 15652; Training Loss: 3.0595786571502686; Gradient Norm 0.37676990032196045; Learning Rate: 0.00020455480704310516; Time: 3261.8844509124756 ms; Tokens/Sec : 160731.628569288\n",
            "\n",
            "Training\n",
            "Step: 15653; Training Loss: 3.021829843521118; Gradient Norm 0.36334487795829773; Learning Rate: 0.00020450758741038288; Time: 3261.723518371582 ms; Tokens/Sec : 160739.55902361436\n",
            "\n",
            "Training\n",
            "Step: 15654; Training Loss: 3.0585412979125977; Gradient Norm 0.3662966787815094; Learning Rate: 0.00020446037704200418; Time: 3261.368751525879 ms; Tokens/Sec : 160757.04403395177\n",
            "\n",
            "Training\n",
            "Step: 15655; Training Loss: 3.0590977668762207; Gradient Norm 0.35763731598854065; Learning Rate: 0.00020441317593923483; Time: 3263.275623321533 ms; Tokens/Sec : 160663.106803817\n",
            "\n",
            "Training\n",
            "Step: 15656; Training Loss: 3.0414929389953613; Gradient Norm 0.35184362530708313; Learning Rate: 0.0002043659841033409; Time: 3260.7710361480713 ms; Tokens/Sec : 160786.51159123954\n",
            "\n",
            "Training\n",
            "Step: 15657; Training Loss: 3.0833942890167236; Gradient Norm 0.3516865074634552; Learning Rate: 0.0002043188015355875; Time: 3262.4166011810303 ms; Tokens/Sec : 160705.41077132884\n",
            "\n",
            "Training\n",
            "Step: 15658; Training Loss: 3.068059206008911; Gradient Norm 0.3620956540107727; Learning Rate: 0.00020427162823723988; Time: 3263.5061740875244 ms; Tokens/Sec : 160651.75674030732\n",
            "\n",
            "Training\n",
            "Step: 15659; Training Loss: 3.057614803314209; Gradient Norm 0.3123888373374939; Learning Rate: 0.00020422446420956307; Time: 3259.359359741211 ms; Tokens/Sec : 160856.15059077984\n",
            "\n",
            "Training\n",
            "Step: 15660; Training Loss: 3.0378692150115967; Gradient Norm 0.3621494472026825; Learning Rate: 0.00020417730945382172; Time: 3259.0084075927734 ms; Tokens/Sec : 160873.47267301433\n",
            "\n",
            "Training\n",
            "Step: 15661; Training Loss: 3.1367201805114746; Gradient Norm 0.48108601570129395; Learning Rate: 0.00020413016397128028; Time: 3259.063243865967 ms; Tokens/Sec : 160870.7658517479\n",
            "\n",
            "Training\n",
            "Step: 15662; Training Loss: 3.042886972427368; Gradient Norm 0.38343575596809387; Learning Rate: 0.00020408302776320296; Time: 3260.3232860565186 ms; Tokens/Sec : 160808.59289084357\n",
            "\n",
            "Training\n",
            "Step: 15663; Training Loss: 3.0415878295898438; Gradient Norm 0.3202626407146454; Learning Rate: 0.0002040359008308537; Time: 3259.5672607421875 ms; Tokens/Sec : 160845.89089921777\n",
            "\n",
            "Training\n",
            "Step: 15664; Training Loss: 3.0288145542144775; Gradient Norm 0.35899823904037476; Learning Rate: 0.0002039887831754962; Time: 3259.4423294067383 ms; Tokens/Sec : 160852.05596977915\n",
            "\n",
            "Training\n",
            "Step: 15665; Training Loss: 3.056863784790039; Gradient Norm 0.3756985664367676; Learning Rate: 0.000203941674798394; Time: 3263.5674476623535 ms; Tokens/Sec : 160648.74049884887\n",
            "\n",
            "Training\n",
            "Step: 15666; Training Loss: 3.1149473190307617; Gradient Norm 0.34128737449645996; Learning Rate: 0.00020389457570081014; Time: 3258.103132247925 ms; Tokens/Sec : 160918.17192977192\n",
            "\n",
            "Training\n",
            "Step: 15667; Training Loss: 3.0480833053588867; Gradient Norm 0.3108539879322052; Learning Rate: 0.00020384748588400775; Time: 3258.991479873657 ms; Tokens/Sec : 160874.3082753703\n",
            "\n",
            "Training\n",
            "Step: 15668; Training Loss: 3.1357474327087402; Gradient Norm 0.3379462659358978; Learning Rate: 0.0002038004053492494; Time: 3258.9187622070312 ms; Tokens/Sec : 160877.89793352733\n",
            "\n",
            "Training\n",
            "Step: 15669; Training Loss: 3.1193864345550537; Gradient Norm 0.305923730134964; Learning Rate: 0.00020375333409779783; Time: 3259.9380016326904 ms; Tokens/Sec : 160827.59848114237\n",
            "\n",
            "Training\n",
            "Step: 15670; Training Loss: 3.0299272537231445; Gradient Norm 0.2866041362285614; Learning Rate: 0.0002037062721309147; Time: 3260.537624359131 ms; Tokens/Sec : 160798.02179956457\n",
            "\n",
            "Training\n",
            "Step: 15671; Training Loss: 3.012993574142456; Gradient Norm 0.31023597717285156; Learning Rate: 0.00020365921944986247; Time: 3261.91782951355 ms; Tokens/Sec : 160729.98383229878\n",
            "\n",
            "Training\n",
            "Step: 15672; Training Loss: 3.0330183506011963; Gradient Norm 0.32913997769355774; Learning Rate: 0.0002036121760559027; Time: 3259.7434520721436 ms; Tokens/Sec : 160837.19707043885\n",
            "\n",
            "Training\n",
            "Step: 15673; Training Loss: 3.0503647327423096; Gradient Norm 0.28592076897621155; Learning Rate: 0.00020356514195029696; Time: 3261.6491317749023 ms; Tokens/Sec : 160743.22492030173\n",
            "\n",
            "Training\n",
            "Step: 15674; Training Loss: 3.0578043460845947; Gradient Norm 0.3431040942668915; Learning Rate: 0.00020351811713430608; Time: 3261.6896629333496 ms; Tokens/Sec : 160741.22745586096\n",
            "\n",
            "Training\n",
            "Step: 15675; Training Loss: 3.071068048477173; Gradient Norm 0.3307739794254303; Learning Rate: 0.00020347110160919143; Time: 3260.7216835021973 ms; Tokens/Sec : 160788.94517513234\n",
            "\n",
            "Training\n",
            "Step: 15676; Training Loss: 3.0617783069610596; Gradient Norm 0.330977201461792; Learning Rate: 0.00020342409537621366; Time: 3259.1257095336914 ms; Tokens/Sec : 160867.6825402399\n",
            "\n",
            "Training\n",
            "Step: 15677; Training Loss: 3.0494072437286377; Gradient Norm 0.3285463750362396; Learning Rate: 0.00020337709843663298; Time: 3262.997627258301 ms; Tokens/Sec : 160676.79474242445\n",
            "\n",
            "Training\n",
            "Step: 15678; Training Loss: 3.041313409805298; Gradient Norm 0.34633156657218933; Learning Rate: 0.0002033301107917099; Time: 3260.169744491577 ms; Tokens/Sec : 160816.16636245503\n",
            "\n",
            "Training\n",
            "Step: 15679; Training Loss: 3.0742669105529785; Gradient Norm 0.34169986844062805; Learning Rate: 0.00020328313244270438; Time: 3276.2064933776855 ms; Tokens/Sec : 160028.98506542927\n",
            "\n",
            "Training\n",
            "Step: 15680; Training Loss: 2.976959228515625; Gradient Norm 0.3240017592906952; Learning Rate: 0.00020323616339087595; Time: 3258.6028575897217 ms; Tokens/Sec : 160893.4942099075\n",
            "\n",
            "Training\n",
            "Step: 15681; Training Loss: 2.9974687099456787; Gradient Norm 0.3015114367008209; Learning Rate: 0.00020318920363748404; Time: 3263.160467147827 ms; Tokens/Sec : 160668.7765674776\n",
            "\n",
            "Training\n",
            "Step: 15682; Training Loss: 3.084855318069458; Gradient Norm 0.30387237668037415; Learning Rate: 0.00020314225318378813; Time: 3263.0398273468018 ms; Tokens/Sec : 160674.71674910627\n",
            "\n",
            "Training\n",
            "Step: 15683; Training Loss: 3.0460569858551025; Gradient Norm 0.3212999701499939; Learning Rate: 0.0002030953120310472; Time: 3258.2271099090576 ms; Tokens/Sec : 160912.0488886466\n",
            "\n",
            "Training\n",
            "Step: 15684; Training Loss: 3.069612979888916; Gradient Norm 0.2911500632762909; Learning Rate: 0.0002030483801805197; Time: 3259.9761486053467 ms; Tokens/Sec : 160825.71653915202\n",
            "\n",
            "Training\n",
            "Step: 15685; Training Loss: 3.103341817855835; Gradient Norm 0.33177658915519714; Learning Rate: 0.0002030014576334641; Time: 3262.3131275177 ms; Tokens/Sec : 160710.5080066093\n",
            "\n",
            "Training\n",
            "Step: 15686; Training Loss: 3.105231285095215; Gradient Norm 0.30771294236183167; Learning Rate: 0.00020295454439113903; Time: 3262.6585960388184 ms; Tokens/Sec : 160693.49108010752\n",
            "\n",
            "Training\n",
            "Step: 15687; Training Loss: 3.0847253799438477; Gradient Norm 0.3870370090007782; Learning Rate: 0.00020290764045480204; Time: 3259.7038745880127 ms; Tokens/Sec : 160839.14986488264\n",
            "\n",
            "Training\n",
            "Step: 15688; Training Loss: 3.1316895484924316; Gradient Norm 0.31446897983551025; Learning Rate: 0.0002028607458257111; Time: 3261.7146968841553 ms; Tokens/Sec : 160739.9937526237\n",
            "\n",
            "Training\n",
            "Step: 15689; Training Loss: 3.0505874156951904; Gradient Norm 0.3434246778488159; Learning Rate: 0.0002028138605051234; Time: 3260.2546215057373 ms; Tokens/Sec : 160811.97969680643\n",
            "\n",
            "Training\n",
            "Step: 15690; Training Loss: 3.0643959045410156; Gradient Norm 0.35138610005378723; Learning Rate: 0.00020276698449429664; Time: 3263.155698776245 ms; Tokens/Sec : 160669.01134892812\n",
            "\n",
            "Training\n",
            "Step: 15691; Training Loss: 3.0601489543914795; Gradient Norm 0.3286992907524109; Learning Rate: 0.00020272011779448736; Time: 3259.9520683288574 ms; Tokens/Sec : 160826.90450990733\n",
            "\n",
            "Training\n",
            "Step: 15692; Training Loss: 3.114173650741577; Gradient Norm 0.3766103982925415; Learning Rate: 0.00020267326040695246; Time: 3263.568878173828 ms; Tokens/Sec : 160648.67008211333\n",
            "\n",
            "Training\n",
            "Step: 15693; Training Loss: 3.220027208328247; Gradient Norm 0.34432920813560486; Learning Rate: 0.0002026264123329484; Time: 3260.5087757110596 ms; Tokens/Sec : 160799.44452401053\n",
            "\n",
            "Training\n",
            "Step: 15694; Training Loss: 3.0816915035247803; Gradient Norm 0.3147832453250885; Learning Rate: 0.0002025795735737314; Time: 3261.1210346221924 ms; Tokens/Sec : 160769.2552449958\n",
            "\n",
            "Training\n",
            "Step: 15695; Training Loss: 3.0645503997802734; Gradient Norm 0.30479782819747925; Learning Rate: 0.00020253274413055742; Time: 3261.799097061157 ms; Tokens/Sec : 160735.83454982785\n",
            "\n",
            "Training\n",
            "Step: 15696; Training Loss: 3.0491456985473633; Gradient Norm 0.29990729689598083; Learning Rate: 0.00020248592400468225; Time: 3258.845806121826 ms; Tokens/Sec : 160881.49952204287\n",
            "\n",
            "Training\n",
            "Step: 15697; Training Loss: 3.070887804031372; Gradient Norm 0.3049617409706116; Learning Rate: 0.00020243911319736132; Time: 3278.0439853668213 ms; Tokens/Sec : 159939.2815778007\n",
            "\n",
            "Training\n",
            "Step: 15698; Training Loss: 3.0907580852508545; Gradient Norm 0.3059428632259369; Learning Rate: 0.0002023923117098498; Time: 3258.984088897705 ms; Tokens/Sec : 160874.67311855804\n",
            "\n",
            "Training\n",
            "Step: 15699; Training Loss: 3.0636119842529297; Gradient Norm 0.3056773543357849; Learning Rate: 0.0002023455195434028; Time: 3260.211706161499 ms; Tokens/Sec : 160814.0965229786\n",
            "\n",
            "Training\n",
            "Step: 15700; Training Loss: 3.135402202606201; Gradient Norm 0.34096357226371765; Learning Rate: 0.00020229873669927494; Time: 3262.563467025757 ms; Tokens/Sec : 160698.17654090127\n",
            "\n",
            "Training\n",
            "Step: 15701; Training Loss: 3.0894203186035156; Gradient Norm 0.2992413341999054; Learning Rate: 0.00020225196317872078; Time: 3264.500617980957 ms; Tokens/Sec : 160602.81842564454\n",
            "\n",
            "Training\n",
            "Step: 15702; Training Loss: 3.0333025455474854; Gradient Norm 0.2982470989227295; Learning Rate: 0.00020220519898299442; Time: 14093.762636184692 ms; Tokens/Sec : 37200.00212391327\n",
            "\n",
            "Training\n",
            "Step: 15703; Training Loss: 3.0359067916870117; Gradient Norm 0.3018389940261841; Learning Rate: 0.00020215844411335012; Time: 3242.457389831543 ms; Tokens/Sec : 161694.64605585413\n",
            "\n",
            "Training\n",
            "Step: 15704; Training Loss: 3.0777437686920166; Gradient Norm 0.29499369859695435; Learning Rate: 0.00020211169857104115; Time: 3249.02606010437 ms; Tokens/Sec : 161367.74230218332\n",
            "\n",
            "Training\n",
            "Step: 15705; Training Loss: 3.038756847381592; Gradient Norm 0.33156490325927734; Learning Rate: 0.0002020649623573213; Time: 3247.889757156372 ms; Tokens/Sec : 161424.19823357253\n",
            "\n",
            "Training\n",
            "Step: 15706; Training Loss: 3.0549449920654297; Gradient Norm 0.2845427095890045; Learning Rate: 0.00020201823547344382; Time: 3247.03311920166 ms; Tokens/Sec : 161466.78544778915\n",
            "\n",
            "Training\n",
            "Step: 15707; Training Loss: 3.017566442489624; Gradient Norm 0.3383013606071472; Learning Rate: 0.00020197151792066166; Time: 3246.2728023529053 ms; Tokens/Sec : 161504.60294649142\n",
            "\n",
            "Training\n",
            "Step: 15708; Training Loss: 3.035827398300171; Gradient Norm 0.29377278685569763; Learning Rate: 0.00020192480970022728; Time: 3241.0829067230225 ms; Tokens/Sec : 161763.21775430744\n",
            "\n",
            "Training\n",
            "Step: 15709; Training Loss: 3.0606775283813477; Gradient Norm 0.2923825979232788; Learning Rate: 0.00020187811081339346; Time: 3246.600866317749 ms; Tokens/Sec : 161488.28315771391\n",
            "\n",
            "Training\n",
            "Step: 15710; Training Loss: 3.037724494934082; Gradient Norm 0.31818893551826477; Learning Rate: 0.0002018314212614125; Time: 3248.7311363220215 ms; Tokens/Sec : 161382.39146301313\n",
            "\n",
            "Training\n",
            "Step: 15711; Training Loss: 3.1137161254882812; Gradient Norm 0.3822195827960968; Learning Rate: 0.00020178474104553607; Time: 3239.9585247039795 ms; Tokens/Sec : 161819.3554029837\n",
            "\n",
            "Training\n",
            "Step: 15712; Training Loss: 3.0117173194885254; Gradient Norm 0.32313090562820435; Learning Rate: 0.0002017380701670159; Time: 3248.4164237976074 ms; Tokens/Sec : 161398.02648426263\n",
            "\n",
            "Training\n",
            "Step: 15713; Training Loss: 3.011761426925659; Gradient Norm 0.3601328432559967; Learning Rate: 0.00020169140862710382; Time: 3251.4970302581787 ms; Tokens/Sec : 161245.11113527603\n",
            "\n",
            "Training\n",
            "Step: 15714; Training Loss: 3.027160406112671; Gradient Norm 0.3251243829727173; Learning Rate: 0.00020164475642705078; Time: 3252.6376247406006 ms; Tokens/Sec : 161188.56770643554\n",
            "\n",
            "Training\n",
            "Step: 15715; Training Loss: 3.043473720550537; Gradient Norm 0.33953946828842163; Learning Rate: 0.00020159811356810776; Time: 3249.171495437622 ms; Tokens/Sec : 161360.51936199356\n",
            "\n",
            "Training\n",
            "Step: 15716; Training Loss: 3.0275001525878906; Gradient Norm 0.31399816274642944; Learning Rate: 0.0002015514800515254; Time: 3249.1157054901123 ms; Tokens/Sec : 161363.29005276648\n",
            "\n",
            "Training\n",
            "Step: 15717; Training Loss: 2.9974236488342285; Gradient Norm 0.3188116252422333; Learning Rate: 0.00020150485587855453; Time: 3245.009660720825 ms; Tokens/Sec : 161567.46968930075\n",
            "\n",
            "Training\n",
            "Step: 15718; Training Loss: 3.047182083129883; Gradient Norm 0.3467269837856293; Learning Rate: 0.00020145824105044507; Time: 3249.1278648376465 ms; Tokens/Sec : 161362.6861761557\n",
            "\n",
            "Training\n",
            "Step: 15719; Training Loss: 3.0702836513519287; Gradient Norm 0.3480548858642578; Learning Rate: 0.00020141163556844707; Time: 3246.5004920959473 ms; Tokens/Sec : 161493.27599871042\n",
            "\n",
            "Training\n",
            "Step: 15720; Training Loss: 3.0376791954040527; Gradient Norm 0.3385898768901825; Learning Rate: 0.00020136503943381023; Time: 3249.2613792419434 ms; Tokens/Sec : 161356.05567143293\n",
            "\n",
            "Training\n",
            "Step: 15721; Training Loss: 3.0747382640838623; Gradient Norm 0.33102256059646606; Learning Rate: 0.00020131845264778403; Time: 3249.8855590820312 ms; Tokens/Sec : 161325.0652887886\n",
            "\n",
            "Training\n",
            "Step: 15722; Training Loss: 3.0993452072143555; Gradient Norm 0.34992650151252747; Learning Rate: 0.0002012718752116177; Time: 3247.878313064575 ms; Tokens/Sec : 161424.76702130557\n",
            "\n",
            "Training\n",
            "Step: 15723; Training Loss: 3.076746702194214; Gradient Norm 0.3793036639690399; Learning Rate: 0.0002012253071265602; Time: 3247.8814125061035 ms; Tokens/Sec : 161424.6129742321\n",
            "\n",
            "Training\n",
            "Step: 15724; Training Loss: 3.038912296295166; Gradient Norm 0.33064690232276917; Learning Rate: 0.0002011787483938603; Time: 3251.523971557617 ms; Tokens/Sec : 161243.7750993556\n",
            "\n",
            "Training\n",
            "Step: 15725; Training Loss: 3.0817813873291016; Gradient Norm 0.36605656147003174; Learning Rate: 0.0002011321990147664; Time: 3251.8110275268555 ms; Tokens/Sec : 161229.54118854317\n",
            "\n",
            "Training\n",
            "Step: 15726; Training Loss: 3.039752721786499; Gradient Norm 0.3466476798057556; Learning Rate: 0.00020108565899052674; Time: 3250.872850418091 ms; Tokens/Sec : 161276.07080436012\n",
            "\n",
            "Training\n",
            "Step: 15727; Training Loss: 3.087698459625244; Gradient Norm 0.39040616154670715; Learning Rate: 0.00020103912832238928; Time: 3249.737501144409 ms; Tokens/Sec : 161332.41525365348\n",
            "\n",
            "Training\n",
            "Step: 15728; Training Loss: 3.0592360496520996; Gradient Norm 0.3523150682449341; Learning Rate: 0.00020099260701160173; Time: 3252.5134086608887 ms; Tokens/Sec : 161194.72362632246\n",
            "\n",
            "Training\n",
            "Step: 15729; Training Loss: 3.0453040599823; Gradient Norm 0.31696197390556335; Learning Rate: 0.0002009460950594116; Time: 3250.9377002716064 ms; Tokens/Sec : 161272.85366194413\n",
            "\n",
            "Training\n",
            "Step: 15730; Training Loss: 3.033460855484009; Gradient Norm 0.3453812599182129; Learning Rate: 0.0002008995924670662; Time: 3255.715847015381 ms; Tokens/Sec : 161036.16674060535\n",
            "\n",
            "Training\n",
            "Step: 15731; Training Loss: 3.0141425132751465; Gradient Norm 0.304451584815979; Learning Rate: 0.0002008530992358121; Time: 3257.3788166046143 ms; Tokens/Sec : 160953.95393603644\n",
            "\n",
            "Training\n",
            "Step: 15732; Training Loss: 3.073078155517578; Gradient Norm 0.32990890741348267; Learning Rate: 0.00020080661536689637; Time: 3257.486581802368 ms; Tokens/Sec : 160948.62920660485\n",
            "\n",
            "Training\n",
            "Step: 15733; Training Loss: 3.0730974674224854; Gradient Norm 0.3586593568325043; Learning Rate: 0.00020076014086156538; Time: 3255.6684017181396 ms; Tokens/Sec : 161038.5135425074\n",
            "\n",
            "Training\n",
            "Step: 15734; Training Loss: 3.043016195297241; Gradient Norm 0.27794188261032104; Learning Rate: 0.00020071367572106538; Time: 3258.877754211426 ms; Tokens/Sec : 160879.92233598395\n",
            "\n",
            "Training\n",
            "Step: 15735; Training Loss: 3.0394582748413086; Gradient Norm 0.33491820096969604; Learning Rate: 0.00020066721994664222; Time: 3259.6189975738525 ms; Tokens/Sec : 160843.33794539474\n",
            "\n",
            "Training\n",
            "Step: 15736; Training Loss: 3.0773842334747314; Gradient Norm 0.34335777163505554; Learning Rate: 0.00020062077353954173; Time: 3260.8189582824707 ms; Tokens/Sec : 160784.14861650323\n",
            "\n",
            "Training\n",
            "Step: 15737; Training Loss: 3.0470821857452393; Gradient Norm 0.33160439133644104; Learning Rate: 0.00020057433650100937; Time: 3258.9354515075684 ms; Tokens/Sec : 160877.07406339294\n",
            "\n",
            "Training\n",
            "Step: 15738; Training Loss: 3.1158251762390137; Gradient Norm 0.3348742127418518; Learning Rate: 0.00020052790883229006; Time: 3258.4385871887207 ms; Tokens/Sec : 160901.60546875285\n",
            "\n",
            "Training\n",
            "Step: 15739; Training Loss: 3.0591909885406494; Gradient Norm 0.2998938262462616; Learning Rate: 0.00020048149053462905; Time: 3257.7691078186035 ms; Tokens/Sec : 160934.67113483138\n",
            "\n",
            "Training\n",
            "Step: 15740; Training Loss: 3.0376458168029785; Gradient Norm 0.2944261431694031; Learning Rate: 0.00020043508160927115; Time: 3259.0532302856445 ms; Tokens/Sec : 160871.26013405065\n",
            "\n",
            "Training\n",
            "Step: 15741; Training Loss: 3.039456605911255; Gradient Norm 0.29443249106407166; Learning Rate: 0.0002003886820574605; Time: 3257.7672004699707 ms; Tokens/Sec : 160934.7653584226\n",
            "\n",
            "Training\n",
            "Step: 15742; Training Loss: 3.029545783996582; Gradient Norm 0.3217965066432953; Learning Rate: 0.00020034229188044126; Time: 3258.007049560547 ms; Tokens/Sec : 160922.91760716666\n",
            "\n",
            "Training\n",
            "Step: 15743; Training Loss: 3.0548291206359863; Gradient Norm 0.2654743790626526; Learning Rate: 0.00020029591107945766; Time: 3256.8001747131348 ms; Tokens/Sec : 160982.5509316611\n",
            "\n",
            "Training\n",
            "Step: 15744; Training Loss: 3.0192737579345703; Gradient Norm 0.2807054817676544; Learning Rate: 0.0002002495396557534; Time: 3256.452798843384 ms; Tokens/Sec : 160999.7234371752\n",
            "\n",
            "Training\n",
            "Step: 15745; Training Loss: 3.043131113052368; Gradient Norm 0.32316386699676514; Learning Rate: 0.00020020317761057168; Time: 3259.5112323760986 ms; Tokens/Sec : 160848.6557102022\n",
            "\n",
            "Training\n",
            "Step: 15746; Training Loss: 3.0956203937530518; Gradient Norm 0.34766387939453125; Learning Rate: 0.0002001568249451557; Time: 3260.319232940674 ms; Tokens/Sec : 160808.79280251148\n",
            "\n",
            "Training\n",
            "Step: 15747; Training Loss: 3.0399277210235596; Gradient Norm 0.28609898686408997; Learning Rate: 0.0002001104816607488; Time: 3259.2947483062744 ms; Tokens/Sec : 160859.33936243464\n",
            "\n",
            "Training\n",
            "Step: 15748; Training Loss: 3.026580333709717; Gradient Norm 0.28959739208221436; Learning Rate: 0.00020006414775859324; Time: 3258.4612369537354 ms; Tokens/Sec : 160900.48703177008\n",
            "\n",
            "Training\n",
            "Step: 15749; Training Loss: 3.0061569213867188; Gradient Norm 0.2950528562068939; Learning Rate: 0.00020001782323993158; Time: 3257.6558589935303 ms; Tokens/Sec : 160940.26585177155\n",
            "\n",
            "Evaluating HellaSwag\n",
            "Evaluating Validation set\n",
            "Training\n",
            "Step: 15750; Training Loss: 3.0873208045959473; Gradient Norm 0.28290048241615295; Learning Rate: 0.00019997150810600593; Time: 3251.8019676208496 ms; Tokens/Sec : 161229.99039316972; Validation_loss: 2.9397850036621094; HellaSwag Acc: 0.327\n",
            "\n",
            "Checkpointing\n",
            "Training\n",
            "Step: 15751; Training Loss: 3.038623094558716; Gradient Norm 0.3158291280269623; Learning Rate: 0.00019992520235805858; Time: 3278.010845184326 ms; Tokens/Sec : 159940.89853919283\n",
            "\n",
            "Training\n",
            "Step: 15752; Training Loss: 2.9996211528778076; Gradient Norm 0.3045230209827423; Learning Rate: 0.0001998789059973308; Time: 3246.5591430664062 ms; Tokens/Sec : 161490.35852918576\n",
            "\n",
            "Training\n",
            "Step: 15753; Training Loss: 3.064087152481079; Gradient Norm 0.3000468611717224; Learning Rate: 0.00019983261902506422; Time: 3246.5100288391113 ms; Tokens/Sec : 161492.8016062452\n",
            "\n",
            "Training\n",
            "Step: 15754; Training Loss: 2.998551607131958; Gradient Norm 0.3335196375846863; Learning Rate: 0.00019978634144249995; Time: 3244.166135787964 ms; Tokens/Sec : 161609.47931005314\n",
            "\n",
            "Training\n",
            "Step: 15755; Training Loss: 3.0735433101654053; Gradient Norm 0.36186814308166504; Learning Rate: 0.000199740073250879; Time: 3240.220785140991 ms; Tokens/Sec : 161806.2578958448\n",
            "\n",
            "Training\n",
            "Step: 15756; Training Loss: 3.1065964698791504; Gradient Norm 0.3715530335903168; Learning Rate: 0.00019969381445144203; Time: 3246.2844848632812 ms; Tokens/Sec : 161504.02173458334\n",
            "\n",
            "Training\n",
            "Step: 15757; Training Loss: 3.0513315200805664; Gradient Norm 0.40147486329078674; Learning Rate: 0.00019964756504542937; Time: 3247.382640838623 ms; Tokens/Sec : 161449.4064871286\n",
            "\n",
            "Training\n",
            "Step: 15758; Training Loss: 3.0496413707733154; Gradient Norm 0.34469830989837646; Learning Rate: 0.0001996013250340813; Time: 3246.1142539978027 ms; Tokens/Sec : 161512.49123603856\n",
            "\n",
            "Training\n",
            "Step: 15759; Training Loss: 3.0203583240509033; Gradient Norm 0.3682781755924225; Learning Rate: 0.00019955509441863765; Time: 3248.1672763824463 ms; Tokens/Sec : 161410.40635810813\n",
            "\n",
            "Training\n",
            "Step: 15760; Training Loss: 3.028348207473755; Gradient Norm 0.3349524736404419; Learning Rate: 0.00019950887320033814; Time: 3248.4047412872314 ms; Tokens/Sec : 161398.60693351983\n",
            "\n",
            "Training\n",
            "Step: 15761; Training Loss: 3.107187509536743; Gradient Norm 0.37639331817626953; Learning Rate: 0.00019946266138042224; Time: 3241.9705390930176 ms; Tokens/Sec : 161718.9279414847\n",
            "\n",
            "Training\n",
            "Step: 15762; Training Loss: 3.0036633014678955; Gradient Norm 0.2935894727706909; Learning Rate: 0.00019941645896012902; Time: 3247.546672821045 ms; Tokens/Sec : 161441.25175715087\n",
            "\n",
            "Training\n",
            "Step: 15763; Training Loss: 3.047122001647949; Gradient Norm 0.31574174761772156; Learning Rate: 0.00019937026594069748; Time: 3246.066093444824 ms; Tokens/Sec : 161514.88753071247\n",
            "\n",
            "Training\n",
            "Step: 15764; Training Loss: 3.002302408218384; Gradient Norm 0.3440091609954834; Learning Rate: 0.0001993240823233663; Time: 3246.04868888855 ms; Tokens/Sec : 161515.75353588327\n",
            "\n",
            "Training\n",
            "Step: 15765; Training Loss: 3.0483646392822266; Gradient Norm 0.2828170657157898; Learning Rate: 0.0001992779081093736; Time: 3255.9168338775635 ms; Tokens/Sec : 161026.22602175333\n",
            "\n",
            "Training\n",
            "Step: 15766; Training Loss: 3.062204122543335; Gradient Norm 0.3313889503479004; Learning Rate: 0.0001992317432999579; Time: 3248.861074447632 ms; Tokens/Sec : 161375.93697789585\n",
            "\n",
            "Training\n",
            "Step: 15767; Training Loss: 3.0767409801483154; Gradient Norm 0.3283405303955078; Learning Rate: 0.00019918558789635708; Time: 3247.7188110351562 ms; Tokens/Sec : 161432.6949176034\n",
            "\n",
            "Training\n",
            "Step: 15768; Training Loss: 3.0682127475738525; Gradient Norm 0.2887839376926422; Learning Rate: 0.00019913944189980855; Time: 3251.628875732422 ms; Tokens/Sec : 161238.5730465336\n",
            "\n",
            "Training\n",
            "Step: 15769; Training Loss: 3.0237386226654053; Gradient Norm 0.2897847592830658; Learning Rate: 0.00019909330531154974; Time: 3249.7010231018066 ms; Tokens/Sec : 161334.2262173929\n",
            "\n",
            "Training\n",
            "Step: 15770; Training Loss: 3.0695555210113525; Gradient Norm 0.29175812005996704; Learning Rate: 0.000199047178132818; Time: 3248.405933380127 ms; Tokens/Sec : 161398.5477038125\n",
            "\n",
            "Training\n",
            "Step: 15771; Training Loss: 3.194225549697876; Gradient Norm 0.38110584020614624; Learning Rate: 0.00019900106036485022; Time: 3241.089105606079 ms; Tokens/Sec : 161762.90836717334\n",
            "\n",
            "Training\n",
            "Step: 15772; Training Loss: 3.0916459560394287; Gradient Norm 0.2944246530532837; Learning Rate: 0.00019895495200888285; Time: 3246.281623840332 ms; Tokens/Sec : 161504.16407180668\n",
            "\n",
            "Training\n",
            "Step: 15773; Training Loss: 3.071826934814453; Gradient Norm 0.3427821099758148; Learning Rate: 0.00019890885306615225; Time: 3248.373508453369 ms; Tokens/Sec : 161400.15876734152\n",
            "\n",
            "Training\n",
            "Step: 15774; Training Loss: 3.0712532997131348; Gradient Norm 0.34324315190315247; Learning Rate: 0.00019886276353789495; Time: 3253.1607151031494 ms; Tokens/Sec : 161162.64947069366\n",
            "\n",
            "Training\n",
            "Step: 15775; Training Loss: 3.03896164894104; Gradient Norm 0.2727465033531189; Learning Rate: 0.00019881668342534647; Time: 3250.178575515747 ms; Tokens/Sec : 161310.52119707133\n",
            "\n",
            "Training\n",
            "Step: 15776; Training Loss: 3.0173234939575195; Gradient Norm 0.3408365249633789; Learning Rate: 0.00019877061272974248; Time: 3247.217893600464 ms; Tokens/Sec : 161457.59760478462\n",
            "\n",
            "Training\n",
            "Step: 15777; Training Loss: 3.048581600189209; Gradient Norm 0.3361873924732208; Learning Rate: 0.00019872455145231827; Time: 3244.2479133605957 ms; Tokens/Sec : 161605.4056290999\n",
            "\n",
            "Training\n",
            "Step: 15778; Training Loss: 3.0300488471984863; Gradient Norm 0.3003578782081604; Learning Rate: 0.00019867849959430946; Time: 3247.5948333740234 ms; Tokens/Sec : 161438.85764693792\n",
            "\n",
            "Training\n",
            "Step: 15779; Training Loss: 3.147127389907837; Gradient Norm 0.35526344180107117; Learning Rate: 0.00019863245715695042; Time: 3257.333755493164 ms; Tokens/Sec : 160956.18053134446\n",
            "\n",
            "Training\n",
            "Step: 15780; Training Loss: 3.030475616455078; Gradient Norm 0.2995809316635132; Learning Rate: 0.00019858642414147582; Time: 3256.192445755005 ms; Tokens/Sec : 161012.59637878518\n",
            "\n",
            "Training\n",
            "Step: 15781; Training Loss: 3.031468629837036; Gradient Norm 0.3507081866264343; Learning Rate: 0.00019854040054912044; Time: 3255.0482749938965 ms; Tokens/Sec : 161069.19335965396\n",
            "\n",
            "Training\n",
            "Step: 15782; Training Loss: 2.9996085166931152; Gradient Norm 0.28768154978752136; Learning Rate: 0.00019849438638111792; Time: 3257.3790550231934 ms; Tokens/Sec : 160953.94215527273\n",
            "\n",
            "Training\n",
            "Step: 15783; Training Loss: 3.0263733863830566; Gradient Norm 0.33571138978004456; Learning Rate: 0.00019844838163870235; Time: 3255.2030086517334 ms; Tokens/Sec : 161061.53705515095\n",
            "\n",
            "Training\n",
            "Step: 15784; Training Loss: 3.088347911834717; Gradient Norm 0.3090667724609375; Learning Rate: 0.00019840238632310732; Time: 3264.732837677002 ms; Tokens/Sec : 160591.39478409925\n",
            "\n",
            "Training\n",
            "Step: 15785; Training Loss: 3.0634329319000244; Gradient Norm 0.340609610080719; Learning Rate: 0.00019835640043556617; Time: 3256.4473152160645 ms; Tokens/Sec : 160999.99454934022\n",
            "\n",
            "Training\n",
            "Step: 15786; Training Loss: 3.0531537532806396; Gradient Norm 0.3892080783843994; Learning Rate: 0.00019831042397731202; Time: 3256.150245666504 ms; Tokens/Sec : 161014.6831208899\n",
            "\n",
            "Training\n",
            "Step: 15787; Training Loss: 3.0252084732055664; Gradient Norm 0.35918790102005005; Learning Rate: 0.00019826445694957776; Time: 3258.3577632904053 ms; Tokens/Sec : 160905.5966495697\n",
            "\n",
            "Training\n",
            "Step: 15788; Training Loss: 3.064565658569336; Gradient Norm 0.31214529275894165; Learning Rate: 0.00019821849935359589; Time: 3259.7012519836426 ms; Tokens/Sec : 160839.27926859935\n",
            "\n",
            "Training\n",
            "Step: 15789; Training Loss: 3.1390676498413086; Gradient Norm 0.3648794889450073; Learning Rate: 0.00019817255119059882; Time: 3259.6328258514404 ms; Tokens/Sec : 160842.65560279848\n",
            "\n",
            "Training\n",
            "Step: 15790; Training Loss: 3.045367956161499; Gradient Norm 0.4399881064891815; Learning Rate: 0.00019812661246181864; Time: 3256.0670375823975 ms; Tokens/Sec : 161018.79781605463\n",
            "\n",
            "Training\n",
            "Step: 15791; Training Loss: 3.129481792449951; Gradient Norm 0.3510354161262512; Learning Rate: 0.0001980806831684872; Time: 3257.4050426483154 ms; Tokens/Sec : 160952.6580623657\n",
            "\n",
            "Training\n",
            "Step: 15792; Training Loss: 3.1001412868499756; Gradient Norm 0.3767613172531128; Learning Rate: 0.00019803476331183608; Time: 3260.037899017334 ms; Tokens/Sec : 160822.6702389058\n",
            "\n",
            "Training\n",
            "Step: 15793; Training Loss: 3.101616859436035; Gradient Norm 0.40631556510925293; Learning Rate: 0.00019798885289309666; Time: 3256.5524578094482 ms; Tokens/Sec : 160994.79642734435\n",
            "\n",
            "Training\n",
            "Step: 15794; Training Loss: 3.0973587036132812; Gradient Norm 0.3479676842689514; Learning Rate: 0.00019794295191349998; Time: 3260.157346725464 ms; Tokens/Sec : 160816.77791613352\n",
            "\n",
            "Training\n",
            "Step: 15795; Training Loss: 3.102837085723877; Gradient Norm 0.33919379115104675; Learning Rate: 0.00019789706037427683; Time: 3257.7805519104004 ms; Tokens/Sec : 160934.10579560106\n",
            "\n",
            "Training\n",
            "Step: 15796; Training Loss: 3.11552095413208; Gradient Norm 0.3106399476528168; Learning Rate: 0.0001978511782766579; Time: 3258.3768367767334 ms; Tokens/Sec : 160904.6547601408\n",
            "\n",
            "Training\n",
            "Step: 15797; Training Loss: 3.1124110221862793; Gradient Norm 0.31855955719947815; Learning Rate: 0.00019780530562187336; Time: 3259.145736694336 ms; Tokens/Sec : 160866.6940226402\n",
            "\n",
            "Training\n",
            "Step: 15798; Training Loss: 3.0731279850006104; Gradient Norm 0.3552578091621399; Learning Rate: 0.00019775944241115357; Time: 3260.3461742401123 ms; Tokens/Sec : 160807.46398722386\n",
            "\n",
            "Training\n",
            "Step: 15799; Training Loss: 3.0365748405456543; Gradient Norm 0.3782893717288971; Learning Rate: 0.00019771358864572786; Time: 3260.1754665374756 ms; Tokens/Sec : 160815.8841084799\n",
            "\n",
            "Training\n",
            "Step: 15800; Training Loss: 3.0360138416290283; Gradient Norm 0.32652896642684937; Learning Rate: 0.00019766774432682617; Time: 3259.1772079467773 ms; Tokens/Sec : 160865.14066238576\n",
            "\n",
            "Training\n",
            "Step: 15801; Training Loss: 3.0424115657806396; Gradient Norm 0.31994181871414185; Learning Rate: 0.00019762190945567785; Time: 3257.66658782959 ms; Tokens/Sec : 160939.73580927606\n",
            "\n",
            "Training\n",
            "Step: 15802; Training Loss: 3.064354658126831; Gradient Norm 0.3134349584579468; Learning Rate: 0.00019757608403351174; Time: 3261.1021995544434 ms; Tokens/Sec : 160770.1837960283\n",
            "\n",
            "Training\n",
            "Step: 15803; Training Loss: 3.090791702270508; Gradient Norm 0.27372732758522034; Learning Rate: 0.0001975302680615566; Time: 3259.7029209136963 ms; Tokens/Sec : 160839.19692075552\n",
            "\n",
            "Training\n",
            "Step: 15804; Training Loss: 3.0507326126098633; Gradient Norm 0.32000893354415894; Learning Rate: 0.00019748446154104122; Time: 3261.6636753082275 ms; Tokens/Sec : 160742.50817735054\n",
            "\n",
            "Training\n",
            "Step: 15805; Training Loss: 3.071187734603882; Gradient Norm 0.2861796021461487; Learning Rate: 0.00019743866447319392; Time: 3261.0714435577393 ms; Tokens/Sec : 160771.70006064518\n",
            "\n",
            "Training\n",
            "Step: 15806; Training Loss: 3.058544874191284; Gradient Norm 0.296385258436203; Learning Rate: 0.00019739287685924245; Time: 3259.5973014831543 ms; Tokens/Sec : 160844.40852906674\n",
            "\n",
            "Training\n",
            "Step: 15807; Training Loss: 3.060523509979248; Gradient Norm 0.2869824469089508; Learning Rate: 0.0001973470987004147; Time: 3259.4680786132812 ms; Tokens/Sec : 160850.78526771607\n",
            "\n",
            "Training\n",
            "Step: 15808; Training Loss: 3.078158378601074; Gradient Norm 0.30631858110427856; Learning Rate: 0.00019730132999793847; Time: 3258.7854862213135 ms; Tokens/Sec : 160884.47742779535\n",
            "\n",
            "Training\n",
            "Step: 15809; Training Loss: 3.0628743171691895; Gradient Norm 0.27764302492141724; Learning Rate: 0.00019725557075304072; Time: 3258.4054470062256 ms; Tokens/Sec : 160903.24194667302\n",
            "\n",
            "Training\n",
            "Step: 15810; Training Loss: 3.0222179889678955; Gradient Norm 0.2842210829257965; Learning Rate: 0.0001972098209669486; Time: 3260.5013847351074 ms; Tokens/Sec : 160799.8090277133\n",
            "\n",
            "Training\n",
            "Step: 15811; Training Loss: 2.9992990493774414; Gradient Norm 0.28814101219177246; Learning Rate: 0.00019716408064088893; Time: 3261.2810134887695 ms; Tokens/Sec : 160761.3688705533\n",
            "\n",
            "Training\n",
            "Step: 15812; Training Loss: 3.012378692626953; Gradient Norm 0.2806314527988434; Learning Rate: 0.00019711834977608814; Time: 3259.7081661224365 ms; Tokens/Sec : 160838.9381137954\n",
            "\n",
            "Training\n",
            "Step: 15813; Training Loss: 3.1781790256500244; Gradient Norm 0.3648807108402252; Learning Rate: 0.00019707262837377257; Time: 3260.8437538146973 ms; Tokens/Sec : 160782.92601007386\n",
            "\n",
            "Training\n",
            "Step: 15814; Training Loss: 3.048391580581665; Gradient Norm 0.3585159480571747; Learning Rate: 0.0001970269164351682; Time: 3260.759115219116 ms; Tokens/Sec : 160787.09940668798\n",
            "\n",
            "Training\n",
            "Step: 15815; Training Loss: 3.089183807373047; Gradient Norm 0.29933279752731323; Learning Rate: 0.00019698121396150077; Time: 3261.66033744812 ms; Tokens/Sec : 160742.67267516765\n",
            "\n",
            "Training\n",
            "Step: 15816; Training Loss: 3.043163537979126; Gradient Norm 0.3132372200489044; Learning Rate: 0.00019693552095399584; Time: 3261.0371112823486 ms; Tokens/Sec : 160773.39266888393\n",
            "\n",
            "Training\n",
            "Step: 15817; Training Loss: 3.0324296951293945; Gradient Norm 0.35342657566070557; Learning Rate: 0.0001968898374138787; Time: 3258.5155963897705 ms; Tokens/Sec : 160897.80284644885\n",
            "\n",
            "Training\n",
            "Step: 15818; Training Loss: 3.091883897781372; Gradient Norm 0.31557682156562805; Learning Rate: 0.00019684416334237424; Time: 3260.3893280029297 ms; Tokens/Sec : 160805.33557663788\n",
            "\n",
            "Training\n",
            "Step: 15819; Training Loss: 3.0009725093841553; Gradient Norm 0.3001243472099304; Learning Rate: 0.00019679849874070724; Time: 3257.9116821289062 ms; Tokens/Sec : 160927.62823374025\n",
            "\n",
            "Training\n",
            "Step: 15820; Training Loss: 3.0510594844818115; Gradient Norm 0.3098325729370117; Learning Rate: 0.00019675284361010223; Time: 3257.957696914673 ms; Tokens/Sec : 160925.35532198817\n",
            "\n",
            "Training\n",
            "Step: 15821; Training Loss: 2.9819347858428955; Gradient Norm 0.28907814621925354; Learning Rate: 0.00019670719795178342; Time: 3261.5766525268555 ms; Tokens/Sec : 160746.79698047755\n",
            "\n",
            "Training\n",
            "Step: 15822; Training Loss: 2.993370294570923; Gradient Norm 0.29984813928604126; Learning Rate: 0.00019666156176697478; Time: 3262.580156326294 ms; Tokens/Sec : 160697.35451047274\n",
            "\n",
            "Training\n",
            "Step: 15823; Training Loss: 3.027499198913574; Gradient Norm 0.3205949664115906; Learning Rate: 0.0001966159350569001; Time: 3260.589361190796 ms; Tokens/Sec : 160795.47036506474\n",
            "\n",
            "Training\n",
            "Step: 15824; Training Loss: 3.0831902027130127; Gradient Norm 0.38054198026657104; Learning Rate: 0.00019657031782278282; Time: 3261.1467838287354 ms; Tokens/Sec : 160767.98585081226\n",
            "\n",
            "Training\n",
            "Step: 15825; Training Loss: 3.0651822090148926; Gradient Norm 0.3173743784427643; Learning Rate: 0.00019652471006584624; Time: 3261.054515838623 ms; Tokens/Sec : 160772.53460608658\n",
            "\n",
            "Training\n",
            "Step: 15826; Training Loss: 3.1084282398223877; Gradient Norm 0.3363170623779297; Learning Rate: 0.00019647911178731302; Time: 3260.0667476654053 ms; Tokens/Sec : 160821.24710343813\n",
            "\n",
            "Training\n",
            "Step: 15827; Training Loss: 3.0783674716949463; Gradient Norm 0.36675888299942017; Learning Rate: 0.00019643352298840625; Time: 3261.049270629883 ms; Tokens/Sec : 160772.7931993901\n",
            "\n",
            "Training\n",
            "Step: 15828; Training Loss: 3.1710431575775146; Gradient Norm 0.3176397681236267; Learning Rate: 0.0001963879436703483; Time: 3258.0206394195557 ms; Tokens/Sec : 160922.24636532887\n",
            "\n",
            "Training\n",
            "Step: 15829; Training Loss: 3.0222952365875244; Gradient Norm 0.33445996046066284; Learning Rate: 0.0001963423738343612; Time: 3260.3237628936768 ms; Tokens/Sec : 160808.5693718565\n",
            "\n",
            "Training\n",
            "Step: 15830; Training Loss: 3.087024688720703; Gradient Norm 0.33785924315452576; Learning Rate: 0.00019629681348166695; Time: 3263.2248401641846 ms; Tokens/Sec : 160665.60708505186\n",
            "\n",
            "Training\n",
            "Step: 15831; Training Loss: 3.0890212059020996; Gradient Norm 0.29724133014678955; Learning Rate: 0.00019625126261348737; Time: 3261.5294456481934 ms; Tokens/Sec : 160749.12360504642\n",
            "\n",
            "Training\n",
            "Step: 15832; Training Loss: 3.0895273685455322; Gradient Norm 0.3639470934867859; Learning Rate: 0.00019620572123104396; Time: 3260.0061893463135 ms; Tokens/Sec : 160824.2345408334\n",
            "\n",
            "Training\n",
            "Step: 15833; Training Loss: 3.0707366466522217; Gradient Norm 0.32942789793014526; Learning Rate: 0.00019616018933555756; Time: 3260.681629180908 ms; Tokens/Sec : 160790.9203118682\n",
            "\n",
            "Training\n",
            "Step: 15834; Training Loss: 3.1040303707122803; Gradient Norm 0.3457334637641907; Learning Rate: 0.00019611466692824944; Time: 3257.7433586120605 ms; Tokens/Sec : 160935.94316262205\n",
            "\n",
            "Training\n",
            "Step: 15835; Training Loss: 3.0512523651123047; Gradient Norm 0.31454750895500183; Learning Rate: 0.00019606915401034028; Time: 3259.5832347869873 ms; Tokens/Sec : 160845.10265137072\n",
            "\n",
            "Training\n",
            "Step: 15836; Training Loss: 3.042625904083252; Gradient Norm 0.282555490732193; Learning Rate: 0.00019602365058305035; Time: 3259.1304779052734 ms; Tokens/Sec : 160867.44717780472\n",
            "\n",
            "Training\n",
            "Step: 15837; Training Loss: 3.066884994506836; Gradient Norm 0.2980995178222656; Learning Rate: 0.00019597815664759965; Time: 3258.9094638824463 ms; Tokens/Sec : 160878.35695054824\n",
            "\n",
            "Training\n",
            "Step: 15838; Training Loss: 3.0671281814575195; Gradient Norm 0.3151664435863495; Learning Rate: 0.00019593267220520847; Time: 3261.0437870025635 ms; Tokens/Sec : 160773.06354782407\n",
            "\n",
            "Training\n",
            "Step: 15839; Training Loss: 3.0781259536743164; Gradient Norm 0.2790434658527374; Learning Rate: 0.00019588719725709646; Time: 3259.673833847046 ms; Tokens/Sec : 160840.63213810528\n",
            "\n",
            "Training\n",
            "Step: 15840; Training Loss: 3.059358596801758; Gradient Norm 0.28707069158554077; Learning Rate: 0.0001958417318044827; Time: 3260.09464263916 ms; Tokens/Sec : 160819.87103772257\n",
            "\n",
            "Training\n",
            "Step: 15841; Training Loss: 3.0433766841888428; Gradient Norm 0.29644519090652466; Learning Rate: 0.0001957962758485864; Time: 3277.5237560272217 ms; Tokens/Sec : 159964.66815407746\n",
            "\n",
            "Training\n",
            "Step: 15842; Training Loss: 3.063547134399414; Gradient Norm 0.35228708386421204; Learning Rate: 0.00019575082939062686; Time: 3261.090040206909 ms; Tokens/Sec : 160770.78324606305\n",
            "\n",
            "Training\n",
            "Step: 15843; Training Loss: 3.0734803676605225; Gradient Norm 0.31984710693359375; Learning Rate: 0.0001957053924318223; Time: 3261.9214057922363 ms; Tokens/Sec : 160729.80761247495\n",
            "\n",
            "Training\n",
            "Step: 15844; Training Loss: 3.087390422821045; Gradient Norm 0.3351871967315674; Learning Rate: 0.00019565996497339117; Time: 3258.821487426758 ms; Tokens/Sec : 160882.70008738348\n",
            "\n",
            "Training\n",
            "Step: 15845; Training Loss: 3.0758464336395264; Gradient Norm 0.28612270951271057; Learning Rate: 0.00019561454701655173; Time: 3260.0185871124268 ms; Tokens/Sec : 160823.62293044163\n",
            "\n",
            "Training\n",
            "Step: 15846; Training Loss: 3.015963554382324; Gradient Norm 0.32812872529029846; Learning Rate: 0.0001955691385625218; Time: 3263.7336254119873 ms; Tokens/Sec : 160640.56083431692\n",
            "\n",
            "Training\n",
            "Step: 15847; Training Loss: 3.066742181777954; Gradient Norm 0.3116055428981781; Learning Rate: 0.00019552373961251903; Time: 3262.557029724121 ms; Tokens/Sec : 160698.4936120284\n",
            "\n",
            "Training\n",
            "Step: 15848; Training Loss: 3.057404041290283; Gradient Norm 0.3154732584953308; Learning Rate: 0.00019547835016776074; Time: 3262.57586479187 ms; Tokens/Sec : 160697.56588892255\n",
            "\n",
            "Training\n",
            "Step: 15849; Training Loss: 3.018165111541748; Gradient Norm 0.2833010256290436; Learning Rate: 0.00019543297022946407; Time: 3260.915517807007 ms; Tokens/Sec : 160779.38760970664\n",
            "\n",
            "Training\n",
            "Step: 15850; Training Loss: 3.004049301147461; Gradient Norm 0.3337021470069885; Learning Rate: 0.00019538759979884584; Time: 3262.444257736206 ms; Tokens/Sec : 160704.0484314055\n",
            "\n",
            "Training\n",
            "Step: 15851; Training Loss: 3.092817544937134; Gradient Norm 0.27851155400276184; Learning Rate: 0.00019534223887712274; Time: 3260.4944705963135 ms; Tokens/Sec : 160800.15001654418\n",
            "\n",
            "Training\n",
            "Step: 15852; Training Loss: 3.012195587158203; Gradient Norm 0.2875567078590393; Learning Rate: 0.00019529688746551104; Time: 3260.7312202453613 ms; Tokens/Sec : 160788.4749116331\n",
            "\n",
            "Training\n",
            "Step: 15853; Training Loss: 3.0653769969940186; Gradient Norm 0.280712753534317; Learning Rate: 0.0001952515455652269; Time: 3260.7340812683105 ms; Tokens/Sec : 160788.33383311972\n",
            "\n",
            "Training\n",
            "Step: 15854; Training Loss: 3.0875658988952637; Gradient Norm 0.33211973309516907; Learning Rate: 0.0001952062131774861; Time: 3260.261297225952 ms; Tokens/Sec : 160811.6504177439\n",
            "\n",
            "Training\n",
            "Step: 15855; Training Loss: 3.0272209644317627; Gradient Norm 0.30372950434684753; Learning Rate: 0.00019516089030350434; Time: 3262.524366378784 ms; Tokens/Sec : 160700.10247369576\n",
            "\n",
            "Training\n",
            "Step: 15856; Training Loss: 3.0067625045776367; Gradient Norm 0.28613296151161194; Learning Rate: 0.0001951155769444966; Time: 3260.540723800659 ms; Tokens/Sec : 160797.86894636977\n",
            "\n",
            "Training\n",
            "Step: 15857; Training Loss: 3.030585289001465; Gradient Norm 0.33830007910728455; Learning Rate: 0.00019507027310167836; Time: 3258.1450939178467 ms; Tokens/Sec : 160916.09946368454\n",
            "\n",
            "Training\n",
            "Step: 15858; Training Loss: 3.198749542236328; Gradient Norm 0.4190768003463745; Learning Rate: 0.00019502497877626427; Time: 3261.218547821045 ms; Tokens/Sec : 160764.4481079928\n",
            "\n",
            "Training\n",
            "Step: 15859; Training Loss: 3.0015556812286377; Gradient Norm 0.4089661240577698; Learning Rate: 0.000194979693969469; Time: 3262.564182281494 ms; Tokens/Sec : 160698.14131085327\n",
            "\n",
            "Training\n",
            "Step: 15860; Training Loss: 3.0643086433410645; Gradient Norm 0.3317980468273163; Learning Rate: 0.00019493441868250654; Time: 3284.7447395324707 ms; Tokens/Sec : 159613.0115348396\n",
            "\n",
            "Training\n",
            "Step: 15861; Training Loss: 3.092902183532715; Gradient Norm 0.38957950472831726; Learning Rate: 0.00019488915291659132; Time: 3257.507085800171 ms; Tokens/Sec : 160947.61613426066\n",
            "\n",
            "Training\n",
            "Step: 15862; Training Loss: 3.167210817337036; Gradient Norm 0.48571932315826416; Learning Rate: 0.0001948438966729371; Time: 3260.361909866333 ms; Tokens/Sec : 160806.68787517963\n",
            "\n",
            "Training\n",
            "Step: 15863; Training Loss: 3.063758134841919; Gradient Norm 0.3579525351524353; Learning Rate: 0.0001947986499527571; Time: 3259.678363800049 ms; Tokens/Sec : 160840.40861896527\n",
            "\n",
            "Training\n",
            "Step: 15864; Training Loss: 3.0879058837890625; Gradient Norm 0.3293576240539551; Learning Rate: 0.00019475341275726477; Time: 3262.2804641723633 ms; Tokens/Sec : 160712.1171088554\n",
            "\n",
            "Training\n",
            "Step: 15865; Training Loss: 3.090287685394287; Gradient Norm 0.37693312764167786; Learning Rate: 0.0001947081850876733; Time: 3258.5551738739014 ms; Tokens/Sec : 160895.84862750853\n",
            "\n",
            "Training\n",
            "Step: 15866; Training Loss: 3.0611443519592285; Gradient Norm 0.3970988690853119; Learning Rate: 0.00019466296694519542; Time: 3261.326313018799 ms; Tokens/Sec : 160759.13590955592\n",
            "\n",
            "Training\n",
            "Step: 15867; Training Loss: 3.0331549644470215; Gradient Norm 0.3355209231376648; Learning Rate: 0.00019461775833104348; Time: 3260.3209018707275 ms; Tokens/Sec : 160808.71048588215\n",
            "\n",
            "Training\n",
            "Step: 15868; Training Loss: 3.0905258655548096; Gradient Norm 0.39897191524505615; Learning Rate: 0.00019457255924642966; Time: 3259.7031593322754 ms; Tokens/Sec : 160839.18515678472\n",
            "\n",
            "Training\n",
            "Step: 15869; Training Loss: 3.032432794570923; Gradient Norm 0.3080289661884308; Learning Rate: 0.0001945273696925665; Time: 3261.6477012634277 ms; Tokens/Sec : 160743.29541995368\n",
            "\n",
            "Training\n",
            "Step: 15870; Training Loss: 3.0661962032318115; Gradient Norm 0.3763599395751953; Learning Rate: 0.00019448218967066519; Time: 3261.8484497070312 ms; Tokens/Sec : 160733.40257334453\n",
            "\n",
            "Training\n",
            "Step: 15871; Training Loss: 3.050166130065918; Gradient Norm 0.28947967290878296; Learning Rate: 0.00019443701918193745; Time: 3261.059284210205 ms; Tokens/Sec : 160772.29952198712\n",
            "\n",
            "Training\n",
            "Step: 15872; Training Loss: 3.057891845703125; Gradient Norm 0.2896195352077484; Learning Rate: 0.00019439185822759454; Time: 3259.6523761749268 ms; Tokens/Sec : 160841.69092142006\n",
            "\n",
            "Training\n",
            "Step: 15873; Training Loss: 3.050732374191284; Gradient Norm 0.3096938133239746; Learning Rate: 0.00019434670680884742; Time: 3260.934352874756 ms; Tokens/Sec : 160778.45895235555\n",
            "\n",
            "Training\n",
            "Step: 15874; Training Loss: 3.0626211166381836; Gradient Norm 0.29116737842559814; Learning Rate: 0.0001943015649269068; Time: 3259.490489959717 ms; Tokens/Sec : 160849.67930263223\n",
            "\n",
            "Training\n",
            "Step: 15875; Training Loss: 3.072429656982422; Gradient Norm 0.3225259482860565; Learning Rate: 0.00019425643258298318; Time: 3260.1795196533203 ms; Tokens/Sec : 160815.68417918027\n",
            "\n",
            "Training\n",
            "Step: 15876; Training Loss: 3.0548501014709473; Gradient Norm 0.28757524490356445; Learning Rate: 0.00019421130977828675; Time: 3263.5345458984375 ms; Tokens/Sec : 160650.36010080465\n",
            "\n",
            "Training\n",
            "Step: 15877; Training Loss: 3.066340446472168; Gradient Norm 0.26777687668800354; Learning Rate: 0.0001941661965140275; Time: 3260.2860927581787 ms; Tokens/Sec : 160810.42739303166\n",
            "\n",
            "Training\n",
            "Step: 15878; Training Loss: 3.0722429752349854; Gradient Norm 0.3325146436691284; Learning Rate: 0.00019412109279141516; Time: 3270.592451095581 ms; Tokens/Sec : 160303.6782599967\n",
            "\n",
            "Training\n",
            "Step: 15879; Training Loss: 3.0576412677764893; Gradient Norm 0.2693410813808441; Learning Rate: 0.00019407599861165912; Time: 3259.4635486602783 ms; Tokens/Sec : 160851.00881569777\n",
            "\n",
            "Training\n",
            "Step: 15880; Training Loss: 3.0409884452819824; Gradient Norm 0.36188504099845886; Learning Rate: 0.00019403091397596868; Time: 3259.4408988952637 ms; Tokens/Sec : 160852.12656492688\n",
            "\n",
            "Training\n",
            "Step: 15881; Training Loss: 3.038836717605591; Gradient Norm 0.2932930588722229; Learning Rate: 0.00019398583888555264; Time: 3260.9453201293945 ms; Tokens/Sec : 160777.91822010564\n",
            "\n",
            "Training\n",
            "Step: 15882; Training Loss: 3.01951265335083; Gradient Norm 0.30831989645957947; Learning Rate: 0.00019394077334161978; Time: 3259.9573135375977 ms; Tokens/Sec : 160826.64574250515\n",
            "\n",
            "Training\n",
            "Step: 15883; Training Loss: 3.005162239074707; Gradient Norm 0.3203376233577728; Learning Rate: 0.00019389571734537846; Time: 3258.556842803955 ms; Tokens/Sec : 160895.76622172885\n",
            "\n",
            "Training\n",
            "Step: 15884; Training Loss: 3.069628953933716; Gradient Norm 0.2789771854877472; Learning Rate: 0.00019385067089803696; Time: 3258.97216796875 ms; Tokens/Sec : 160875.26157879952\n",
            "\n",
            "Training\n",
            "Step: 15885; Training Loss: 3.0081470012664795; Gradient Norm 0.3535518944263458; Learning Rate: 0.0001938056340008031; Time: 3258.3515644073486 ms; Tokens/Sec : 160905.90276600834\n",
            "\n",
            "Training\n",
            "Step: 15886; Training Loss: 3.0638859272003174; Gradient Norm 0.26027798652648926; Learning Rate: 0.00019376060665488465; Time: 3260.9314918518066 ms; Tokens/Sec : 160778.60001354065\n",
            "\n",
            "Training\n",
            "Step: 15887; Training Loss: 3.0696675777435303; Gradient Norm 0.32260948419570923; Learning Rate: 0.00019371558886148877; Time: 3259.5744132995605 ms; Tokens/Sec : 160845.53795146538\n",
            "\n",
            "Training\n",
            "Step: 15888; Training Loss: 3.0049285888671875; Gradient Norm 0.34770965576171875; Learning Rate: 0.00019367058062182282; Time: 3259.6595287323 ms; Tokens/Sec : 160841.3379920996\n",
            "\n",
            "Training\n",
            "Step: 15889; Training Loss: 3.026745557785034; Gradient Norm 0.33524060249328613; Learning Rate: 0.0001936255819370938; Time: 3258.4996223449707 ms; Tokens/Sec : 160898.59161091372\n",
            "\n",
            "Training\n",
            "Step: 15890; Training Loss: 3.0189208984375; Gradient Norm 0.28226253390312195; Learning Rate: 0.00019358059280850793; Time: 3260.8654499053955 ms; Tokens/Sec : 160781.85624470052\n",
            "\n",
            "Training\n",
            "Step: 15891; Training Loss: 3.032238721847534; Gradient Norm 0.3729293942451477; Learning Rate: 0.00019353561323727197; Time: 3258.2015991210938 ms; Tokens/Sec : 160913.30878403218\n",
            "\n",
            "Training\n",
            "Step: 15892; Training Loss: 3.0638246536254883; Gradient Norm 0.3815472424030304; Learning Rate: 0.00019349064322459196; Time: 3261.683225631714 ms; Tokens/Sec : 160741.5446968972\n",
            "\n",
            "Training\n",
            "Step: 15893; Training Loss: 3.074784517288208; Gradient Norm 0.3209024965763092; Learning Rate: 0.00019344568277167382; Time: 3259.176731109619 ms; Tokens/Sec : 160865.16419792335\n",
            "\n",
            "Training\n",
            "Step: 15894; Training Loss: 3.016763210296631; Gradient Norm 0.39222851395606995; Learning Rate: 0.0001934007318797228; Time: 3260.8213424682617 ms; Tokens/Sec : 160784.03105738474\n",
            "\n",
            "Training\n",
            "Step: 15895; Training Loss: 3.0792882442474365; Gradient Norm 0.32612189650535583; Learning Rate: 0.00019335579054994463; Time: 17233.461618423462 ms; Tokens/Sec : 30422.674887296525\n",
            "\n",
            "Training\n",
            "Step: 15896; Training Loss: 3.1174135208129883; Gradient Norm 0.38112929463386536; Learning Rate: 0.00019331085878354447; Time: 3247.380018234253 ms; Tokens/Sec : 161449.5368746769\n",
            "\n",
            "Training\n",
            "Step: 15897; Training Loss: 3.0985939502716064; Gradient Norm 0.27772876620292664; Learning Rate: 0.0001932659365817268; Time: 3248.643159866333 ms; Tokens/Sec : 161386.76185708624\n",
            "\n",
            "Training\n",
            "Step: 15898; Training Loss: 3.062079668045044; Gradient Norm 0.34720873832702637; Learning Rate: 0.00019322102394569632; Time: 3250.7195472717285 ms; Tokens/Sec : 161283.6765447901\n",
            "\n",
            "Training\n",
            "Step: 15899; Training Loss: 3.09531831741333; Gradient Norm 0.32149016857147217; Learning Rate: 0.00019317612087665762; Time: 3254.3606758117676 ms; Tokens/Sec : 161103.22494270603\n",
            "\n",
            "Training\n",
            "Step: 15900; Training Loss: 3.0583229064941406; Gradient Norm 0.30437517166137695; Learning Rate: 0.00019313122737581455; Time: 3252.472400665283 ms; Tokens/Sec : 161196.75601021503\n",
            "\n",
            "Training\n",
            "Step: 15901; Training Loss: 3.0957374572753906; Gradient Norm 0.32812878489494324; Learning Rate: 0.00019308634344437084; Time: 3248.525619506836 ms; Tokens/Sec : 161392.60126247458\n",
            "\n",
            "Training\n",
            "Step: 15902; Training Loss: 3.046933650970459; Gradient Norm 0.31462904810905457; Learning Rate: 0.00019304146908353013; Time: 3247.4794387817383 ms; Tokens/Sec : 161444.59414858738\n",
            "\n",
            "Training\n",
            "Step: 15903; Training Loss: 3.0264270305633545; Gradient Norm 0.3310653865337372; Learning Rate: 0.00019299660429449597; Time: 3251.018285751343 ms; Tokens/Sec : 161268.85606822473\n",
            "\n",
            "Training\n",
            "Step: 15904; Training Loss: 3.06793212890625; Gradient Norm 0.327111154794693; Learning Rate: 0.00019295174907847103; Time: 3249.537706375122 ms; Tokens/Sec : 161342.33462545238\n",
            "\n",
            "Training\n",
            "Step: 15905; Training Loss: 3.094661235809326; Gradient Norm 0.2995699942111969; Learning Rate: 0.00019290690343665831; Time: 3252.687931060791 ms; Tokens/Sec : 161186.0747517255\n",
            "\n",
            "Training\n",
            "Step: 15906; Training Loss: 3.0081183910369873; Gradient Norm 0.2961472272872925; Learning Rate: 0.00019286206737026027; Time: 3251.4407634735107 ms; Tokens/Sec : 161247.90151179125\n",
            "\n",
            "Training\n",
            "Step: 15907; Training Loss: 3.0431973934173584; Gradient Norm 0.31036341190338135; Learning Rate: 0.00019281724088047916; Time: 3252.1746158599854 ms; Tokens/Sec : 161211.51596325354\n",
            "\n",
            "Training\n",
            "Step: 15908; Training Loss: 3.0469820499420166; Gradient Norm 0.301815927028656; Learning Rate: 0.0001927724239685171; Time: 3251.9025802612305 ms; Tokens/Sec : 161225.00199802514\n",
            "\n",
            "Training\n",
            "Step: 15909; Training Loss: 3.0908830165863037; Gradient Norm 0.323372483253479; Learning Rate: 0.00019272761663557576; Time: 3253.164291381836 ms; Tokens/Sec : 161162.4723008686\n",
            "\n",
            "Training\n",
            "Step: 15910; Training Loss: 3.105350971221924; Gradient Norm 0.28176257014274597; Learning Rate: 0.0001926828188828567; Time: 3254.0504932403564 ms; Tokens/Sec : 161118.58162284334\n",
            "\n",
            "Training\n",
            "Step: 15911; Training Loss: 3.0478172302246094; Gradient Norm 0.310223251581192; Learning Rate: 0.00019263803071156108; Time: 3254.6494007110596 ms; Tokens/Sec : 161088.933230552\n",
            "\n",
            "Training\n",
            "Step: 15912; Training Loss: 3.0857887268066406; Gradient Norm 0.29599690437316895; Learning Rate: 0.00019259325212289; Time: 3259.5317363739014 ms; Tokens/Sec : 160847.6438960062\n",
            "\n",
            "Training\n",
            "Step: 15913; Training Loss: 3.048260450363159; Gradient Norm 0.35721319913864136; Learning Rate: 0.00019254848311804417; Time: 3250.324010848999 ms; Tokens/Sec : 161303.30337837723\n",
            "\n",
            "Training\n",
            "Step: 15914; Training Loss: 3.0290164947509766; Gradient Norm 0.2866242527961731; Learning Rate: 0.00019250372369822397; Time: 3249.7103214263916 ms; Tokens/Sec : 161333.76459532394\n",
            "\n",
            "Training\n",
            "Step: 15915; Training Loss: 3.0145034790039062; Gradient Norm 0.306937038898468; Learning Rate: 0.00019245897386462973; Time: 3253.875494003296 ms; Tokens/Sec : 161127.24686799862\n",
            "\n",
            "Training\n",
            "Step: 15916; Training Loss: 3.0459470748901367; Gradient Norm 0.329750120639801; Learning Rate: 0.00019241423361846148; Time: 3256.117820739746 ms; Tokens/Sec : 161016.2865301013\n",
            "\n",
            "Training\n",
            "Step: 15917; Training Loss: 3.052236557006836; Gradient Norm 0.3341958820819855; Learning Rate: 0.0001923695029609186; Time: 3253.4515857696533 ms; Tokens/Sec : 161148.24093070737\n",
            "\n",
            "Training\n",
            "Step: 15918; Training Loss: 3.034120559692383; Gradient Norm 0.2858893871307373; Learning Rate: 0.00019232478189320086; Time: 3254.063129425049 ms; Tokens/Sec : 161117.95596683305\n",
            "\n",
            "Training\n",
            "Step: 15919; Training Loss: 3.028439998626709; Gradient Norm 0.28560641407966614; Learning Rate: 0.00019228007041650734; Time: 3255.107879638672 ms; Tokens/Sec : 161066.24400362355\n",
            "\n",
            "Training\n",
            "Step: 15920; Training Loss: 2.984550952911377; Gradient Norm 0.2948154807090759; Learning Rate: 0.00019223536853203713; Time: 3257.5552463531494 ms; Tokens/Sec : 160945.23664240024\n",
            "\n",
            "Training\n",
            "Step: 15921; Training Loss: 3.0214600563049316; Gradient Norm 0.30692559480667114; Learning Rate: 0.00019219067624098851; Time: 3254.086971282959 ms; Tokens/Sec : 161116.7754970279\n",
            "\n",
            "Training\n",
            "Step: 15922; Training Loss: 2.9951841831207275; Gradient Norm 0.2929878532886505; Learning Rate: 0.00019214599354456022; Time: 3254.581928253174 ms; Tokens/Sec : 161092.27285035662\n",
            "\n",
            "Training\n",
            "Step: 15923; Training Loss: 3.014195680618286; Gradient Norm 0.3093768060207367; Learning Rate: 0.00019210132044395048; Time: 3259.1137886047363 ms; Tokens/Sec : 160868.270949341\n",
            "\n",
            "Training\n",
            "Step: 15924; Training Loss: 3.0164642333984375; Gradient Norm 0.31612712144851685; Learning Rate: 0.00019205665694035698; Time: 3255.3865909576416 ms; Tokens/Sec : 161052.45424807427\n",
            "\n",
            "Training\n",
            "Step: 15925; Training Loss: 3.085362672805786; Gradient Norm 0.2892402112483978; Learning Rate: 0.0001920120030349773; Time: 3254.29368019104 ms; Tokens/Sec : 161106.5415488937\n",
            "\n",
            "Training\n",
            "Step: 15926; Training Loss: 3.0171496868133545; Gradient Norm 0.3214719295501709; Learning Rate: 0.00019196735872900914; Time: 3255.4798126220703 ms; Tokens/Sec : 161047.84246157596\n",
            "\n",
            "Training\n",
            "Step: 15927; Training Loss: 3.14487624168396; Gradient Norm 0.35380828380584717; Learning Rate: 0.00019192272402364957; Time: 3256.8414211273193 ms; Tokens/Sec : 160980.51216092787\n",
            "\n",
            "Training\n",
            "Step: 15928; Training Loss: 3.025818347930908; Gradient Norm 0.35301169753074646; Learning Rate: 0.00019187809892009521; Time: 3253.5908222198486 ms; Tokens/Sec : 161141.34463973274\n",
            "\n",
            "Training\n",
            "Step: 15929; Training Loss: 3.1071691513061523; Gradient Norm 0.29758909344673157; Learning Rate: 0.00019183348341954276; Time: 3255.030393600464 ms; Tokens/Sec : 161070.0781875259\n",
            "\n",
            "Training\n",
            "Step: 15930; Training Loss: 3.0249691009521484; Gradient Norm 0.37724432349205017; Learning Rate: 0.00019178887752318885; Time: 3254.5807361602783 ms; Tokens/Sec : 161092.33185548492\n",
            "\n",
            "Training\n",
            "Step: 15931; Training Loss: 3.0502374172210693; Gradient Norm 0.38248470425605774; Learning Rate: 0.00019174428123222933; Time: 3256.7996978759766 ms; Tokens/Sec : 160982.5745015669\n",
            "\n",
            "Training\n",
            "Step: 15932; Training Loss: 3.089688539505005; Gradient Norm 0.3262162506580353; Learning Rate: 0.00019169969454786004; Time: 3255.4783821105957 ms; Tokens/Sec : 161047.91322868282\n",
            "\n",
            "Training\n",
            "Step: 15933; Training Loss: 3.0425567626953125; Gradient Norm 0.32202062010765076; Learning Rate: 0.00019165511747127668; Time: 3258.3847045898438 ms; Tokens/Sec : 160904.2662339639\n",
            "\n",
            "Training\n",
            "Step: 15934; Training Loss: 2.982356548309326; Gradient Norm 0.3260107636451721; Learning Rate: 0.00019161055000367448; Time: 3256.9684982299805 ms; Tokens/Sec : 160974.23118612522\n",
            "\n",
            "Training\n",
            "Step: 15935; Training Loss: 3.2084834575653076; Gradient Norm 0.4463023841381073; Learning Rate: 0.0001915659921462486; Time: 3257.8413486480713 ms; Tokens/Sec : 160931.10249753794\n",
            "\n",
            "Training\n",
            "Step: 15936; Training Loss: 3.102660894393921; Gradient Norm 0.3969154357910156; Learning Rate: 0.00019152144390019373; Time: 3257.802963256836 ms; Tokens/Sec : 160932.99868444703\n",
            "\n",
            "Training\n",
            "Step: 15937; Training Loss: 3.0761609077453613; Gradient Norm 0.40336930751800537; Learning Rate: 0.00019147690526670466; Time: 3255.6753158569336 ms; Tokens/Sec : 161038.17154199266\n",
            "\n",
            "Training\n",
            "Step: 15938; Training Loss: 3.0601391792297363; Gradient Norm 0.3134024441242218; Learning Rate: 0.0001914323762469755; Time: 3258.509635925293 ms; Tokens/Sec : 160898.09716064323\n",
            "\n",
            "Training\n",
            "Step: 15939; Training Loss: 3.037334442138672; Gradient Norm 0.3294116258621216; Learning Rate: 0.00019138785684220028; Time: 3256.782293319702 ms; Tokens/Sec : 160983.43480785232\n",
            "\n",
            "Training\n",
            "Step: 15940; Training Loss: 3.0221049785614014; Gradient Norm 0.3175389766693115; Learning Rate: 0.00019134334705357285; Time: 3257.441282272339 ms; Tokens/Sec : 160950.86743490433\n",
            "\n",
            "Training\n",
            "Step: 15941; Training Loss: 3.036221981048584; Gradient Norm 0.2991008758544922; Learning Rate: 0.00019129884688228675; Time: 3257.385492324829 ms; Tokens/Sec : 160953.62407530413\n",
            "\n",
            "Training\n",
            "Step: 15942; Training Loss: 3.0547099113464355; Gradient Norm 0.359543114900589; Learning Rate: 0.00019125435632953518; Time: 3254.5995712280273 ms; Tokens/Sec : 161091.3995795112\n",
            "\n",
            "Training\n",
            "Step: 15943; Training Loss: 3.0880179405212402; Gradient Norm 0.30064791440963745; Learning Rate: 0.00019120987539651132; Time: 3255.3327083587646 ms; Tokens/Sec : 161055.12000471662\n",
            "\n",
            "Training\n",
            "Step: 15944; Training Loss: 3.056464195251465; Gradient Norm 0.2894721031188965; Learning Rate: 0.00019116540408440757; Time: 3258.124828338623 ms; Tokens/Sec : 160917.10036393662\n",
            "\n",
            "Training\n",
            "Step: 15945; Training Loss: 3.0822994709014893; Gradient Norm 0.2962495982646942; Learning Rate: 0.00019112094239441673; Time: 3257.349967956543 ms; Tokens/Sec : 160955.37942117575\n",
            "\n",
            "Training\n",
            "Step: 15946; Training Loss: 3.095245599746704; Gradient Norm 0.3053504526615143; Learning Rate: 0.00019107649032773096; Time: 3255.615234375 ms; Tokens/Sec : 161041.14345706787\n",
            "\n",
            "Training\n",
            "Step: 15947; Training Loss: 3.0314483642578125; Gradient Norm 0.3197546601295471; Learning Rate: 0.00019103204788554223; Time: 3258.269786834717 ms; Tokens/Sec : 160909.94125729703\n",
            "\n",
            "Training\n",
            "Step: 15948; Training Loss: 3.0466396808624268; Gradient Norm 0.28943780064582825; Learning Rate: 0.00019098761506904224; Time: 3257.586717605591 ms; Tokens/Sec : 160943.68176493704\n",
            "\n",
            "Training\n",
            "Step: 15949; Training Loss: 2.991549253463745; Gradient Norm 0.3048856854438782; Learning Rate: 0.0001909431918794225; Time: 3255.236864089966 ms; Tokens/Sec : 161059.86196693245\n",
            "\n",
            "Training\n",
            "Step: 15950; Training Loss: 3.0685875415802; Gradient Norm 0.2868206202983856; Learning Rate: 0.0001908987783178743; Time: 3257.1418285369873 ms; Tokens/Sec : 160965.66486805238\n",
            "\n",
            "Training\n",
            "Step: 15951; Training Loss: 3.0214357376098633; Gradient Norm 0.3030281662940979; Learning Rate: 0.00019085437438558822; Time: 3256.587266921997 ms; Tokens/Sec : 160993.07558109355\n",
            "\n",
            "Training\n",
            "Step: 15952; Training Loss: 3.0394558906555176; Gradient Norm 0.2826279103755951; Learning Rate: 0.0001908099800837553; Time: 3258.4478855133057 ms; Tokens/Sec : 160901.14631905753\n",
            "\n",
            "Training\n",
            "Step: 15953; Training Loss: 3.0065672397613525; Gradient Norm 0.2999970018863678; Learning Rate: 0.00019076559541356586; Time: 3257.6286792755127 ms; Tokens/Sec : 160941.60864171915\n",
            "\n",
            "Training\n",
            "Step: 15954; Training Loss: 3.0618295669555664; Gradient Norm 0.29635411500930786; Learning Rate: 0.0001907212203762102; Time: 3256.927967071533 ms; Tokens/Sec : 160976.23444568028\n",
            "\n",
            "Training\n",
            "Step: 15955; Training Loss: 3.018519163131714; Gradient Norm 0.27317535877227783; Learning Rate: 0.00019067685497287784; Time: 3259.803533554077 ms; Tokens/Sec : 160834.23267793772\n",
            "\n",
            "Training\n",
            "Step: 15956; Training Loss: 3.0378308296203613; Gradient Norm 0.284864217042923; Learning Rate: 0.00019063249920475884; Time: 3257.7943801879883 ms; Tokens/Sec : 160933.42268266372\n",
            "\n",
            "Training\n",
            "Step: 15957; Training Loss: 3.0172460079193115; Gradient Norm 0.29225191473960876; Learning Rate: 0.00019058815307304258; Time: 3257.852077484131 ms; Tokens/Sec : 160930.57251539803\n",
            "\n",
            "Training\n",
            "Step: 15958; Training Loss: 3.1159584522247314; Gradient Norm 0.2892014682292938; Learning Rate: 0.00019054381657891792; Time: 3256.8373680114746 ms; Tokens/Sec : 160980.7125002727\n",
            "\n",
            "Training\n",
            "Step: 15959; Training Loss: 2.988926887512207; Gradient Norm 0.2986357808113098; Learning Rate: 0.0001904994897235738; Time: 3257.2479248046875 ms; Tokens/Sec : 160960.42183569356\n",
            "\n",
            "Training\n",
            "Step: 15960; Training Loss: 2.9941251277923584; Gradient Norm 0.2915504276752472; Learning Rate: 0.0001904551725081992; Time: 3258.3651542663574 ms; Tokens/Sec : 160905.2316661074\n",
            "\n",
            "Training\n",
            "Step: 15961; Training Loss: 3.050563097000122; Gradient Norm 0.28797367215156555; Learning Rate: 0.000190410864933982; Time: 3259.1075897216797 ms; Tokens/Sec : 160868.5769237747\n",
            "\n",
            "Training\n",
            "Step: 15962; Training Loss: 3.100055694580078; Gradient Norm 0.365612268447876; Learning Rate: 0.0001903665670021106; Time: 3258.833169937134 ms; Tokens/Sec : 160882.12334297373\n",
            "\n",
            "Training\n",
            "Step: 15963; Training Loss: 3.043534278869629; Gradient Norm 0.3160440921783447; Learning Rate: 0.00019032227871377259; Time: 3259.0091228485107 ms; Tokens/Sec : 160873.43736606368\n",
            "\n",
            "Training\n",
            "Step: 15964; Training Loss: 3.0270979404449463; Gradient Norm 0.3484465479850769; Learning Rate: 0.00019027800007015598; Time: 3257.540464401245 ms; Tokens/Sec : 160945.96697400263\n",
            "\n",
            "Training\n",
            "Step: 15965; Training Loss: 3.062399387359619; Gradient Norm 0.284874826669693; Learning Rate: 0.0001902337310724477; Time: 3256.18314743042 ms; Tokens/Sec : 161013.05616477254\n",
            "\n",
            "Training\n",
            "Step: 15966; Training Loss: 3.006004810333252; Gradient Norm 0.4506278336048126; Learning Rate: 0.00019018947172183497; Time: 3260.3344917297363 ms; Tokens/Sec : 160808.040196466\n",
            "\n",
            "Training\n",
            "Step: 15967; Training Loss: 3.032306671142578; Gradient Norm 0.37870919704437256; Learning Rate: 0.00019014522201950463; Time: 3256.5221786499023 ms; Tokens/Sec : 160996.2933577688\n",
            "\n",
            "Training\n",
            "Step: 15968; Training Loss: 3.2287638187408447; Gradient Norm 0.4083419740200043; Learning Rate: 0.00019010098196664316; Time: 3273.9124298095703 ms; Tokens/Sec : 160141.1189945895\n",
            "\n",
            "Training\n",
            "Step: 15969; Training Loss: 3.0822484493255615; Gradient Norm 0.3420870304107666; Learning Rate: 0.00019005675156443685; Time: 3258.8980197906494 ms; Tokens/Sec : 160878.92189816976\n",
            "\n",
            "Training\n",
            "Step: 15970; Training Loss: 3.025097131729126; Gradient Norm 0.35200998187065125; Learning Rate: 0.00019001253081407182; Time: 3257.460832595825 ms; Tokens/Sec : 160949.90145505517\n",
            "\n",
            "Training\n",
            "Step: 15971; Training Loss: 3.0355448722839355; Gradient Norm 0.38466405868530273; Learning Rate: 0.00018996831971673377; Time: 3259.7334384918213 ms; Tokens/Sec : 160837.69114647974\n",
            "\n",
            "Training\n",
            "Step: 15972; Training Loss: 3.0558114051818848; Gradient Norm 0.34647616744041443; Learning Rate: 0.0001899241182736082; Time: 3257.1544647216797 ms; Tokens/Sec : 160965.04039908954\n",
            "\n",
            "Training\n",
            "Step: 15973; Training Loss: 3.1076772212982178; Gradient Norm 0.35610735416412354; Learning Rate: 0.00018987992648588045; Time: 3256.7992210388184 ms; Tokens/Sec : 160982.59807147962\n",
            "\n",
            "Training\n",
            "Step: 15974; Training Loss: 3.1029155254364014; Gradient Norm 0.3260652422904968; Learning Rate: 0.00018983574435473544; Time: 3257.87091255188 ms; Tokens/Sec : 160929.64211075107\n",
            "\n",
            "Training\n",
            "Step: 15975; Training Loss: 3.032562255859375; Gradient Norm 0.2940357029438019; Learning Rate: 0.000189791571881358; Time: 3258.185386657715 ms; Tokens/Sec : 160914.10947546505\n",
            "\n",
            "Training\n",
            "Step: 15976; Training Loss: 3.0860848426818848; Gradient Norm 0.3518592417240143; Learning Rate: 0.0001897474090669325; Time: 3259.8061561584473 ms; Tokens/Sec : 160834.10328234138\n",
            "\n",
            "Training\n",
            "Step: 15977; Training Loss: 3.0768847465515137; Gradient Norm 0.2937352657318115; Learning Rate: 0.00018970325591264337; Time: 3257.5623989105225 ms; Tokens/Sec : 160944.88325852048\n",
            "\n",
            "Training\n",
            "Step: 15978; Training Loss: 3.064021348953247; Gradient Norm 0.2892206907272339; Learning Rate: 0.0001896591124196741; Time: 3257.9755783081055 ms; Tokens/Sec : 160924.47208344858\n",
            "\n",
            "Training\n",
            "Step: 15979; Training Loss: 3.0703442096710205; Gradient Norm 0.3105472922325134; Learning Rate: 0.0001896149785892089; Time: 3259.4640254974365 ms; Tokens/Sec : 160850.98528430203\n",
            "\n",
            "Training\n",
            "Step: 15980; Training Loss: 3.022813081741333; Gradient Norm 0.2619551122188568; Learning Rate: 0.00018957085442243102; Time: 3258.427381515503 ms; Tokens/Sec : 160902.158806483\n",
            "\n",
            "Training\n",
            "Step: 15981; Training Loss: 3.0763206481933594; Gradient Norm 0.28515785932540894; Learning Rate: 0.00018952673992052378; Time: 3257.828712463379 ms; Tokens/Sec : 160931.7267032017\n",
            "\n",
            "Training\n",
            "Step: 15982; Training Loss: 3.0870420932769775; Gradient Norm 0.32368022203445435; Learning Rate: 0.00018948263508466968; Time: 3257.812023162842 ms; Tokens/Sec : 160932.55113319762\n",
            "\n",
            "Training\n",
            "Step: 15983; Training Loss: 3.0549674034118652; Gradient Norm 0.29576653242111206; Learning Rate: 0.00018943853991605186; Time: 3256.932497024536 ms; Tokens/Sec : 160976.01054949046\n",
            "\n",
            "Training\n",
            "Step: 15984; Training Loss: 2.9748904705047607; Gradient Norm 0.3291904926300049; Learning Rate: 0.00018939445441585267; Time: 3258.2573890686035 ms; Tokens/Sec : 160910.55352440145\n",
            "\n",
            "Training\n",
            "Step: 15985; Training Loss: 3.027956247329712; Gradient Norm 0.33261606097221375; Learning Rate: 0.00018935037858525396; Time: 3258.0811977386475 ms; Tokens/Sec : 160919.25528556353\n",
            "\n",
            "Training\n",
            "Step: 15986; Training Loss: 3.043788433074951; Gradient Norm 0.3026288151741028; Learning Rate: 0.00018930631242543771; Time: 3258.4962844848633 ms; Tokens/Sec : 160898.75642834586\n",
            "\n",
            "Training\n",
            "Step: 15987; Training Loss: 3.0458791255950928; Gradient Norm 0.35155755281448364; Learning Rate: 0.00018926225593758587; Time: 3257.8887939453125 ms; Tokens/Sec : 160928.75882515492\n",
            "\n",
            "Training\n",
            "Step: 15988; Training Loss: 3.047245502471924; Gradient Norm 0.30414465069770813; Learning Rate: 0.00018921820912287947; Time: 3258.2321166992188 ms; Tokens/Sec : 160911.80162177476\n",
            "\n",
            "Training\n",
            "Step: 15989; Training Loss: 3.0262160301208496; Gradient Norm 0.2686218321323395; Learning Rate: 0.0001891741719824997; Time: 3258.7711811065674 ms; Tokens/Sec : 160885.18366667576\n",
            "\n",
            "Training\n",
            "Step: 15990; Training Loss: 3.0610790252685547; Gradient Norm 0.27468159794807434; Learning Rate: 0.00018913014451762733; Time: 3258.7413787841797 ms; Tokens/Sec : 160886.65501759126\n",
            "\n",
            "Training\n",
            "Step: 15991; Training Loss: 3.047786235809326; Gradient Norm 0.3051687777042389; Learning Rate: 0.00018908612672944333; Time: 3257.3578357696533 ms; Tokens/Sec : 160954.99064999731\n",
            "\n",
            "Training\n",
            "Step: 15992; Training Loss: 3.0213024616241455; Gradient Norm 0.30040499567985535; Learning Rate: 0.00018904211861912757; Time: 3257.0700645446777 ms; Tokens/Sec : 160969.21147236446\n",
            "\n",
            "Training\n",
            "Step: 15993; Training Loss: 3.0803890228271484; Gradient Norm 0.2741651237010956; Learning Rate: 0.00018899812018786027; Time: 3257.8182220458984 ms; Tokens/Sec : 160932.24491535596\n",
            "\n",
            "Training\n",
            "Step: 15994; Training Loss: 2.9876112937927246; Gradient Norm 0.26777854561805725; Learning Rate: 0.00018895413143682152; Time: 3256.1748027801514 ms; Tokens/Sec : 161013.46879545847\n",
            "\n",
            "Training\n",
            "Step: 15995; Training Loss: 3.057238817214966; Gradient Norm 0.31532374024391174; Learning Rate: 0.00018891015236719046; Time: 3261.566162109375 ms; Tokens/Sec : 160747.31400233918\n",
            "\n",
            "Training\n",
            "Step: 15996; Training Loss: 3.0855026245117188; Gradient Norm 0.38479968905448914; Learning Rate: 0.0001888661829801466; Time: 3258.6965560913086 ms; Tokens/Sec : 160888.86798004442\n",
            "\n",
            "Training\n",
            "Step: 15997; Training Loss: 3.0474557876586914; Gradient Norm 0.31377050280570984; Learning Rate: 0.00018882222327686884; Time: 3259.573221206665 ms; Tokens/Sec : 160845.59677598323\n",
            "\n",
            "Training\n",
            "Step: 15998; Training Loss: 3.065845251083374; Gradient Norm 0.3692784905433655; Learning Rate: 0.00018877827325853626; Time: 3256.2296390533447 ms; Tokens/Sec : 161010.75726109467\n",
            "\n",
            "Training\n",
            "Step: 15999; Training Loss: 3.06160306930542; Gradient Norm 0.35587799549102783; Learning Rate: 0.0001887343329263271; Time: 3255.8038234710693 ms; Tokens/Sec : 161031.81531405888\n",
            "\n",
            "Evaluating HellaSwag\n",
            "Evaluating Validation set\n",
            "Training\n",
            "Step: 16000; Training Loss: 3.073988676071167; Gradient Norm 0.3930852711200714; Learning Rate: 0.0001886904022814196; Time: 3249.022960662842 ms; Tokens/Sec : 161367.8962407328; Validation_loss: 2.9357361793518066; HellaSwag Acc: 0.329\n",
            "\n",
            "Checkpointing\n",
            "Training\n",
            "Step: 16001; Training Loss: 3.107663869857788; Gradient Norm 0.32836511731147766; Learning Rate: 0.0001886464813249919; Time: 3275.4006385803223 ms; Tokens/Sec : 160068.3573864251\n",
            "\n",
            "Training\n",
            "Step: 16002; Training Loss: 3.0576024055480957; Gradient Norm 0.3488670289516449; Learning Rate: 0.00018860257005822175; Time: 3269.181251525879 ms; Tokens/Sec : 160372.8761613601\n",
            "\n",
            "Training\n",
            "Step: 16003; Training Loss: 3.062931537628174; Gradient Norm 0.3291913568973541; Learning Rate: 0.00018855866848228647; Time: 3245.4912662506104 ms; Tokens/Sec : 161543.49433997693\n",
            "\n",
            "Training\n",
            "Step: 16004; Training Loss: 3.116018295288086; Gradient Norm 0.40813687443733215; Learning Rate: 0.00018851477659836334; Time: 3246.9682693481445 ms; Tokens/Sec : 161470.0103322091\n",
            "\n",
            "Training\n",
            "Step: 16005; Training Loss: 3.100841522216797; Gradient Norm 0.37103939056396484; Learning Rate: 0.0001884708944076294; Time: 3240.689277648926 ms; Tokens/Sec : 161782.86626120587\n",
            "\n",
            "Training\n",
            "Step: 16006; Training Loss: 3.0997064113616943; Gradient Norm 0.35422244668006897; Learning Rate: 0.0001884270219112613; Time: 3239.5596504211426 ms; Tokens/Sec : 161839.2795859902\n",
            "\n",
            "Training\n",
            "Step: 16007; Training Loss: 3.0874602794647217; Gradient Norm 0.33506280183792114; Learning Rate: 0.00018838315911043536; Time: 3246.434450149536 ms; Tokens/Sec : 161496.56124301246\n",
            "\n",
            "Training\n",
            "Step: 16008; Training Loss: 3.0282702445983887; Gradient Norm 0.35704705119132996; Learning Rate: 0.0001883393060063279; Time: 3248.616933822632 ms; Tokens/Sec : 161388.06473038753\n",
            "\n",
            "Training\n",
            "Step: 16009; Training Loss: 3.063509702682495; Gradient Norm 0.33180007338523865; Learning Rate: 0.00018829546260011482; Time: 3248.3887672424316 ms; Tokens/Sec : 161399.40061579202\n",
            "\n",
            "Training\n",
            "Step: 16010; Training Loss: 3.041003704071045; Gradient Norm 0.2916198968887329; Learning Rate: 0.00018825162889297178; Time: 3241.3175106048584 ms; Tokens/Sec : 161751.50946633526\n",
            "\n",
            "Training\n",
            "Step: 16011; Training Loss: 3.0688915252685547; Gradient Norm 0.31612205505371094; Learning Rate: 0.00018820780488607415; Time: 3248.493194580078 ms; Tokens/Sec : 161394.21220729168\n",
            "\n",
            "Training\n",
            "Step: 16012; Training Loss: 3.0544235706329346; Gradient Norm 0.31464090943336487; Learning Rate: 0.00018816399058059694; Time: 3246.0741996765137 ms; Tokens/Sec : 161514.48418900828\n",
            "\n",
            "Training\n",
            "Step: 16013; Training Loss: 3.073376417160034; Gradient Norm 0.2743319272994995; Learning Rate: 0.00018812018597771517; Time: 3250.0088214874268 ms; Tokens/Sec : 161318.94674675065\n",
            "\n",
            "Training\n",
            "Step: 16014; Training Loss: 3.076199531555176; Gradient Norm 0.2940213978290558; Learning Rate: 0.00018807639107860352; Time: 3247.5738525390625 ms; Tokens/Sec : 161439.9006169156\n",
            "\n",
            "Training\n",
            "Step: 16015; Training Loss: 3.0707600116729736; Gradient Norm 0.3270534873008728; Learning Rate: 0.00018803260588443635; Time: 3247.5476264953613 ms; Tokens/Sec : 161441.2043483387\n",
            "\n",
            "Training\n",
            "Step: 16016; Training Loss: 3.1023778915405273; Gradient Norm 0.2687866687774658; Learning Rate: 0.00018798883039638738; Time: 3250.3747940063477 ms; Tokens/Sec : 161300.78321022572\n",
            "\n",
            "Training\n",
            "Step: 16017; Training Loss: 3.058605432510376; Gradient Norm 0.3017912209033966; Learning Rate: 0.0001879450646156309; Time: 3250.458240509033 ms; Tokens/Sec : 161296.64225986015\n",
            "\n",
            "Training\n",
            "Step: 16018; Training Loss: 3.019544839859009; Gradient Norm 0.3133385479450226; Learning Rate: 0.00018790130854334043; Time: 3248.032569885254 ms; Tokens/Sec : 161417.10057375501\n",
            "\n",
            "Training\n",
            "Step: 16019; Training Loss: 3.0683887004852295; Gradient Norm 0.303867369890213; Learning Rate: 0.0001878575621806891; Time: 3242.898941040039 ms; Tokens/Sec : 161672.62980814755\n",
            "\n",
            "Training\n",
            "Step: 16020; Training Loss: 3.057037353515625; Gradient Norm 0.32427531480789185; Learning Rate: 0.00018781382552884981; Time: 3251.0242462158203 ms; Tokens/Sec : 161268.56039608724\n",
            "\n",
            "Training\n",
            "Step: 16021; Training Loss: 3.039416551589966; Gradient Norm 0.34447818994522095; Learning Rate: 0.00018777009858899588; Time: 3252.5241374969482 ms; Tokens/Sec : 161194.19190643652\n",
            "\n",
            "Training\n",
            "Step: 16022; Training Loss: 3.051729440689087; Gradient Norm 0.31516119837760925; Learning Rate: 0.00018772638136229942; Time: 3247.258424758911 ms; Tokens/Sec : 161455.58234679926\n",
            "\n",
            "Training\n",
            "Step: 16023; Training Loss: 3.082702159881592; Gradient Norm 0.28965380787849426; Learning Rate: 0.00018768267384993277; Time: 3244.2948818206787 ms; Tokens/Sec : 161603.0660276395\n",
            "\n",
            "Training\n",
            "Step: 16024; Training Loss: 3.1420116424560547; Gradient Norm 0.34093427658081055; Learning Rate: 0.00018763897605306797; Time: 3249.7754096984863 ms; Tokens/Sec : 161330.53331480632\n",
            "\n",
            "Training\n",
            "Step: 16025; Training Loss: 3.02067232131958; Gradient Norm 0.27690955996513367; Learning Rate: 0.00018759528797287697; Time: 3250.2167224884033 ms; Tokens/Sec : 161308.6279362316\n",
            "\n",
            "Training\n",
            "Step: 16026; Training Loss: 3.0432400703430176; Gradient Norm 0.331295371055603; Learning Rate: 0.000187551609610531; Time: 3250.16450881958 ms; Tokens/Sec : 161311.21934822155\n",
            "\n",
            "Training\n",
            "Step: 16027; Training Loss: 3.051143169403076; Gradient Norm 0.31921812891960144; Learning Rate: 0.00018750794096720135; Time: 3245.576858520508 ms; Tokens/Sec : 161539.23411907614\n",
            "\n",
            "Training\n",
            "Step: 16028; Training Loss: 3.02236008644104; Gradient Norm 0.33602800965309143; Learning Rate: 0.00018746428204405907; Time: 3250.744581222534 ms; Tokens/Sec : 161282.43450084495\n",
            "\n",
            "Training\n",
            "Step: 16029; Training Loss: 3.0789952278137207; Gradient Norm 0.32619166374206543; Learning Rate: 0.0001874206328422748; Time: 3251.3511180877686 ms; Tokens/Sec : 161252.34739591947\n",
            "\n",
            "Training\n",
            "Step: 16030; Training Loss: 2.9870529174804688; Gradient Norm 0.32603487372398376; Learning Rate: 0.00018737699336301896; Time: 3251.8973350524902 ms; Tokens/Sec : 161225.26204891314\n",
            "\n",
            "Training\n",
            "Step: 16031; Training Loss: 3.072824001312256; Gradient Norm 0.332426518201828; Learning Rate: 0.00018733336360746182; Time: 3257.1144104003906 ms; Tokens/Sec : 160967.01986454026\n",
            "\n",
            "Training\n",
            "Step: 16032; Training Loss: 3.0188498497009277; Gradient Norm 0.3276047706604004; Learning Rate: 0.00018728974357677333; Time: 3255.1798820495605 ms; Tokens/Sec : 161062.68132558386\n",
            "\n",
            "Training\n",
            "Step: 16033; Training Loss: 3.0029027462005615; Gradient Norm 0.3981238007545471; Learning Rate: 0.000187246133272123; Time: 3262.932777404785 ms; Tokens/Sec : 160679.98814765626\n",
            "\n",
            "Training\n",
            "Step: 16034; Training Loss: 3.0558838844299316; Gradient Norm 0.40631064772605896; Learning Rate: 0.00018720253269468044; Time: 3258.3885192871094 ms; Tokens/Sec : 160904.07785831107\n",
            "\n",
            "Training\n",
            "Step: 16035; Training Loss: 3.0234999656677246; Gradient Norm 0.35982275009155273; Learning Rate: 0.00018715894184561464; Time: 3258.0676078796387 ms; Tokens/Sec : 160919.9265024486\n",
            "\n",
            "Training\n",
            "Step: 16036; Training Loss: 3.087348461151123; Gradient Norm 0.3293737471103668; Learning Rate: 0.00018711536072609457; Time: 3255.9118270874023 ms; Tokens/Sec : 161026.47364041346\n",
            "\n",
            "Training\n",
            "Step: 16037; Training Loss: 3.0581037998199463; Gradient Norm 0.35884860157966614; Learning Rate: 0.00018707178933728875; Time: 3257.094383239746 ms; Tokens/Sec : 160968.00961552258\n",
            "\n",
            "Training\n",
            "Step: 16038; Training Loss: 2.9693374633789062; Gradient Norm 0.3424120843410492; Learning Rate: 0.00018702822768036582; Time: 3259.4192028045654 ms; Tokens/Sec : 160853.1972655977\n",
            "\n",
            "Training\n",
            "Step: 16039; Training Loss: 3.0830976963043213; Gradient Norm 0.4100310504436493; Learning Rate: 0.00018698467575649336; Time: 3257.7977180480957 ms; Tokens/Sec : 160933.25779420286\n",
            "\n",
            "Training\n",
            "Step: 16040; Training Loss: 3.046140193939209; Gradient Norm 0.31374549865722656; Learning Rate: 0.00018694113356683965; Time: 3258.058547973633 ms; Tokens/Sec : 160920.37398348283\n",
            "\n",
            "Training\n",
            "Step: 16041; Training Loss: 3.059410572052002; Gradient Norm 0.46663179993629456; Learning Rate: 0.0001868976011125722; Time: 3258.772850036621 ms; Tokens/Sec : 160885.10127182023\n",
            "\n",
            "Training\n",
            "Step: 16042; Training Loss: 3.0604732036590576; Gradient Norm 0.31612223386764526; Learning Rate: 0.00018685407839485836; Time: 3258.1849098205566 ms; Tokens/Sec : 160914.13302533372\n",
            "\n",
            "Training\n",
            "Step: 16043; Training Loss: 3.067474126815796; Gradient Norm 0.3226979672908783; Learning Rate: 0.00018681056541486484; Time: 3257.9686641693115 ms; Tokens/Sec : 160924.8136012009\n",
            "\n",
            "Training\n",
            "Step: 16044; Training Loss: 3.0594708919525146; Gradient Norm 0.33419543504714966; Learning Rate: 0.00018676706217375883; Time: 3257.8978538513184 ms; Tokens/Sec : 160928.31129748706\n",
            "\n",
            "Training\n",
            "Step: 16045; Training Loss: 3.049403190612793; Gradient Norm 0.27688851952552795; Learning Rate: 0.00018672356867270687; Time: 3259.3913078308105 ms; Tokens/Sec : 160854.5739016909\n",
            "\n",
            "Training\n",
            "Step: 16046; Training Loss: 3.055542230606079; Gradient Norm 0.3200617730617523; Learning Rate: 0.00018668008491287496; Time: 3258.5771083831787 ms; Tokens/Sec : 160894.765586854\n",
            "\n",
            "Training\n",
            "Step: 16047; Training Loss: 3.0514168739318848; Gradient Norm 0.3258228003978729; Learning Rate: 0.00018663661089542914; Time: 3258.4803104400635 ms; Tokens/Sec : 160899.5452021602\n",
            "\n",
            "Training\n",
            "Step: 16048; Training Loss: 2.9995651245117188; Gradient Norm 0.32030272483825684; Learning Rate: 0.00018659314662153548; Time: 3258.2950592041016 ms; Tokens/Sec : 160908.6931887829\n",
            "\n",
            "Training\n",
            "Step: 16049; Training Loss: 3.096748113632202; Gradient Norm 0.31176868081092834; Learning Rate: 0.0001865496920923591; Time: 3257.8461170196533 ms; Tokens/Sec : 160930.86694948925\n",
            "\n",
            "Training\n",
            "Step: 16050; Training Loss: 3.0352916717529297; Gradient Norm 0.31684088706970215; Learning Rate: 0.00018650624730906528; Time: 3261.3511085510254 ms; Tokens/Sec : 160757.91368348995\n",
            "\n",
            "Training\n",
            "Step: 16051; Training Loss: 3.069366693496704; Gradient Norm 0.30029964447021484; Learning Rate: 0.0001864628122728193; Time: 3259.800910949707 ms; Tokens/Sec : 160834.36207374226\n",
            "\n",
            "Training\n",
            "Step: 16052; Training Loss: 3.099275588989258; Gradient Norm 0.2784828245639801; Learning Rate: 0.00018641938698478572; Time: 3259.7923278808594 ms; Tokens/Sec : 160834.78555237644\n",
            "\n",
            "Training\n",
            "Step: 16053; Training Loss: 3.100536823272705; Gradient Norm 0.2953789234161377; Learning Rate: 0.0001863759714461288; Time: 3262.8376483917236 ms; Tokens/Sec : 160684.67282104134\n",
            "\n",
            "Training\n",
            "Step: 16054; Training Loss: 3.0018889904022217; Gradient Norm 0.30209121108055115; Learning Rate: 0.00018633256565801272; Time: 3257.26580619812 ms; Tokens/Sec : 160959.5382121881\n",
            "\n",
            "Training\n",
            "Step: 16055; Training Loss: 2.999906539916992; Gradient Norm 0.29531216621398926; Learning Rate: 0.00018628916962160183; Time: 3262.108325958252 ms; Tokens/Sec : 160720.5977275415\n",
            "\n",
            "Training\n",
            "Step: 16056; Training Loss: 3.05119252204895; Gradient Norm 0.3159675598144531; Learning Rate: 0.00018624578333805927; Time: 3258.2225799560547 ms; Tokens/Sec : 160912.27260694734\n",
            "\n",
            "Training\n",
            "Step: 16057; Training Loss: 3.030968427658081; Gradient Norm 0.29684165120124817; Learning Rate: 0.0001862024068085487; Time: 3260.136842727661 ms; Tokens/Sec : 160817.78934203988\n",
            "\n",
            "Training\n",
            "Step: 16058; Training Loss: 3.0359392166137695; Gradient Norm 0.269989937543869; Learning Rate: 0.00018615904003423312; Time: 3262.0577812194824 ms; Tokens/Sec : 160723.08805149398\n",
            "\n",
            "Training\n",
            "Step: 16059; Training Loss: 3.0388734340667725; Gradient Norm 0.2967854142189026; Learning Rate: 0.00018611568301627574; Time: 3261.045217514038 ms; Tokens/Sec : 160772.99302205798\n",
            "\n",
            "Training\n",
            "Step: 16060; Training Loss: 3.0337798595428467; Gradient Norm 0.256613165140152; Learning Rate: 0.0001860723357558388; Time: 3260.6964111328125 ms; Tokens/Sec : 160790.19138670896\n",
            "\n",
            "Training\n",
            "Step: 16061; Training Loss: 2.9999029636383057; Gradient Norm 0.29469749331474304; Learning Rate: 0.00018602899825408484; Time: 3259.13405418396 ms; Tokens/Sec : 160867.27065643028\n",
            "\n",
            "Training\n",
            "Step: 16062; Training Loss: 3.0591530799865723; Gradient Norm 0.2764739990234375; Learning Rate: 0.00018598567051217584; Time: 3259.7949504852295 ms; Tokens/Sec : 160834.6561558905\n",
            "\n",
            "Training\n",
            "Step: 16063; Training Loss: 3.0577595233917236; Gradient Norm 0.30409038066864014; Learning Rate: 0.00018594235253127368; Time: 3263.1702423095703 ms; Tokens/Sec : 160668.29526764908\n",
            "\n",
            "Training\n",
            "Step: 16064; Training Loss: 3.0995278358459473; Gradient Norm 0.2811291217803955; Learning Rate: 0.00018589904431253997; Time: 3262.7615928649902 ms; Tokens/Sec : 160688.4184080484\n",
            "\n",
            "Training\n",
            "Step: 16065; Training Loss: 3.028886318206787; Gradient Norm 0.3070804178714752; Learning Rate: 0.00018585574585713594; Time: 3261.395215988159 ms; Tokens/Sec : 160755.73957728632\n",
            "\n",
            "Training\n",
            "Step: 16066; Training Loss: 3.0556838512420654; Gradient Norm 0.32922664284706116; Learning Rate: 0.0001858124571662227; Time: 3261.0530853271484 ms; Tokens/Sec : 160772.60513145052\n",
            "\n",
            "Training\n",
            "Step: 16067; Training Loss: 3.078505516052246; Gradient Norm 0.3349260985851288; Learning Rate: 0.000185769178240961; Time: 3262.932300567627 ms; Tokens/Sec : 160680.0116290472\n",
            "\n",
            "Training\n",
            "Step: 16068; Training Loss: 3.083810329437256; Gradient Norm 0.29270654916763306; Learning Rate: 0.0001857259090825114; Time: 3260.84041595459 ms; Tokens/Sec : 160783.09059062556\n",
            "\n",
            "Training\n",
            "Step: 16069; Training Loss: 3.0146822929382324; Gradient Norm 0.3136632442474365; Learning Rate: 0.00018568264969203406; Time: 3260.6985569000244 ms; Tokens/Sec : 160790.08557554163\n",
            "\n",
            "Training\n",
            "Step: 16070; Training Loss: 3.0247292518615723; Gradient Norm 0.3393190801143646; Learning Rate: 0.00018563940007068906; Time: 3261.544704437256 ms; Tokens/Sec : 160748.37155741523\n",
            "\n",
            "Training\n",
            "Step: 16071; Training Loss: 3.0360660552978516; Gradient Norm 0.2932663857936859; Learning Rate: 0.00018559616021963612; Time: 3259.0696811676025 ms; Tokens/Sec : 160870.44810044297\n",
            "\n",
            "Training\n",
            "Step: 16072; Training Loss: 3.050337314605713; Gradient Norm 0.3275856375694275; Learning Rate: 0.00018555293014003483; Time: 3260.3752613067627 ms; Tokens/Sec : 160806.0293617443\n",
            "\n",
            "Training\n",
            "Step: 16073; Training Loss: 2.993366241455078; Gradient Norm 0.32999539375305176; Learning Rate: 0.00018550970983304403; Time: 3260.132074356079 ms; Tokens/Sec : 160818.02455919032\n",
            "\n",
            "Training\n",
            "Step: 16074; Training Loss: 3.0340723991394043; Gradient Norm 0.30596664547920227; Learning Rate: 0.00018546649929982305; Time: 3260.826587677002 ms; Tokens/Sec : 160783.7724279292\n",
            "\n",
            "Training\n",
            "Step: 16075; Training Loss: 3.0348236560821533; Gradient Norm 0.32375282049179077; Learning Rate: 0.00018542329854153052; Time: 3262.0317935943604 ms; Tokens/Sec : 160724.36848394378\n",
            "\n",
            "Training\n",
            "Step: 16076; Training Loss: 3.043037176132202; Gradient Norm 0.300414115190506; Learning Rate: 0.0001853801075593247; Time: 3260.3626251220703 ms; Tokens/Sec : 160806.65259753747\n",
            "\n",
            "Training\n",
            "Step: 16077; Training Loss: 3.0537846088409424; Gradient Norm 0.3264973759651184; Learning Rate: 0.00018533692635436372; Time: 3266.023635864258 ms; Tokens/Sec : 160527.92583702857\n",
            "\n",
            "Training\n",
            "Step: 16078; Training Loss: 3.0231752395629883; Gradient Norm 0.28358033299446106; Learning Rate: 0.0001852937549278057; Time: 3262.4661922454834 ms; Tokens/Sec : 160702.9679713383\n",
            "\n",
            "Training\n",
            "Step: 16079; Training Loss: 3.080458641052246; Gradient Norm 0.3547270596027374; Learning Rate: 0.00018525059328080827; Time: 3259.8330974578857 ms; Tokens/Sec : 160832.77404872517\n",
            "\n",
            "Training\n",
            "Step: 16080; Training Loss: 3.0779733657836914; Gradient Norm 0.3047957122325897; Learning Rate: 0.00018520744141452858; Time: 3261.042833328247 ms; Tokens/Sec : 160773.11056503584\n",
            "\n",
            "Training\n",
            "Step: 16081; Training Loss: 3.0287766456604004; Gradient Norm 0.30817484855651855; Learning Rate: 0.00018516429933012382; Time: 3259.587526321411 ms; Tokens/Sec : 160844.89088460902\n",
            "\n",
            "Training\n",
            "Step: 16082; Training Loss: 3.0925800800323486; Gradient Norm 0.32211461663246155; Learning Rate: 0.00018512116702875108; Time: 3260.1406574249268 ms; Tokens/Sec : 160817.60116881493\n",
            "\n",
            "Training\n",
            "Step: 16083; Training Loss: 3.0563666820526123; Gradient Norm 0.29927733540534973; Learning Rate: 0.00018507804451156664; Time: 3262.465476989746 ms; Tokens/Sec : 160703.00320350265\n",
            "\n",
            "Training\n",
            "Step: 16084; Training Loss: 3.0661051273345947; Gradient Norm 0.27846014499664307; Learning Rate: 0.00018503493177972693; Time: 3263.0574703216553 ms; Tokens/Sec : 160673.847999471\n",
            "\n",
            "Training\n",
            "Step: 16085; Training Loss: 3.047492504119873; Gradient Norm 0.3039308786392212; Learning Rate: 0.00018499182883438797; Time: 7868.684530258179 ms; Tokens/Sec : 66629.68860727698\n",
            "\n",
            "Training\n",
            "Step: 16086; Training Loss: 3.080286741256714; Gradient Norm 0.2864817976951599; Learning Rate: 0.0001849487356767058; Time: 3252.6001930236816 ms; Tokens/Sec : 161190.42270381577\n",
            "\n",
            "Training\n",
            "Step: 16087; Training Loss: 3.05686616897583; Gradient Norm 0.27808716893196106; Learning Rate: 0.00018490565230783567; Time: 3256.746530532837 ms; Tokens/Sec : 160985.20258935262\n",
            "\n",
            "Training\n",
            "Step: 16088; Training Loss: 3.0600109100341797; Gradient Norm 0.2848086953163147; Learning Rate: 0.000184862578728933; Time: 3251.6930103302 ms; Tokens/Sec : 161235.39286593357\n",
            "\n",
            "Training\n",
            "Step: 16089; Training Loss: 3.0985922813415527; Gradient Norm 0.3338509202003479; Learning Rate: 0.00018481951494115273; Time: 3253.1347274780273 ms; Tokens/Sec : 161163.93691645566\n",
            "\n",
            "Training\n",
            "Step: 16090; Training Loss: 3.061614990234375; Gradient Norm 0.356342613697052; Learning Rate: 0.0001847764609456497; Time: 3248.4588623046875 ms; Tokens/Sec : 161395.9179486216\n",
            "\n",
            "Training\n",
            "Step: 16091; Training Loss: 3.036503553390503; Gradient Norm 0.2909751236438751; Learning Rate: 0.00018473341674357825; Time: 3249.8059272766113 ms; Tokens/Sec : 161329.0183267533\n",
            "\n",
            "Training\n",
            "Step: 16092; Training Loss: 3.0539228916168213; Gradient Norm 0.306537002325058; Learning Rate: 0.00018469038233609274; Time: 3252.131223678589 ms; Tokens/Sec : 161213.66695866632\n",
            "\n",
            "Training\n",
            "Step: 16093; Training Loss: 3.1083168983459473; Gradient Norm 0.28772401809692383; Learning Rate: 0.00018464735772434713; Time: 3253.9258003234863 ms; Tokens/Sec : 161124.75580969866\n",
            "\n",
            "Training\n",
            "Step: 16094; Training Loss: 3.0326013565063477; Gradient Norm 0.31529363989830017; Learning Rate: 0.00018460434290949512; Time: 3252.0134449005127 ms; Tokens/Sec : 161219.5056641407\n",
            "\n",
            "Training\n",
            "Step: 16095; Training Loss: 3.0269391536712646; Gradient Norm 0.3039848208427429; Learning Rate: 0.00018456133789269003; Time: 3253.4379959106445 ms; Tokens/Sec : 161148.91405921834\n",
            "\n",
            "Training\n",
            "Step: 16096; Training Loss: 3.0546984672546387; Gradient Norm 0.31063082814216614; Learning Rate: 0.00018451834267508514; Time: 3250.0009536743164 ms; Tokens/Sec : 161319.3372781204\n",
            "\n",
            "Training\n",
            "Step: 16097; Training Loss: 3.011629819869995; Gradient Norm 0.3066973388195038; Learning Rate: 0.00018447535725783337; Time: 3250.544786453247 ms; Tokens/Sec : 161292.34772736792\n",
            "\n",
            "Training\n",
            "Step: 16098; Training Loss: 3.02847957611084; Gradient Norm 0.30875730514526367; Learning Rate: 0.00018443238164208734; Time: 3253.01194190979 ms; Tokens/Sec : 161170.02008059\n",
            "\n",
            "Training\n",
            "Step: 16099; Training Loss: 3.045194149017334; Gradient Norm 0.3122609257698059; Learning Rate: 0.00018438941582899957; Time: 3251.5876293182373 ms; Tokens/Sec : 161240.61835907766\n",
            "\n",
            "Training\n",
            "Step: 16100; Training Loss: 3.01676869392395; Gradient Norm 0.28395214676856995; Learning Rate: 0.00018434645981972174; Time: 3253.782272338867 ms; Tokens/Sec : 161131.86320335255\n",
            "\n",
            "Training\n",
            "Step: 16101; Training Loss: 3.0465574264526367; Gradient Norm 0.28288355469703674; Learning Rate: 0.00018430351361540626; Time: 3251.13582611084 ms; Tokens/Sec : 161263.02561378301\n",
            "\n",
            "Training\n",
            "Step: 16102; Training Loss: 3.0247838497161865; Gradient Norm 0.3040473759174347; Learning Rate: 0.00018426057721720437; Time: 3250.34761428833 ms; Tokens/Sec : 161302.13202282178\n",
            "\n",
            "Training\n",
            "Step: 16103; Training Loss: 3.079230785369873; Gradient Norm 0.3308578431606293; Learning Rate: 0.00018421765062626763; Time: 3248.2376098632812 ms; Tokens/Sec : 161406.91136879835\n",
            "\n",
            "Training\n",
            "Step: 16104; Training Loss: 2.978710889816284; Gradient Norm 0.29900768399238586; Learning Rate: 0.00018417473384374695; Time: 3252.9361248016357 ms; Tokens/Sec : 161173.77651612237\n",
            "\n",
            "Training\n",
            "Step: 16105; Training Loss: 3.0931034088134766; Gradient Norm 0.2918143570423126; Learning Rate: 0.0001841318268707932; Time: 3250.933885574341 ms; Tokens/Sec : 161273.04290206268\n",
            "\n",
            "Training\n",
            "Step: 16106; Training Loss: 3.026317596435547; Gradient Norm 0.3295508027076721; Learning Rate: 0.00018408892970855705; Time: 3250.916004180908 ms; Tokens/Sec : 161273.92997103848\n",
            "\n",
            "Training\n",
            "Step: 16107; Training Loss: 3.050980806350708; Gradient Norm 0.31323951482772827; Learning Rate: 0.00018404604235818846; Time: 3250.333547592163 ms; Tokens/Sec : 161302.8301013571\n",
            "\n",
            "Training\n",
            "Step: 16108; Training Loss: 2.966181755065918; Gradient Norm 0.3128686547279358; Learning Rate: 0.0001840031648208377; Time: 3253.5219192504883 ms; Tokens/Sec : 161144.7572852928\n",
            "\n",
            "Training\n",
            "Step: 16109; Training Loss: 3.041069507598877; Gradient Norm 0.32686376571655273; Learning Rate: 0.0001839602970976546; Time: 3251.016616821289 ms; Tokens/Sec : 161268.93885661752\n",
            "\n",
            "Training\n",
            "Step: 16110; Training Loss: 3.0708038806915283; Gradient Norm 0.3211303949356079; Learning Rate: 0.00018391743918978845; Time: 3252.469539642334 ms; Tokens/Sec : 161196.8978063526\n",
            "\n",
            "Training\n",
            "Step: 16111; Training Loss: 3.012463331222534; Gradient Norm 0.3286292552947998; Learning Rate: 0.00018387459109838838; Time: 3252.7387142181396 ms; Tokens/Sec : 161183.55824532406\n",
            "\n",
            "Training\n",
            "Step: 16112; Training Loss: 3.067373037338257; Gradient Norm 0.3483256995677948; Learning Rate: 0.00018383175282460373; Time: 3254.242420196533 ms; Tokens/Sec : 161109.07925794192\n",
            "\n",
            "Training\n",
            "Step: 16113; Training Loss: 3.0315394401550293; Gradient Norm 0.3065922260284424; Learning Rate: 0.0001837889243695831; Time: 3254.3389797210693 ms; Tokens/Sec : 161104.29898883397\n",
            "\n",
            "Training\n",
            "Step: 16114; Training Loss: 3.0380239486694336; Gradient Norm 0.2902323305606842; Learning Rate: 0.00018374610573447465; Time: 3256.9267749786377 ms; Tokens/Sec : 160976.29336583376\n",
            "\n",
            "Training\n",
            "Step: 16115; Training Loss: 3.087993860244751; Gradient Norm 0.3212195634841919; Learning Rate: 0.00018370329692042668; Time: 3252.3741722106934 ms; Tokens/Sec : 161201.62448702287\n",
            "\n",
            "Training\n",
            "Step: 16116; Training Loss: 3.0873029232025146; Gradient Norm 0.308647096157074; Learning Rate: 0.00018366049792858743; Time: 3252.472162246704 ms; Tokens/Sec : 161196.7678265503\n",
            "\n",
            "Training\n",
            "Step: 16117; Training Loss: 3.0457584857940674; Gradient Norm 0.2796769440174103; Learning Rate: 0.0001836177087601042; Time: 3255.169153213501 ms; Tokens/Sec : 161063.212178213\n",
            "\n",
            "Training\n",
            "Step: 16118; Training Loss: 3.0640976428985596; Gradient Norm 0.315416157245636; Learning Rate: 0.0001835749294161244; Time: 3253.4327507019043 ms; Tokens/Sec : 161149.17386470913\n",
            "\n",
            "Training\n",
            "Step: 16119; Training Loss: 3.0568835735321045; Gradient Norm 0.2669638991355896; Learning Rate: 0.00018353215989779512; Time: 3253.4635066986084 ms; Tokens/Sec : 161147.65047173112\n",
            "\n",
            "Training\n",
            "Step: 16120; Training Loss: 3.113236427307129; Gradient Norm 0.3005092740058899; Learning Rate: 0.0001834894002062636; Time: 3252.120018005371 ms; Tokens/Sec : 161214.22244482924\n",
            "\n",
            "Training\n",
            "Step: 16121; Training Loss: 3.047783136367798; Gradient Norm 0.2735067307949066; Learning Rate: 0.00018344665034267596; Time: 3258.2507133483887 ms; Tokens/Sec : 160910.88320861835\n",
            "\n",
            "Training\n",
            "Step: 16122; Training Loss: 3.0579607486724854; Gradient Norm 0.2870822250843048; Learning Rate: 0.00018340391030817878; Time: 3255.8603286743164 ms; Tokens/Sec : 161029.02061940523\n",
            "\n",
            "Training\n",
            "Step: 16123; Training Loss: 3.0307741165161133; Gradient Norm 0.3071717321872711; Learning Rate: 0.0001833611801039181; Time: 3255.650043487549 ms; Tokens/Sec : 161039.4216198886\n",
            "\n",
            "Training\n",
            "Step: 16124; Training Loss: 2.985036849975586; Gradient Norm 0.31141114234924316; Learning Rate: 0.00018331845973103966; Time: 3254.4732093811035 ms; Tokens/Sec : 161097.65429585538\n",
            "\n",
            "Training\n",
            "Step: 16125; Training Loss: 3.0126054286956787; Gradient Norm 0.2655888795852661; Learning Rate: 0.00018327574919068908; Time: 3253.5600662231445 ms; Tokens/Sec : 161142.8679134894\n",
            "\n",
            "Training\n",
            "Step: 16126; Training Loss: 2.9948275089263916; Gradient Norm 0.32917460799217224; Learning Rate: 0.0001832330484840116; Time: 3251.5857219696045 ms; Tokens/Sec : 161240.71294125982\n",
            "\n",
            "Training\n",
            "Step: 16127; Training Loss: 3.001218557357788; Gradient Norm 0.3783467710018158; Learning Rate: 0.00018319035761215228; Time: 3253.873586654663 ms; Tokens/Sec : 161127.34131722222\n",
            "\n",
            "Training\n",
            "Step: 16128; Training Loss: 3.026871681213379; Gradient Norm 0.28177085518836975; Learning Rate: 0.00018314767657625578; Time: 3256.9758892059326 ms; Tokens/Sec : 160973.86589122834\n",
            "\n",
            "Training\n",
            "Step: 16129; Training Loss: 3.0046017169952393; Gradient Norm 0.3420383930206299; Learning Rate: 0.00018310500537746665; Time: 3255.324125289917 ms; Tokens/Sec : 161055.54464666627\n",
            "\n",
            "Training\n",
            "Step: 16130; Training Loss: 3.04270601272583; Gradient Norm 0.32171663641929626; Learning Rate: 0.00018306234401692915; Time: 3254.375457763672 ms; Tokens/Sec : 161102.4931832168\n",
            "\n",
            "Training\n",
            "Step: 16131; Training Loss: 3.0415687561035156; Gradient Norm 0.33802396059036255; Learning Rate: 0.00018301969249578717; Time: 3254.5318603515625 ms; Tokens/Sec : 161094.75110296358\n",
            "\n",
            "Training\n",
            "Step: 16132; Training Loss: 3.0501506328582764; Gradient Norm 0.3546161353588104; Learning Rate: 0.00018297705081518438; Time: 3254.477024078369 ms; Tokens/Sec : 161097.46546711982\n",
            "\n",
            "Training\n",
            "Step: 16133; Training Loss: 3.0262033939361572; Gradient Norm 0.2827628552913666; Learning Rate: 0.00018293441897626448; Time: 3266.3867473602295 ms; Tokens/Sec : 160510.08057258063\n",
            "\n",
            "Training\n",
            "Step: 16134; Training Loss: 3.0272247791290283; Gradient Norm 0.3338790535926819; Learning Rate: 0.00018289179698017014; Time: 3253.878355026245 ms; Tokens/Sec : 161127.1051943708\n",
            "\n",
            "Training\n",
            "Step: 16135; Training Loss: 3.060718536376953; Gradient Norm 0.3889956772327423; Learning Rate: 0.00018284918482804467; Time: 3255.1703453063965 ms; Tokens/Sec : 161063.1531944147\n",
            "\n",
            "Training\n",
            "Step: 16136; Training Loss: 3.0275344848632812; Gradient Norm 0.35236209630966187; Learning Rate: 0.0001828065825210307; Time: 3260.16902923584 ms; Tokens/Sec : 160816.20164427158\n",
            "\n",
            "Training\n",
            "Step: 16137; Training Loss: 2.9961960315704346; Gradient Norm 0.3923589289188385; Learning Rate: 0.0001827639900602704; Time: 3255.772829055786 ms; Tokens/Sec : 161033.34831013068\n",
            "\n",
            "Training\n",
            "Step: 16138; Training Loss: 3.0667614936828613; Gradient Norm 0.3811572790145874; Learning Rate: 0.0001827214074469058; Time: 3255.695581436157 ms; Tokens/Sec : 161037.1691350594\n",
            "\n",
            "Training\n",
            "Step: 16139; Training Loss: 3.0632362365722656; Gradient Norm 0.3718538284301758; Learning Rate: 0.00018267883468207912; Time: 3256.0250759124756 ms; Tokens/Sec : 161020.87292834267\n",
            "\n",
            "Training\n",
            "Step: 16140; Training Loss: 2.9914939403533936; Gradient Norm 0.37694790959358215; Learning Rate: 0.0001826362717669318; Time: 3254.8999786376953 ms; Tokens/Sec : 161076.5318261593\n",
            "\n",
            "Training\n",
            "Step: 16141; Training Loss: 3.0328075885772705; Gradient Norm 0.36764100193977356; Learning Rate: 0.00018259371870260504; Time: 3253.714084625244 ms; Tokens/Sec : 161135.24002536517\n",
            "\n",
            "Training\n",
            "Step: 16142; Training Loss: 3.03220272064209; Gradient Norm 0.35158613324165344; Learning Rate: 0.00018255117549023983; Time: 3254.2033195495605 ms; Tokens/Sec : 161111.0150525477\n",
            "\n",
            "Training\n",
            "Step: 16143; Training Loss: 3.015941619873047; Gradient Norm 0.3433884382247925; Learning Rate: 0.00018250864213097736; Time: 3256.5252780914307 ms; Tokens/Sec : 160996.14012739132\n",
            "\n",
            "Training\n",
            "Step: 16144; Training Loss: 3.0382354259490967; Gradient Norm 0.3393290042877197; Learning Rate: 0.00018246611862595774; Time: 3256.377696990967 ms; Tokens/Sec : 161003.43657446883\n",
            "\n",
            "Training\n",
            "Step: 16145; Training Loss: 3.0003159046173096; Gradient Norm 0.31883159279823303; Learning Rate: 0.0001824236049763214; Time: 3257.230043411255 ms; Tokens/Sec : 160961.3054689008\n",
            "\n",
            "Training\n",
            "Step: 16146; Training Loss: 3.0482144355773926; Gradient Norm 0.33425450325012207; Learning Rate: 0.00018238110118320826; Time: 3257.7290534973145 ms; Tokens/Sec : 160936.64985341672\n",
            "\n",
            "Training\n",
            "Step: 16147; Training Loss: 3.085520029067993; Gradient Norm 0.31430351734161377; Learning Rate: 0.00018233860724775828; Time: 3253.5574436187744 ms; Tokens/Sec : 161142.99780638263\n",
            "\n",
            "Training\n",
            "Step: 16148; Training Loss: 3.089047908782959; Gradient Norm 0.31988269090652466; Learning Rate: 0.00018229612317111068; Time: 3255.9962272644043 ms; Tokens/Sec : 161022.29959906676\n",
            "\n",
            "Training\n",
            "Step: 16149; Training Loss: 3.018893003463745; Gradient Norm 0.29208651185035706; Learning Rate: 0.00018225364895440465; Time: 3255.025625228882 ms; Tokens/Sec : 161070.31414326697\n",
            "\n",
            "Training\n",
            "Step: 16150; Training Loss: 3.0333430767059326; Gradient Norm 0.27452048659324646; Learning Rate: 0.00018221118459877948; Time: 3256.171464920044 ms; Tokens/Sec : 161013.633848325\n",
            "\n",
            "Training\n",
            "Step: 16151; Training Loss: 3.054232358932495; Gradient Norm 0.27831214666366577; Learning Rate: 0.00018216873010537348; Time: 3279.4926166534424 ms; Tokens/Sec : 159868.63252493297\n",
            "\n",
            "Training\n",
            "Step: 16152; Training Loss: 3.037710189819336; Gradient Norm 0.318173348903656; Learning Rate: 0.0001821262854753252; Time: 3256.779432296753 ms; Tokens/Sec : 160983.5762289436\n",
            "\n",
            "Training\n",
            "Step: 16153; Training Loss: 3.191110372543335; Gradient Norm 0.34108445048332214; Learning Rate: 0.0001820838507097728; Time: 3255.98406791687 ms; Tokens/Sec : 161022.90093066444\n",
            "\n",
            "Training\n",
            "Step: 16154; Training Loss: 3.0129737854003906; Gradient Norm 0.3215177655220032; Learning Rate: 0.00018204142580985412; Time: 3255.091667175293 ms; Tokens/Sec : 161067.0462177697\n",
            "\n",
            "Training\n",
            "Step: 16155; Training Loss: 3.0555648803710938; Gradient Norm 0.3438095450401306; Learning Rate: 0.0001819990107767068; Time: 3256.277322769165 ms; Tokens/Sec : 161008.39947936044\n",
            "\n",
            "Training\n",
            "Step: 16156; Training Loss: 3.048703670501709; Gradient Norm 0.3263484239578247; Learning Rate: 0.00018195660561146822; Time: 3254.4286251068115 ms; Tokens/Sec : 161099.8612645231\n",
            "\n",
            "Training\n",
            "Step: 16157; Training Loss: 3.0659055709838867; Gradient Norm 0.3498365879058838; Learning Rate: 0.00018191421031527543; Time: 3258.209466934204 ms; Tokens/Sec : 160912.92021606155\n",
            "\n",
            "Training\n",
            "Step: 16158; Training Loss: 3.0291781425476074; Gradient Norm 0.31565502285957336; Learning Rate: 0.00018187182488926527; Time: 3255.4006576538086 ms; Tokens/Sec : 161051.7583349812\n",
            "\n",
            "Training\n",
            "Step: 16159; Training Loss: 2.97824764251709; Gradient Norm 0.3148338794708252; Learning Rate: 0.00018182944933457432; Time: 3255.5723190307617 ms; Tokens/Sec : 161043.26632070926\n",
            "\n",
            "Training\n",
            "Step: 16160; Training Loss: 3.0187020301818848; Gradient Norm 0.27737852931022644; Learning Rate: 0.00018178708365233884; Time: 3255.8655738830566 ms; Tokens/Sec : 161028.76120119303\n",
            "\n",
            "Training\n",
            "Step: 16161; Training Loss: 3.0244107246398926; Gradient Norm 0.33232739567756653; Learning Rate: 0.00018174472784369491; Time: 3255.6586265563965 ms; Tokens/Sec : 161038.9970629551\n",
            "\n",
            "Training\n",
            "Step: 16162; Training Loss: 3.0240821838378906; Gradient Norm 0.29286497831344604; Learning Rate: 0.00018170238190977827; Time: 3256.490468978882 ms; Tokens/Sec : 160997.86103915662\n",
            "\n",
            "Training\n",
            "Step: 16163; Training Loss: 2.9310829639434814; Gradient Norm 0.34408819675445557; Learning Rate: 0.00018166004585172443; Time: 3258.368730545044 ms; Tokens/Sec : 160905.05506180073\n",
            "\n",
            "Training\n",
            "Step: 16164; Training Loss: 3.020305633544922; Gradient Norm 0.2851879298686981; Learning Rate: 0.00018161771967066856; Time: 3256.4761638641357 ms; Tokens/Sec : 160998.5682738361\n",
            "\n",
            "Training\n",
            "Step: 16165; Training Loss: 3.0450990200042725; Gradient Norm 0.32524415850639343; Learning Rate: 0.00018157540336774574; Time: 3257.40909576416 ms; Tokens/Sec : 160952.4577928418\n",
            "\n",
            "Training\n",
            "Step: 16166; Training Loss: 3.0030734539031982; Gradient Norm 0.2846875786781311; Learning Rate: 0.0001815330969440906; Time: 3255.6142807006836 ms; Tokens/Sec : 161041.19063120743\n",
            "\n",
            "Training\n",
            "Step: 16167; Training Loss: 3.0450313091278076; Gradient Norm 0.2903115749359131; Learning Rate: 0.00018149080040083776; Time: 3257.798671722412 ms; Tokens/Sec : 160933.21068327612\n",
            "\n",
            "Training\n",
            "Step: 16168; Training Loss: 3.0527188777923584; Gradient Norm 0.315206378698349; Learning Rate: 0.00018144851373912098; Time: 3258.223056793213 ms; Tokens/Sec : 160912.24905762324\n",
            "\n",
            "Training\n",
            "Step: 16169; Training Loss: 2.983537435531616; Gradient Norm 0.289652556180954; Learning Rate: 0.00018140623696007456; Time: 3257.525682449341 ms; Tokens/Sec : 160946.6973122332\n",
            "\n",
            "Training\n",
            "Step: 16170; Training Loss: 3.039588212966919; Gradient Norm 0.266437828540802; Learning Rate: 0.00018136397006483215; Time: 3259.0904235839844 ms; Tokens/Sec : 160869.42424366565\n",
            "\n",
            "Training\n",
            "Step: 16171; Training Loss: 3.056307315826416; Gradient Norm 0.32829463481903076; Learning Rate: 0.0001813217130545269; Time: 3256.6118240356445 ms; Tokens/Sec : 160991.86158155443\n",
            "\n",
            "Training\n",
            "Step: 16172; Training Loss: 3.074936628341675; Gradient Norm 0.3389025330543518; Learning Rate: 0.00018127946593029192; Time: 3258.863687515259 ms; Tokens/Sec : 160880.61676484134\n",
            "\n",
            "Training\n",
            "Step: 16173; Training Loss: 3.0717649459838867; Gradient Norm 0.3314915895462036; Learning Rate: 0.0001812372286932603; Time: 3257.050037384033 ms; Tokens/Sec : 160970.20125029847\n",
            "\n",
            "Training\n",
            "Step: 16174; Training Loss: 3.039191961288452; Gradient Norm 0.33596673607826233; Learning Rate: 0.0001811950013445646; Time: 3257.4126720428467 ms; Tokens/Sec : 160952.2810848523\n",
            "\n",
            "Training\n",
            "Step: 16175; Training Loss: 3.07291316986084; Gradient Norm 0.39269688725471497; Learning Rate: 0.00018115278388533697; Time: 3257.148027420044 ms; Tokens/Sec : 160965.35852418214\n",
            "\n",
            "Training\n",
            "Step: 16176; Training Loss: 3.041929006576538; Gradient Norm 0.3198545575141907; Learning Rate: 0.0001811105763167094; Time: 3256.1919689178467 ms; Tokens/Sec : 161012.6199574899\n",
            "\n",
            "Training\n",
            "Step: 16177; Training Loss: 3.0568506717681885; Gradient Norm 0.37141910195350647; Learning Rate: 0.00018106837863981408; Time: 3261.5129947662354 ms; Tokens/Sec : 160749.93441428174\n",
            "\n",
            "Training\n",
            "Step: 16178; Training Loss: 3.079648494720459; Gradient Norm 0.2951452136039734; Learning Rate: 0.00018102619085578212; Time: 3257.1640014648438 ms; Tokens/Sec : 160964.5691049674\n",
            "\n",
            "Training\n",
            "Step: 16179; Training Loss: 3.013601779937744; Gradient Norm 0.2835560142993927; Learning Rate: 0.000180984012965745; Time: 3256.8156719207764 ms; Tokens/Sec : 160981.7849134796\n",
            "\n",
            "Training\n",
            "Step: 16180; Training Loss: 3.0518126487731934; Gradient Norm 0.31042271852493286; Learning Rate: 0.00018094184497083365; Time: 3255.14554977417 ms; Tokens/Sec : 161064.38006631474\n",
            "\n",
            "Training\n",
            "Step: 16181; Training Loss: 3.02677059173584; Gradient Norm 0.3216446340084076; Learning Rate: 0.00018089968687217875; Time: 3260.9071731567383 ms; Tokens/Sec : 160779.79904360793\n",
            "\n",
            "Training\n",
            "Step: 16182; Training Loss: 3.0310261249542236; Gradient Norm 0.28738340735435486; Learning Rate: 0.00018085753867091092; Time: 3261.8324756622314 ms; Tokens/Sec : 160734.18972675988\n",
            "\n",
            "Training\n",
            "Step: 16183; Training Loss: 3.0500340461730957; Gradient Norm 0.35915109515190125; Learning Rate: 0.0001808154003681602; Time: 3265.2270793914795 ms; Tokens/Sec : 160567.0868372525\n",
            "\n",
            "Training\n",
            "Step: 16184; Training Loss: 3.0804364681243896; Gradient Norm 0.30914565920829773; Learning Rate: 0.0001807732719650566; Time: 3259.620428085327 ms; Tokens/Sec : 160843.26735796113\n",
            "\n",
            "Training\n",
            "Step: 16185; Training Loss: 3.053405523300171; Gradient Norm 0.2838798761367798; Learning Rate: 0.00018073115346272978; Time: 3262.432336807251 ms; Tokens/Sec : 160704.6356440574\n",
            "\n",
            "Training\n",
            "Step: 16186; Training Loss: 3.0428011417388916; Gradient Norm 0.28177040815353394; Learning Rate: 0.00018068904486230917; Time: 3262.0060443878174 ms; Tokens/Sec : 160725.63718942876\n",
            "\n",
            "Training\n",
            "Step: 16187; Training Loss: 3.027230739593506; Gradient Norm 0.28591546416282654; Learning Rate: 0.00018064694616492388; Time: 3262.2125148773193 ms; Tokens/Sec : 160715.4646145782\n",
            "\n",
            "Training\n",
            "Step: 16188; Training Loss: 3.097996950149536; Gradient Norm 0.3142454922199249; Learning Rate: 0.00018060485737170272; Time: 3261.4409923553467 ms; Tokens/Sec : 160753.48326978923\n",
            "\n",
            "Training\n",
            "Step: 16189; Training Loss: 3.046917200088501; Gradient Norm 0.297721803188324; Learning Rate: 0.0001805627784837744; Time: 3260.986566543579 ms; Tokens/Sec : 160775.88462920568\n",
            "\n",
            "Training\n",
            "Step: 16190; Training Loss: 3.0383434295654297; Gradient Norm 0.32423532009124756; Learning Rate: 0.0001805207095022672; Time: 3261.4846229553223 ms; Tokens/Sec : 160751.33278565883\n",
            "\n",
            "Training\n",
            "Step: 16191; Training Loss: 3.0847105979919434; Gradient Norm 0.29885074496269226; Learning Rate: 0.00018047865042830928; Time: 3261.3742351531982 ms; Tokens/Sec : 160756.77373939037\n",
            "\n",
            "Training\n",
            "Step: 16192; Training Loss: 3.022489070892334; Gradient Norm 0.3222311735153198; Learning Rate: 0.0001804366012630283; Time: 3262.647867202759 ms; Tokens/Sec : 160694.0195018655\n",
            "\n",
            "Training\n",
            "Step: 16193; Training Loss: 3.089606285095215; Gradient Norm 0.3338507413864136; Learning Rate: 0.00018039456200755194; Time: 3260.7924938201904 ms; Tokens/Sec : 160785.45353426307\n",
            "\n",
            "Training\n",
            "Step: 16194; Training Loss: 3.043712615966797; Gradient Norm 0.3083714544773102; Learning Rate: 0.00018035253266300753; Time: 3263.336181640625 ms; Tokens/Sec : 160660.1253495179\n",
            "\n",
            "Training\n",
            "Step: 16195; Training Loss: 3.0376336574554443; Gradient Norm 0.3144511282444; Learning Rate: 0.00018031051323052173; Time: 3262.6900672912598 ms; Tokens/Sec : 160691.9410629992\n",
            "\n",
            "Training\n",
            "Step: 16196; Training Loss: 3.026662826538086; Gradient Norm 0.3349335193634033; Learning Rate: 0.0001802685037112217; Time: 3261.3229751586914 ms; Tokens/Sec : 160759.3004414072\n",
            "\n",
            "Training\n",
            "Step: 16197; Training Loss: 3.035344123840332; Gradient Norm 0.302117258310318; Learning Rate: 0.00018022650410623383; Time: 3258.983373641968 ms; Tokens/Sec : 160874.70842605113\n",
            "\n",
            "Training\n",
            "Step: 16198; Training Loss: 3.031273365020752; Gradient Norm 0.2627653479576111; Learning Rate: 0.00018018451441668417; Time: 3261.930465698242 ms; Tokens/Sec : 160729.36119065064\n",
            "\n",
            "Training\n",
            "Step: 16199; Training Loss: 3.0574378967285156; Gradient Norm 0.34781768918037415; Learning Rate: 0.00018014253464369861; Time: 3263.3683681488037 ms; Tokens/Sec : 160658.54076333114\n",
            "\n",
            "Training\n",
            "Step: 16200; Training Loss: 3.05631685256958; Gradient Norm 0.2981751263141632; Learning Rate: 0.00018010056478840313; Time: 3261.1641883850098 ms; Tokens/Sec : 160767.1278457272\n",
            "\n",
            "Training\n",
            "Step: 16201; Training Loss: 2.9828896522521973; Gradient Norm 0.3012148439884186; Learning Rate: 0.00018005860485192313; Time: 3261.8255615234375 ms; Tokens/Sec : 160734.53043734533\n",
            "\n",
            "Training\n",
            "Step: 16202; Training Loss: 3.038078784942627; Gradient Norm 0.2994503974914551; Learning Rate: 0.00018001665483538347; Time: 3261.636734008789 ms; Tokens/Sec : 160743.83591933976\n",
            "\n",
            "Training\n",
            "Step: 16203; Training Loss: 3.0429532527923584; Gradient Norm 0.27962884306907654; Learning Rate: 0.00017997471473990914; Time: 3260.3330612182617 ms; Tokens/Sec : 160808.1107529835\n",
            "\n",
            "Training\n",
            "Step: 16204; Training Loss: 3.03254771232605; Gradient Norm 0.30637356638908386; Learning Rate: 0.00017993278456662504; Time: 3265.692710876465 ms; Tokens/Sec : 160544.19273860235\n",
            "\n",
            "Training\n",
            "Step: 16205; Training Loss: 3.016068935394287; Gradient Norm 0.29477283358573914; Learning Rate: 0.00017989086431665513; Time: 3262.880802154541 ms; Tokens/Sec : 160682.54765966407\n",
            "\n",
            "Training\n",
            "Step: 16206; Training Loss: 3.0763051509857178; Gradient Norm 0.297629714012146; Learning Rate: 0.00017984895399112362; Time: 3261.672019958496 ms; Tokens/Sec : 160742.0969342808\n",
            "\n",
            "Training\n",
            "Step: 16207; Training Loss: 3.0492100715637207; Gradient Norm 0.3484160304069519; Learning Rate: 0.00017980705359115447; Time: 3261.5833282470703 ms; Tokens/Sec : 160746.46796829725\n",
            "\n",
            "Training\n",
            "Step: 16208; Training Loss: 3.0353832244873047; Gradient Norm 0.3335418403148651; Learning Rate: 0.00017976516311787129; Time: 3263.6728286743164 ms; Tokens/Sec : 160643.5532978845\n",
            "\n",
            "Training\n",
            "Step: 16209; Training Loss: 2.993253707885742; Gradient Norm 0.3571023941040039; Learning Rate: 0.00017972328257239708; Time: 3261.0411643981934 ms; Tokens/Sec : 160773.1928452226\n",
            "\n",
            "Training\n",
            "Step: 16210; Training Loss: 2.9960103034973145; Gradient Norm 0.3209924101829529; Learning Rate: 0.00017968141195585485; Time: 3261.5582942962646 ms; Tokens/Sec : 160747.701770918\n",
            "\n",
            "Training\n",
            "Step: 16211; Training Loss: 3.0989367961883545; Gradient Norm 0.33884894847869873; Learning Rate: 0.00017963955126936782; Time: 3262.8695964813232 ms; Tokens/Sec : 160683.09949174552\n",
            "\n",
            "Training\n",
            "Step: 16212; Training Loss: 3.035067558288574; Gradient Norm 0.3373759090900421; Learning Rate: 0.00017959770051405798; Time: 3262.195348739624 ms; Tokens/Sec : 160716.31032229966\n",
            "\n",
            "Training\n",
            "Step: 16213; Training Loss: 3.0276222229003906; Gradient Norm 0.33356034755706787; Learning Rate: 0.0001795558596910478; Time: 3261.763095855713 ms; Tokens/Sec : 160737.60864672938\n",
            "\n",
            "Training\n",
            "Step: 16214; Training Loss: 3.012356758117676; Gradient Norm 0.3680388629436493; Learning Rate: 0.00017951402880145919; Time: 3262.028455734253 ms; Tokens/Sec : 160724.53294463598\n",
            "\n",
            "Training\n",
            "Step: 16215; Training Loss: 3.0794730186462402; Gradient Norm 0.31216898560523987; Learning Rate: 0.0001794722078464138; Time: 3263.798236846924 ms; Tokens/Sec : 160637.38073052638\n",
            "\n",
            "Training\n",
            "Step: 16216; Training Loss: 3.1580023765563965; Gradient Norm 0.31028690934181213; Learning Rate: 0.0001794303968270331; Time: 3262.0761394500732 ms; Tokens/Sec : 160722.1835381149\n",
            "\n",
            "Training\n",
            "Step: 16217; Training Loss: 3.014014482498169; Gradient Norm 0.2981295883655548; Learning Rate: 0.00017938859574443823; Time: 3261.059045791626 ms; Tokens/Sec : 160772.31127617575\n",
            "\n",
            "Training\n",
            "Step: 16218; Training Loss: 3.0627737045288086; Gradient Norm 0.30236294865608215; Learning Rate: 0.0001793468045997501; Time: 3260.3981494903564 ms; Tokens/Sec : 160804.900494117\n",
            "\n",
            "Training\n",
            "Step: 16219; Training Loss: 3.0443332195281982; Gradient Norm 0.3149837851524353; Learning Rate: 0.00017930502339408934; Time: 3265.4924392700195 ms; Tokens/Sec : 160554.0388625739\n",
            "\n",
            "Training\n",
            "Step: 16220; Training Loss: 3.067126750946045; Gradient Norm 0.30042582750320435; Learning Rate: 0.0001792632521285763; Time: 3259.6898078918457 ms; Tokens/Sec : 160839.84394179986\n",
            "\n",
            "Training\n",
            "Step: 16221; Training Loss: 3.027616500854492; Gradient Norm 0.28383904695510864; Learning Rate: 0.00017922149080433104; Time: 3262.3302936553955 ms; Tokens/Sec : 160709.6623599515\n",
            "\n",
            "Training\n",
            "Step: 16222; Training Loss: 3.081132173538208; Gradient Norm 0.28529274463653564; Learning Rate: 0.0001791797394224734; Time: 3261.918783187866 ms; Tokens/Sec : 160729.936840308\n",
            "\n",
            "Training\n",
            "Step: 16223; Training Loss: 3.0510752201080322; Gradient Norm 0.2922957241535187; Learning Rate: 0.00017913799798412297; Time: 3262.3026371002197 ms; Tokens/Sec : 160711.02479505906\n",
            "\n",
            "Training\n",
            "Step: 16224; Training Loss: 3.047236919403076; Gradient Norm 0.2791332006454468; Learning Rate: 0.00017909626649039915; Time: 3261.608600616455 ms; Tokens/Sec : 160745.22243438647\n",
            "\n",
            "Training\n",
            "Step: 16225; Training Loss: 3.02689528465271; Gradient Norm 0.2839216887950897; Learning Rate: 0.00017905454494242056; Time: 3260.5695724487305 ms; Tokens/Sec : 160796.4462498044\n",
            "\n",
            "Training\n",
            "Step: 16226; Training Loss: 2.9856972694396973; Gradient Norm 0.3088664412498474; Learning Rate: 0.0001790128333413064; Time: 3264.3213272094727 ms; Tokens/Sec : 160611.639433239\n",
            "\n",
            "Training\n",
            "Step: 16227; Training Loss: 3.0574545860290527; Gradient Norm 0.27150794863700867; Learning Rate: 0.000178971131688175; Time: 3259.6335411071777 ms; Tokens/Sec : 160842.62030937336\n",
            "\n",
            "Training\n",
            "Step: 16228; Training Loss: 3.0283281803131104; Gradient Norm 0.3343087434768677; Learning Rate: 0.00017892943998414467; Time: 3261.545419692993 ms; Tokens/Sec : 160748.33630535516\n",
            "\n",
            "Training\n",
            "Step: 16229; Training Loss: 3.0376956462860107; Gradient Norm 0.3155684173107147; Learning Rate: 0.00017888775823033304; Time: 3262.253999710083 ms; Tokens/Sec : 160713.42085766268\n",
            "\n",
            "Training\n",
            "Step: 16230; Training Loss: 3.0569007396698; Gradient Norm 0.32274848222732544; Learning Rate: 0.00017884608642785817; Time: 3260.545492172241 ms; Tokens/Sec : 160797.63378817597\n",
            "\n",
            "Training\n",
            "Step: 16231; Training Loss: 3.010334014892578; Gradient Norm 0.2715942859649658; Learning Rate: 0.00017880442457783753; Time: 3262.1214389801025 ms; Tokens/Sec : 160719.9516655388\n",
            "\n",
            "Training\n",
            "Step: 16232; Training Loss: 3.0108449459075928; Gradient Norm 0.3667432963848114; Learning Rate: 0.00017876277268138793; Time: 3261.1958980560303 ms; Tokens/Sec : 160765.5646545255\n",
            "\n",
            "Training\n",
            "Step: 16233; Training Loss: 2.9830069541931152; Gradient Norm 0.2694043517112732; Learning Rate: 0.00017872113073962636; Time: 3260.1406574249268 ms; Tokens/Sec : 160817.60116881493\n",
            "\n",
            "Training\n",
            "Step: 16234; Training Loss: 3.071199417114258; Gradient Norm 0.26888030767440796; Learning Rate: 0.0001786794987536696; Time: 3264.587879180908 ms; Tokens/Sec : 160598.5255730181\n",
            "\n",
            "Training\n",
            "Step: 16235; Training Loss: 3.04803204536438; Gradient Norm 0.30270591378211975; Learning Rate: 0.00017863787672463407; Time: 3261.6279125213623 ms; Tokens/Sec : 160744.27067148362\n",
            "\n",
            "Training\n",
            "Step: 16236; Training Loss: 3.0357539653778076; Gradient Norm 0.2717975378036499; Learning Rate: 0.00017859626465363562; Time: 3261.202335357666 ms; Tokens/Sec : 160765.24731867018\n",
            "\n",
            "Training\n",
            "Step: 16237; Training Loss: 3.084411144256592; Gradient Norm 0.3259440064430237; Learning Rate: 0.00017855466254179008; Time: 3261.101007461548 ms; Tokens/Sec : 160770.2425654419\n",
            "\n",
            "Training\n",
            "Step: 16238; Training Loss: 3.004563093185425; Gradient Norm 0.24697346985340118; Learning Rate: 0.0001785130703902133; Time: 3262.04514503479 ms; Tokens/Sec : 160723.71064454058\n",
            "\n",
            "Training\n",
            "Step: 16239; Training Loss: 3.076885938644409; Gradient Norm 0.29989996552467346; Learning Rate: 0.00017847148820002027; Time: 3261.1913681030273 ms; Tokens/Sec : 160765.78796569313\n",
            "\n",
            "Training\n",
            "Step: 16240; Training Loss: 3.026287794113159; Gradient Norm 0.25491011142730713; Learning Rate: 0.0001784299159723261; Time: 3262.3772621154785 ms; Tokens/Sec : 160707.3486222213\n",
            "\n",
            "Training\n",
            "Step: 16241; Training Loss: 3.0422611236572266; Gradient Norm 0.28044480085372925; Learning Rate: 0.00017838835370824555; Time: 3260.0138187408447 ms; Tokens/Sec : 160823.85816465717\n",
            "\n",
            "Training\n",
            "Step: 16242; Training Loss: 3.012521266937256; Gradient Norm 0.41035744547843933; Learning Rate: 0.00017834680140889312; Time: 3261.43217086792 ms; Tokens/Sec : 160753.91807411972\n",
            "\n",
            "Training\n",
            "Step: 16243; Training Loss: 3.030423641204834; Gradient Norm 0.2976636588573456; Learning Rate: 0.00017830525907538305; Time: 3260.7338428497314 ms; Tokens/Sec : 160788.34558965304\n",
            "\n",
            "Training\n",
            "Step: 16244; Training Loss: 3.114995241165161; Gradient Norm 0.36551913619041443; Learning Rate: 0.00017826372670882925; Time: 3262.918710708618 ms; Tokens/Sec : 160680.68085157376\n",
            "\n",
            "Training\n",
            "Step: 16245; Training Loss: 3.145566701889038; Gradient Norm 0.3383167088031769; Learning Rate: 0.00017822220431034546; Time: 3262.4504566192627 ms; Tokens/Sec : 160703.74308252244\n",
            "\n",
            "Training\n",
            "Step: 16246; Training Loss: 3.08530592918396; Gradient Norm 0.28385475277900696; Learning Rate: 0.00017818069188104503; Time: 3259.5808506011963 ms; Tokens/Sec : 160845.22029981262\n",
            "\n",
            "Training\n",
            "Step: 16247; Training Loss: 3.0031609535217285; Gradient Norm 0.3654332458972931; Learning Rate: 0.00017813918942204116; Time: 3262.9005908966064 ms; Tokens/Sec : 160681.57315694742\n",
            "\n",
            "Training\n",
            "Step: 16248; Training Loss: 3.074143886566162; Gradient Norm 0.33202388882637024; Learning Rate: 0.00017809769693444668; Time: 3261.537790298462 ms; Tokens/Sec : 160748.71232812625\n",
            "\n",
            "Training\n",
            "Step: 16249; Training Loss: 3.0804195404052734; Gradient Norm 0.3336144983768463; Learning Rate: 0.00017805621441937432; Time: 3261.812686920166 ms; Tokens/Sec : 160735.16486780165\n",
            "\n",
            "Evaluating HellaSwag\n",
            "Evaluating Validation set\n",
            "Training\n",
            "Step: 16250; Training Loss: 3.0525214672088623; Gradient Norm 0.32475125789642334; Learning Rate: 0.00017801474187793633; Time: 3255.2616596221924 ms; Tokens/Sec : 161058.63516386243; Validation_loss: 2.9383387565612793; HellaSwag Acc: 0.331\n",
            "\n",
            "Checkpointing\n",
            "Training\n",
            "Step: 16251; Training Loss: 3.0233240127563477; Gradient Norm 0.31870803236961365; Learning Rate: 0.0001779732793112448; Time: 3272.7367877960205 ms; Tokens/Sec : 160198.6453524344\n",
            "\n",
            "Training\n",
            "Step: 16252; Training Loss: 3.0651700496673584; Gradient Norm 0.3298414647579193; Learning Rate: 0.00017793182672041157; Time: 3243.4825897216797 ms; Tokens/Sec : 161643.5376164571\n",
            "\n",
            "Training\n",
            "Step: 16253; Training Loss: 3.039736270904541; Gradient Norm 0.3077440559864044; Learning Rate: 0.0001778903841065482; Time: 3246.6824054718018 ms; Tokens/Sec : 161484.22744287841\n",
            "\n",
            "Training\n",
            "Step: 16254; Training Loss: 3.09945011138916; Gradient Norm 0.30778807401657104; Learning Rate: 0.00017784895147076593; Time: 3251.1017322540283 ms; Tokens/Sec : 161264.71675695758\n",
            "\n",
            "Training\n",
            "Step: 16255; Training Loss: 3.089240789413452; Gradient Norm 0.2764562964439392; Learning Rate: 0.00017780752881417592; Time: 3250.0510215759277 ms; Tokens/Sec : 161316.85211076358\n",
            "\n",
            "Training\n",
            "Step: 16256; Training Loss: 3.029876470565796; Gradient Norm 0.3001374900341034; Learning Rate: 0.00017776611613788858; Time: 3250.731945037842 ms; Tokens/Sec : 161283.06143492146\n",
            "\n",
            "Training\n",
            "Step: 16257; Training Loss: 3.0628647804260254; Gradient Norm 0.32099056243896484; Learning Rate: 0.00017772471344301466; Time: 3250.9777545928955 ms; Tokens/Sec : 161270.86666751248\n",
            "\n",
            "Training\n",
            "Step: 16258; Training Loss: 3.105557680130005; Gradient Norm 0.2666504681110382; Learning Rate: 0.00017768332073066445; Time: 3250.3931522369385 ms; Tokens/Sec : 161299.8721829026\n",
            "\n",
            "Training\n",
            "Step: 16259; Training Loss: 3.0599021911621094; Gradient Norm 0.33106669783592224; Learning Rate: 0.00017764193800194763; Time: 3276.1425971984863 ms; Tokens/Sec : 160032.10618742058\n",
            "\n",
            "Training\n",
            "Step: 16260; Training Loss: 3.026789426803589; Gradient Norm 0.286930114030838; Learning Rate: 0.00017760056525797387; Time: 3250.9303092956543 ms; Tokens/Sec : 161273.22031507717\n",
            "\n",
            "Training\n",
            "Step: 16261; Training Loss: 3.061753988265991; Gradient Norm 0.28108108043670654; Learning Rate: 0.0001775592024998528; Time: 3250.591039657593 ms; Tokens/Sec : 161290.0526715372\n",
            "\n",
            "Training\n",
            "Step: 16262; Training Loss: 3.0676965713500977; Gradient Norm 0.2739282548427582; Learning Rate: 0.00017751784972869355; Time: 3248.626708984375 ms; Tokens/Sec : 161387.5791115161\n",
            "\n",
            "Training\n",
            "Step: 16263; Training Loss: 3.0560545921325684; Gradient Norm 0.2896381914615631; Learning Rate: 0.00017747650694560467; Time: 3251.5223026275635 ms; Tokens/Sec : 161243.85786199945\n",
            "\n",
            "Training\n",
            "Step: 16264; Training Loss: 3.018343448638916; Gradient Norm 0.3327714800834656; Learning Rate: 0.0001774351741516951; Time: 3249.906301498413 ms; Tokens/Sec : 161324.03563704898\n",
            "\n",
            "Training\n",
            "Step: 16265; Training Loss: 2.9671506881713867; Gradient Norm 0.3111046552658081; Learning Rate: 0.00017739385134807318; Time: 3248.044013977051 ms; Tokens/Sec : 161416.53184004678\n",
            "\n",
            "Training\n",
            "Step: 16266; Training Loss: 3.0453498363494873; Gradient Norm 0.318531334400177; Learning Rate: 0.00017735253853584673; Time: 3251.829147338867 ms; Tokens/Sec : 161228.64278679917\n",
            "\n",
            "Training\n",
            "Step: 16267; Training Loss: 2.9830338954925537; Gradient Norm 0.32379013299942017; Learning Rate: 0.00017731123571612358; Time: 3248.0921745300293 ms; Tokens/Sec : 161414.13846294553\n",
            "\n",
            "Training\n",
            "Step: 16268; Training Loss: 3.0287506580352783; Gradient Norm 0.31510472297668457; Learning Rate: 0.00017726994289001154; Time: 3249.150514602661 ms; Tokens/Sec : 161361.5613200102\n",
            "\n",
            "Training\n",
            "Step: 16269; Training Loss: 2.976823091506958; Gradient Norm 0.30062365531921387; Learning Rate: 0.0001772286600586176; Time: 3251.786231994629 ms; Tokens/Sec : 161230.7705966282\n",
            "\n",
            "Training\n",
            "Step: 16270; Training Loss: 3.020406723022461; Gradient Norm 0.3118382692337036; Learning Rate: 0.00017718738722304886; Time: 3251.8551349639893 ms; Tokens/Sec : 161227.354307038\n",
            "\n",
            "Training\n",
            "Step: 16271; Training Loss: 2.995120048522949; Gradient Norm 0.26963579654693604; Learning Rate: 0.00017714612438441188; Time: 3252.336263656616 ms; Tokens/Sec : 161203.50341958203\n",
            "\n",
            "Training\n",
            "Step: 16272; Training Loss: 3.027756929397583; Gradient Norm 0.27811163663864136; Learning Rate: 0.00017710487154381355; Time: 3251.6884803771973 ms; Tokens/Sec : 161235.6174842377\n",
            "\n",
            "Training\n",
            "Step: 16273; Training Loss: 2.967472791671753; Gradient Norm 0.2817938029766083; Learning Rate: 0.00017706362870235963; Time: 3250.338077545166 ms; Tokens/Sec : 161302.60529574545\n",
            "\n",
            "Training\n",
            "Step: 16274; Training Loss: 3.0335655212402344; Gradient Norm 0.311026394367218; Learning Rate: 0.00017702239586115618; Time: 3258.2991123199463 ms; Tokens/Sec : 160908.49302865288\n",
            "\n",
            "Training\n",
            "Step: 16275; Training Loss: 3.0043704509735107; Gradient Norm 0.28891876339912415; Learning Rate: 0.00017698117302130886; Time: 3253.3438205718994 ms; Tokens/Sec : 161153.57887622106\n",
            "\n",
            "Training\n",
            "Step: 16276; Training Loss: 3.0029678344726562; Gradient Norm 0.3231528699398041; Learning Rate: 0.00017693996018392313; Time: 3250.4727840423584 ms; Tokens/Sec : 161295.92057312478\n",
            "\n",
            "Training\n",
            "Step: 16277; Training Loss: 3.0650055408477783; Gradient Norm 0.38861289620399475; Learning Rate: 0.00017689875735010395; Time: 3253.6611557006836 ms; Tokens/Sec : 161137.86129247173\n",
            "\n",
            "Training\n",
            "Step: 16278; Training Loss: 3.0120930671691895; Gradient Norm 0.380190908908844; Learning Rate: 0.00017685756452095628; Time: 3254.8346519470215 ms; Tokens/Sec : 161079.76473900885\n",
            "\n",
            "Training\n",
            "Step: 16279; Training Loss: 3.064495801925659; Gradient Norm 0.3810355067253113; Learning Rate: 0.00017681638169758472; Time: 3253.272294998169 ms; Tokens/Sec : 161157.12195566314\n",
            "\n",
            "Training\n",
            "Step: 16280; Training Loss: 3.0401391983032227; Gradient Norm 0.4442864656448364; Learning Rate: 0.00017677520888109355; Time: 3253.6535263061523 ms; Tokens/Sec : 161138.23913980788\n",
            "\n",
            "Training\n",
            "Step: 16281; Training Loss: 3.0659029483795166; Gradient Norm 0.3542073965072632; Learning Rate: 0.00017673404607258675; Time: 9016.111373901367 ms; Tokens/Sec : 58150.12462220007\n",
            "\n",
            "Training\n",
            "Step: 16282; Training Loss: 3.0623345375061035; Gradient Norm 0.3880680203437805; Learning Rate: 0.0001766928932731682; Time: 3245.5337047576904 ms; Tokens/Sec : 161541.3820018064\n",
            "\n",
            "Training\n",
            "Step: 16283; Training Loss: 3.0362071990966797; Gradient Norm 0.29818859696388245; Learning Rate: 0.00017665175048394135; Time: 3248.69704246521 ms; Tokens/Sec : 161384.08511067388\n",
            "\n",
            "Training\n",
            "Step: 16284; Training Loss: 3.056260585784912; Gradient Norm 0.357132226228714; Learning Rate: 0.0001766106177060095; Time: 3248.7640380859375 ms; Tokens/Sec : 161380.75706750707\n",
            "\n",
            "Training\n",
            "Step: 16285; Training Loss: 3.0451080799102783; Gradient Norm 0.3228895664215088; Learning Rate: 0.00017656949494047563; Time: 3248.234510421753 ms; Tokens/Sec : 161407.06538208848\n",
            "\n",
            "Training\n",
            "Step: 16286; Training Loss: 3.035539150238037; Gradient Norm 0.3166380822658539; Learning Rate: 0.0001765283821884422; Time: 3249.30739402771 ms; Tokens/Sec : 161353.77064159935\n",
            "\n",
            "Training\n",
            "Step: 16287; Training Loss: 3.050149440765381; Gradient Norm 0.3126842975616455; Learning Rate: 0.0001764872794510119; Time: 3247.3955154418945 ms; Tokens/Sec : 161448.76640585516\n",
            "\n",
            "Training\n",
            "Step: 16288; Training Loss: 3.082441568374634; Gradient Norm 0.33274349570274353; Learning Rate: 0.00017644618672928684; Time: 3248.690366744995 ms; Tokens/Sec : 161384.4167381538\n",
            "\n",
            "Training\n",
            "Step: 16289; Training Loss: 3.0475120544433594; Gradient Norm 0.30863168835639954; Learning Rate: 0.000176405104024369; Time: 3248.9967346191406 ms; Tokens/Sec : 161369.19880944694\n",
            "\n",
            "Training\n",
            "Step: 16290; Training Loss: 3.106081485748291; Gradient Norm 0.29180285334587097; Learning Rate: 0.00017636403133735972; Time: 3249.1791248321533 ms; Tokens/Sec : 161360.14047150564\n",
            "\n",
            "Training\n",
            "Step: 16291; Training Loss: 3.051917552947998; Gradient Norm 0.27623292803764343; Learning Rate: 0.00017632296866936068; Time: 3244.07696723938 ms; Tokens/Sec : 161613.9214003158\n",
            "\n",
            "Training\n",
            "Step: 16292; Training Loss: 3.044644355773926; Gradient Norm 0.31829604506492615; Learning Rate: 0.00017628191602147295; Time: 3247.145652770996 ms; Tokens/Sec : 161461.18963052725\n",
            "\n",
            "Training\n",
            "Step: 16293; Training Loss: 3.020989418029785; Gradient Norm 0.274664968252182; Learning Rate: 0.0001762408733947971; Time: 3247.0688819885254 ms; Tokens/Sec : 161465.0070739869\n",
            "\n",
            "Training\n",
            "Step: 16294; Training Loss: 3.050743341445923; Gradient Norm 0.32236167788505554; Learning Rate: 0.0001761998407904338; Time: 3249.4890689849854 ms; Tokens/Sec : 161344.7495497399\n",
            "\n",
            "Training\n",
            "Step: 16295; Training Loss: 3.050478458404541; Gradient Norm 0.2764369249343872; Learning Rate: 0.0001761588182094834; Time: 3246.342182159424 ms; Tokens/Sec : 161501.15132079224\n",
            "\n",
            "Training\n",
            "Step: 16296; Training Loss: 3.0182361602783203; Gradient Norm 0.3099741041660309; Learning Rate: 0.00017611780565304598; Time: 3244.7614669799805 ms; Tokens/Sec : 161579.82808146888\n",
            "\n",
            "Training\n",
            "Step: 16297; Training Loss: 3.069732189178467; Gradient Norm 0.34563547372817993; Learning Rate: 0.00017607680312222112; Time: 3246.8955516815186 ms; Tokens/Sec : 161473.6266242008\n",
            "\n",
            "Training\n",
            "Step: 16298; Training Loss: 3.0137147903442383; Gradient Norm 0.31048575043678284; Learning Rate: 0.00017603581061810826; Time: 3251.159906387329 ms; Tokens/Sec : 161261.831191375\n",
            "\n",
            "Training\n",
            "Step: 16299; Training Loss: 3.065596103668213; Gradient Norm 0.3086075782775879; Learning Rate: 0.00017599482814180693; Time: 3249.976634979248 ms; Tokens/Sec : 161320.5443870361\n",
            "\n",
            "Training\n",
            "Step: 16300; Training Loss: 2.9805760383605957; Gradient Norm 0.34296950697898865; Learning Rate: 0.0001759538556944157; Time: 3251.816987991333 ms; Tokens/Sec : 161229.24566054865\n",
            "\n",
            "Training\n",
            "Step: 16301; Training Loss: 3.005277633666992; Gradient Norm 0.3332971930503845; Learning Rate: 0.00017591289327703347; Time: 3244.575023651123 ms; Tokens/Sec : 161589.11295878072\n",
            "\n",
            "Training\n",
            "Step: 16302; Training Loss: 2.9924733638763428; Gradient Norm 0.3255213499069214; Learning Rate: 0.00017587194089075856; Time: 3249.720335006714 ms; Tokens/Sec : 161333.26746681938\n",
            "\n",
            "Training\n",
            "Step: 16303; Training Loss: 3.000415325164795; Gradient Norm 0.28135472536087036; Learning Rate: 0.00017583099853668913; Time: 3248.485565185547 ms; Tokens/Sec : 161394.59125780468\n",
            "\n",
            "Training\n",
            "Step: 16304; Training Loss: 3.0652873516082764; Gradient Norm 0.3180304169654846; Learning Rate: 0.00017579006621592296; Time: 3245.457410812378 ms; Tokens/Sec : 161545.17950329973\n",
            "\n",
            "Training\n",
            "Step: 16305; Training Loss: 3.0780105590820312; Gradient Norm 0.29109954833984375; Learning Rate: 0.00017574914392955778; Time: 3250.343084335327 ms; Tokens/Sec : 161302.35682711424\n",
            "\n",
            "Training\n",
            "Step: 16306; Training Loss: 3.0617904663085938; Gradient Norm 0.4192013740539551; Learning Rate: 0.00017570823167869086; Time: 3251.2760162353516 ms; Tokens/Sec : 161256.07219502467\n",
            "\n",
            "Training\n",
            "Step: 16307; Training Loss: 2.991473436355591; Gradient Norm 0.33600395917892456; Learning Rate: 0.0001756673294644192; Time: 3255.6793689727783 ms; Tokens/Sec : 161037.97105960766\n",
            "\n",
            "Training\n",
            "Step: 16308; Training Loss: 3.0003669261932373; Gradient Norm 0.30362701416015625; Learning Rate: 0.0001756264372878397; Time: 3255.030870437622 ms; Tokens/Sec : 161070.05459198984\n",
            "\n",
            "Training\n",
            "Step: 16309; Training Loss: 2.9892971515655518; Gradient Norm 0.31857922673225403; Learning Rate: 0.0001755855551500488; Time: 3256.3459873199463 ms; Tokens/Sec : 161005.0043949728\n",
            "\n",
            "Training\n",
            "Step: 16310; Training Loss: 2.9917776584625244; Gradient Norm 0.3587694764137268; Learning Rate: 0.00017554468305214276; Time: 3255.077600479126 ms; Tokens/Sec : 161067.74226298885\n",
            "\n",
            "Training\n",
            "Step: 16311; Training Loss: 3.034559965133667; Gradient Norm 0.30323171615600586; Learning Rate: 0.0001755038209952176; Time: 3256.6850185394287 ms; Tokens/Sec : 160988.24326435316\n",
            "\n",
            "Training\n",
            "Step: 16312; Training Loss: 2.9923722743988037; Gradient Norm 0.3552217185497284; Learning Rate: 0.00017546296898036912; Time: 3257.0323944091797 ms; Tokens/Sec : 160971.0732076108\n",
            "\n",
            "Training\n",
            "Step: 16313; Training Loss: 3.1333813667297363; Gradient Norm 0.3335129916667938; Learning Rate: 0.00017542212700869236; Time: 3257.530689239502 ms; Tokens/Sec : 160946.44993886442\n",
            "\n",
            "Training\n",
            "Step: 16314; Training Loss: 2.9906296730041504; Gradient Norm 0.37227293848991394; Learning Rate: 0.00017538129508128293; Time: 3256.92081451416 ms; Tokens/Sec : 160976.58796724808\n",
            "\n",
            "Training\n",
            "Step: 16315; Training Loss: 3.0244064331054688; Gradient Norm 0.3535928726196289; Learning Rate: 0.0001753404731992356; Time: 3261.982202529907 ms; Tokens/Sec : 160726.81193458874\n",
            "\n",
            "Training\n",
            "Step: 16316; Training Loss: 3.0281624794006348; Gradient Norm 0.3000063896179199; Learning Rate: 0.0001752996613636451; Time: 3258.7270736694336 ms; Tokens/Sec : 160887.36127558988\n",
            "\n",
            "Training\n",
            "Step: 16317; Training Loss: 3.055983543395996; Gradient Norm 0.38314110040664673; Learning Rate: 0.00017525885957560549; Time: 3259.795665740967 ms; Tokens/Sec : 160834.62086597594\n",
            "\n",
            "Training\n",
            "Step: 16318; Training Loss: 2.9983439445495605; Gradient Norm 0.34609997272491455; Learning Rate: 0.0001752180678362111; Time: 3258.335590362549 ms; Tokens/Sec : 160906.69160989136\n",
            "\n",
            "Training\n",
            "Step: 16319; Training Loss: 3.0413858890533447; Gradient Norm 0.3540264964103699; Learning Rate: 0.00017517728614655588; Time: 3258.3184242248535 ms; Tokens/Sec : 160907.53933134294\n",
            "\n",
            "Training\n",
            "Step: 16320; Training Loss: 3.0505783557891846; Gradient Norm 0.310429185628891; Learning Rate: 0.000175136514507733; Time: 3257.281541824341 ms; Tokens/Sec : 160958.76063152845\n",
            "\n",
            "Training\n",
            "Step: 16321; Training Loss: 3.0010173320770264; Gradient Norm 0.3593730032444; Learning Rate: 0.00017509575292083603; Time: 3258.3022117614746 ms; Tokens/Sec : 160908.33996536006\n",
            "\n",
            "Training\n",
            "Step: 16322; Training Loss: 3.0328359603881836; Gradient Norm 0.32535332441329956; Learning Rate: 0.000175055001386958; Time: 3259.5133781433105 ms; Tokens/Sec : 160848.5498220737\n",
            "\n",
            "Training\n",
            "Step: 16323; Training Loss: 3.043119430541992; Gradient Norm 0.35589277744293213; Learning Rate: 0.00017501425990719166; Time: 3259.829044342041 ms; Tokens/Sec : 160832.97402052\n",
            "\n",
            "Training\n",
            "Step: 16324; Training Loss: 3.06571888923645; Gradient Norm 0.31334176659584045; Learning Rate: 0.00017497352848262917; Time: 3256.4685344696045 ms; Tokens/Sec : 160998.9454682058\n",
            "\n",
            "Training\n",
            "Step: 16325; Training Loss: 3.1052916049957275; Gradient Norm 0.3430016040802002; Learning Rate: 0.00017493280711436312; Time: 3260.7078552246094 ms; Tokens/Sec : 160789.62706209236\n",
            "\n",
            "Training\n",
            "Step: 16326; Training Loss: 3.053319215774536; Gradient Norm 0.30591341853141785; Learning Rate: 0.00017489209580348543; Time: 3259.575128555298 ms; Tokens/Sec : 160845.50265677535\n",
            "\n",
            "Training\n",
            "Step: 16327; Training Loss: 3.0814785957336426; Gradient Norm 0.36256706714630127; Learning Rate: 0.00017485139455108756; Time: 3258.8186264038086 ms; Tokens/Sec : 160882.84133154273\n",
            "\n",
            "Training\n",
            "Step: 16328; Training Loss: 3.124906539916992; Gradient Norm 0.38379794359207153; Learning Rate: 0.00017481070335826086; Time: 3259.211301803589 ms; Tokens/Sec : 160863.45788929623\n",
            "\n",
            "Training\n",
            "Step: 16329; Training Loss: 3.0137252807617188; Gradient Norm 0.4124101996421814; Learning Rate: 0.00017477002222609684; Time: 3261.9285583496094 ms; Tokens/Sec : 160729.45517398653\n",
            "\n",
            "Training\n",
            "Step: 16330; Training Loss: 3.052015542984009; Gradient Norm 0.3701249361038208; Learning Rate: 0.000174729351155686; Time: 3259.9117755889893 ms; Tokens/Sec : 160828.89234180993\n",
            "\n",
            "Training\n",
            "Step: 16331; Training Loss: 3.0556414127349854; Gradient Norm 0.2985561192035675; Learning Rate: 0.00017468869014811904; Time: 3259.5319747924805 ms; Tokens/Sec : 160847.6321307997\n",
            "\n",
            "Training\n",
            "Step: 16332; Training Loss: 3.0797059535980225; Gradient Norm 0.34986332058906555; Learning Rate: 0.00017464803920448624; Time: 3258.1355571746826 ms; Tokens/Sec : 160916.57047401686\n",
            "\n",
            "Training\n",
            "Step: 16333; Training Loss: 3.036130428314209; Gradient Norm 0.3186787962913513; Learning Rate: 0.0001746073983258779; Time: 3259.183645248413 ms; Tokens/Sec : 160864.8229333021\n",
            "\n",
            "Training\n",
            "Step: 16334; Training Loss: 3.103147506713867; Gradient Norm 0.2836841344833374; Learning Rate: 0.00017456676751338344; Time: 3258.721351623535 ms; Tokens/Sec : 160887.64378052554\n",
            "\n",
            "Training\n",
            "Step: 16335; Training Loss: 3.015955686569214; Gradient Norm 0.29790619015693665; Learning Rate: 0.00017452614676809262; Time: 3261.970281600952 ms; Tokens/Sec : 160727.39931360845\n",
            "\n",
            "Training\n",
            "Step: 16336; Training Loss: 3.0015406608581543; Gradient Norm 0.2818951904773712; Learning Rate: 0.00017448553609109452; Time: 3260.4005336761475 ms; Tokens/Sec : 160804.78290465064\n",
            "\n",
            "Training\n",
            "Step: 16337; Training Loss: 2.9740705490112305; Gradient Norm 0.30243343114852905; Learning Rate: 0.00017444493548347828; Time: 3258.922815322876 ms; Tokens/Sec : 160877.6978500046\n",
            "\n",
            "Training\n",
            "Step: 16338; Training Loss: 3.009705066680908; Gradient Norm 0.29609543085098267; Learning Rate: 0.00017440434494633242; Time: 3259.3514919281006 ms; Tokens/Sec : 160856.53888462714\n",
            "\n",
            "Training\n",
            "Step: 16339; Training Loss: 2.9980804920196533; Gradient Norm 0.3296223282814026; Learning Rate: 0.00017436376448074552; Time: 3260.815143585205 ms; Tokens/Sec : 160784.33671145036\n",
            "\n",
            "Training\n",
            "Step: 16340; Training Loss: 2.950075626373291; Gradient Norm 0.3007814884185791; Learning Rate: 0.00017432319408780566; Time: 3259.7084045410156 ms; Tokens/Sec : 160838.92634986245\n",
            "\n",
            "Training\n",
            "Step: 16341; Training Loss: 2.9575788974761963; Gradient Norm 0.28473031520843506; Learning Rate: 0.00017428263376860077; Time: 3257.7552795410156 ms; Tokens/Sec : 160935.35425836738\n",
            "\n",
            "Training\n",
            "Step: 16342; Training Loss: 3.0496654510498047; Gradient Norm 0.3461340367794037; Learning Rate: 0.00017424208352421846; Time: 3259.1488361358643 ms; Tokens/Sec : 160866.54103885914\n",
            "\n",
            "Training\n",
            "Step: 16343; Training Loss: 3.0214507579803467; Gradient Norm 0.2658056318759918; Learning Rate: 0.0001742015433557461; Time: 3260.493278503418 ms; Tokens/Sec : 160800.2088078681\n",
            "\n",
            "Training\n",
            "Step: 16344; Training Loss: 3.002094268798828; Gradient Norm 0.27645251154899597; Learning Rate: 0.00017416101326427066; Time: 3257.6301097869873 ms; Tokens/Sec : 160941.5379680054\n",
            "\n",
            "Training\n",
            "Step: 16345; Training Loss: 3.053147554397583; Gradient Norm 0.35759106278419495; Learning Rate: 0.0001741204932508791; Time: 3257.890462875366 ms; Tokens/Sec : 160928.67638566066\n",
            "\n",
            "Training\n",
            "Step: 16346; Training Loss: 3.021822929382324; Gradient Norm 0.327562153339386; Learning Rate: 0.000174079983316658; Time: 3260.8556747436523 ms; Tokens/Sec : 160782.33822513968\n",
            "\n",
            "Training\n",
            "Step: 16347; Training Loss: 3.0492167472839355; Gradient Norm 0.3051798343658447; Learning Rate: 0.00017403948346269332; Time: 3260.788917541504 ms; Tokens/Sec : 160785.62987612546\n",
            "\n",
            "Training\n",
            "Step: 16348; Training Loss: 3.050551414489746; Gradient Norm 0.4114101529121399; Learning Rate: 0.00017399899369007133; Time: 3260.3988647460938 ms; Tokens/Sec : 160804.86521725904\n",
            "\n",
            "Training\n",
            "Step: 16349; Training Loss: 3.0017781257629395; Gradient Norm 0.3272508978843689; Learning Rate: 0.0001739585139998777; Time: 3260.161876678467 ms; Tokens/Sec : 160816.55446328866\n",
            "\n",
            "Training\n",
            "Step: 16350; Training Loss: 3.100407123565674; Gradient Norm 0.31718331575393677; Learning Rate: 0.00017391804439319806; Time: 3260.0889205932617 ms; Tokens/Sec : 160820.15330569312\n",
            "\n",
            "Training\n",
            "Step: 16351; Training Loss: 3.0365045070648193; Gradient Norm 0.35090070962905884; Learning Rate: 0.00017387758487111716; Time: 3259.7391605377197 ms; Tokens/Sec : 160837.40881694184\n",
            "\n",
            "Training\n",
            "Step: 16352; Training Loss: 3.0427489280700684; Gradient Norm 0.33185622096061707; Learning Rate: 0.00017383713543472025; Time: 3258.911609649658 ms; Tokens/Sec : 160878.25102331094\n",
            "\n",
            "Training\n",
            "Step: 16353; Training Loss: 3.013833999633789; Gradient Norm 0.2961020767688751; Learning Rate: 0.00017379669608509206; Time: 3259.5324516296387 ms; Tokens/Sec : 160847.60860039192\n",
            "\n",
            "Training\n",
            "Step: 16354; Training Loss: 2.988912582397461; Gradient Norm 0.3143947422504425; Learning Rate: 0.00017375626682331668; Time: 3259.702682495117 ms; Tokens/Sec : 160839.20868472804\n",
            "\n",
            "Training\n",
            "Step: 16355; Training Loss: 3.0324673652648926; Gradient Norm 0.30921316146850586; Learning Rate: 0.00017371584765047825; Time: 3261.103630065918 ms; Tokens/Sec : 160770.11327278867\n",
            "\n",
            "Training\n",
            "Step: 16356; Training Loss: 3.04256534576416; Gradient Norm 0.2683347761631012; Learning Rate: 0.00017367543856766095; Time: 3259.084939956665 ms; Tokens/Sec : 160869.6949171786\n",
            "\n",
            "Training\n",
            "Step: 16357; Training Loss: 3.060504198074341; Gradient Norm 0.32290005683898926; Learning Rate: 0.00017363503957594796; Time: 3260.8020305633545 ms; Tokens/Sec : 160784.98329118773\n",
            "\n",
            "Training\n",
            "Step: 16358; Training Loss: 3.0280845165252686; Gradient Norm 0.3306179642677307; Learning Rate: 0.00017359465067642274; Time: 3262.1009349823 ms; Tokens/Sec : 160720.9618738682\n",
            "\n",
            "Training\n",
            "Step: 16359; Training Loss: 3.069201946258545; Gradient Norm 0.29881083965301514; Learning Rate: 0.00017355427187016817; Time: 3260.4117393493652 ms; Tokens/Sec : 160804.23023646235\n",
            "\n",
            "Training\n",
            "Step: 16360; Training Loss: 3.0626819133758545; Gradient Norm 0.3077099323272705; Learning Rate: 0.00017351390315826736; Time: 3258.9893341064453 ms; Tokens/Sec : 160874.4141974156\n",
            "\n",
            "Training\n",
            "Step: 16361; Training Loss: 3.0858142375946045; Gradient Norm 0.3791581392288208; Learning Rate: 0.00017347354454180243; Time: 3260.4353427886963 ms; Tokens/Sec : 160803.06611802615\n",
            "\n",
            "Training\n",
            "Step: 16362; Training Loss: 3.06843638420105; Gradient Norm 0.31433746218681335; Learning Rate: 0.0001734331960218556; Time: 3260.13445854187 ms; Tokens/Sec : 160817.90695052908\n",
            "\n",
            "Training\n",
            "Step: 16363; Training Loss: 3.06962251663208; Gradient Norm 0.3033624589443207; Learning Rate: 0.00017339285759950914; Time: 3259.477138519287 ms; Tokens/Sec : 160850.33817361676\n",
            "\n",
            "Training\n",
            "Step: 16364; Training Loss: 3.022106647491455; Gradient Norm 0.2984853982925415; Learning Rate: 0.00017335252927584444; Time: 3261.1992359161377 ms; Tokens/Sec : 160765.40010985153\n",
            "\n",
            "Training\n",
            "Step: 16365; Training Loss: 3.080014944076538; Gradient Norm 0.3145686388015747; Learning Rate: 0.00017331221105194294; Time: 3262.2525691986084 ms; Tokens/Sec : 160713.4913311738\n",
            "\n",
            "Training\n",
            "Step: 16366; Training Loss: 3.066833734512329; Gradient Norm 0.3304589092731476; Learning Rate: 0.0001732719029288857; Time: 3259.7262859344482 ms; Tokens/Sec : 160838.04405979603\n",
            "\n",
            "Training\n",
            "Step: 16367; Training Loss: 3.0671489238739014; Gradient Norm 0.3067465126514435; Learning Rate: 0.00017323160490775391; Time: 3261.16943359375 ms; Tokens/Sec : 160766.86927064814\n",
            "\n",
            "Training\n",
            "Step: 16368; Training Loss: 3.0258052349090576; Gradient Norm 0.3186874985694885; Learning Rate: 0.00017319131698962783; Time: 3259.784698486328 ms; Tokens/Sec : 160835.1619797012\n",
            "\n",
            "Training\n",
            "Step: 16369; Training Loss: 3.0315120220184326; Gradient Norm 0.29990309476852417; Learning Rate: 0.00017315103917558782; Time: 3262.145519256592 ms; Tokens/Sec : 160718.76527429704\n",
            "\n",
            "Training\n",
            "Step: 16370; Training Loss: 3.0619912147521973; Gradient Norm 0.2878028452396393; Learning Rate: 0.00017311077146671405; Time: 3262.5036239624023 ms; Tokens/Sec : 160701.1241762967\n",
            "\n",
            "Training\n",
            "Step: 16371; Training Loss: 3.013218402862549; Gradient Norm 0.2908645272254944; Learning Rate: 0.00017307051386408615; Time: 3259.7334384918213 ms; Tokens/Sec : 160837.69114647974\n",
            "\n",
            "Training\n",
            "Step: 16372; Training Loss: 2.992137908935547; Gradient Norm 0.28619375824928284; Learning Rate: 0.00017303026636878376; Time: 3262.4340057373047 ms; Tokens/Sec : 160704.5534340278\n",
            "\n",
            "Training\n",
            "Step: 16373; Training Loss: 3.00126314163208; Gradient Norm 0.286077082157135; Learning Rate: 0.000172990028981886; Time: 3262.3720169067383 ms; Tokens/Sec : 160707.60700587137\n",
            "\n",
            "Training\n",
            "Step: 16374; Training Loss: 3.028304100036621; Gradient Norm 0.29096075892448425; Learning Rate: 0.0001729498017044719; Time: 3262.8116607666016 ms; Tokens/Sec : 160685.95264147667\n",
            "\n",
            "Training\n",
            "Step: 16375; Training Loss: 2.9978647232055664; Gradient Norm 0.30619412660598755; Learning Rate: 0.00017290958453762014; Time: 3258.189916610718 ms; Tokens/Sec : 160913.88575205664\n",
            "\n",
            "Training\n",
            "Step: 16376; Training Loss: 3.0190341472625732; Gradient Norm 0.28424766659736633; Learning Rate: 0.0001728693774824091; Time: 3265.010118484497 ms; Tokens/Sec : 160577.75656859405\n",
            "\n",
            "Training\n",
            "Step: 16377; Training Loss: 3.0020270347595215; Gradient Norm 0.2672014832496643; Learning Rate: 0.00017282918053991697; Time: 3259.2525482177734 ms; Tokens/Sec : 160861.42213394647\n",
            "\n",
            "Training\n",
            "Step: 16378; Training Loss: 3.0185773372650146; Gradient Norm 0.32118698954582214; Learning Rate: 0.0001727889937112216; Time: 3258.8393688201904 ms; Tokens/Sec : 160881.8173170069\n",
            "\n",
            "Training\n",
            "Step: 16379; Training Loss: 2.982077121734619; Gradient Norm 0.28931838274002075; Learning Rate: 0.00017274881699740063; Time: 3260.941505432129 ms; Tokens/Sec : 160778.1063004757\n",
            "\n",
            "Training\n",
            "Step: 16380; Training Loss: 2.9621312618255615; Gradient Norm 0.26314282417297363; Learning Rate: 0.00017270865039953147; Time: 3260.2434158325195 ms; Tokens/Sec : 160812.53241826437\n",
            "\n",
            "Training\n",
            "Step: 16381; Training Loss: 3.047861337661743; Gradient Norm 0.299956738948822; Learning Rate: 0.0001726684939186909; Time: 3260.0669860839844 ms; Tokens/Sec : 160821.23534209293\n",
            "\n",
            "Training\n",
            "Step: 16382; Training Loss: 3.074789047241211; Gradient Norm 0.3444102108478546; Learning Rate: 0.00017262834755595594; Time: 3262.7196311950684 ms; Tokens/Sec : 160690.48501356028\n",
            "\n",
            "Training\n",
            "Step: 16383; Training Loss: 3.037384271621704; Gradient Norm 0.3268710672855377; Learning Rate: 0.00017258821131240314; Time: 3261.052370071411 ms; Tokens/Sec : 160772.64039415566\n",
            "\n",
            "Training\n",
            "Step: 16384; Training Loss: 3.042745351791382; Gradient Norm 0.34318700432777405; Learning Rate: 0.00017254808518910877; Time: 3260.844945907593 ms; Tokens/Sec : 160782.86723138706\n",
            "\n",
            "Training\n",
            "Step: 16385; Training Loss: 2.9347035884857178; Gradient Norm 0.32594114542007446; Learning Rate: 0.00017250796918714847; Time: 3259.8419189453125 ms; Tokens/Sec : 160832.33881771416\n",
            "\n",
            "Training\n",
            "Step: 16386; Training Loss: 3.0575249195098877; Gradient Norm 0.33301806449890137; Learning Rate: 0.00017246786330759836; Time: 3262.666702270508 ms; Tokens/Sec : 160693.09183041746\n",
            "\n",
            "Training\n",
            "Step: 16387; Training Loss: 3.059790849685669; Gradient Norm 0.32875654101371765; Learning Rate: 0.00017242776755153373; Time: 3261.3308429718018 ms; Tokens/Sec : 160758.9126168679\n",
            "\n",
            "Training\n",
            "Step: 16388; Training Loss: 3.0032219886779785; Gradient Norm 0.3045198619365692; Learning Rate: 0.0001723876819200297; Time: 3261.0268592834473 ms; Tokens/Sec : 160773.89810742097\n",
            "\n",
            "Training\n",
            "Step: 16389; Training Loss: 3.027919292449951; Gradient Norm 0.3566429316997528; Learning Rate: 0.00017234760641416101; Time: 3261.000871658325 ms; Tokens/Sec : 160775.1793495788\n",
            "\n",
            "Training\n",
            "Step: 16390; Training Loss: 3.026798963546753; Gradient Norm 0.3317342698574066; Learning Rate: 0.00017230754103500273; Time: 3261.1188888549805 ms; Tokens/Sec : 160769.36102874926\n",
            "\n",
            "Training\n",
            "Step: 16391; Training Loss: 2.9716992378234863; Gradient Norm 0.3783859610557556; Learning Rate: 0.0001722674857836288; Time: 3262.010097503662 ms; Tokens/Sec : 160725.43748445934\n",
            "\n",
            "Training\n",
            "Step: 16392; Training Loss: 3.067004442214966; Gradient Norm 0.2984815239906311; Learning Rate: 0.0001722274406611134; Time: 3263.298511505127 ms; Tokens/Sec : 160661.9799419402\n",
            "\n",
            "Training\n",
            "Step: 16393; Training Loss: 3.071056842803955; Gradient Norm 0.30320969223976135; Learning Rate: 0.0001721874056685303; Time: 3262.9780769348145 ms; Tokens/Sec : 160677.75744681287\n",
            "\n",
            "Training\n",
            "Step: 16394; Training Loss: 3.063746929168701; Gradient Norm 0.28391340374946594; Learning Rate: 0.0001721473808069533; Time: 3261.78240776062 ms; Tokens/Sec : 160736.65697398572\n",
            "\n",
            "Training\n",
            "Step: 16395; Training Loss: 3.1184051036834717; Gradient Norm 0.3093113899230957; Learning Rate: 0.00017210736607745536; Time: 3262.5515460968018 ms; Tokens/Sec : 160698.7637106421\n",
            "\n",
            "Training\n",
            "Step: 16396; Training Loss: 3.015626907348633; Gradient Norm 0.31352609395980835; Learning Rate: 0.00017206736148110956; Time: 3261.401414871216 ms; Tokens/Sec : 160755.43403193832\n",
            "\n",
            "Training\n",
            "Step: 16397; Training Loss: 3.0678553581237793; Gradient Norm 0.3058290481567383; Learning Rate: 0.00017202736701898866; Time: 3257.256269454956 ms; Tokens/Sec : 160960.00947685036\n",
            "\n",
            "Training\n",
            "Step: 16398; Training Loss: 3.049623966217041; Gradient Norm 0.3329519033432007; Learning Rate: 0.00017198738269216509; Time: 3263.028144836426 ms; Tokens/Sec : 160675.29200741305\n",
            "\n",
            "Training\n",
            "Step: 16399; Training Loss: 3.0838072299957275; Gradient Norm 0.28145623207092285; Learning Rate: 0.00017194740850171096; Time: 3259.629726409912 ms; Tokens/Sec : 160842.80854115286\n",
            "\n",
            "Training\n",
            "Step: 16400; Training Loss: 3.0594356060028076; Gradient Norm 0.3509973883628845; Learning Rate: 0.00017190744444869826; Time: 3259.6120834350586 ms; Tokens/Sec : 160843.67911886392\n",
            "\n",
            "Training\n",
            "Step: 16401; Training Loss: 3.0444254875183105; Gradient Norm 0.30925971269607544; Learning Rate: 0.0001718674905341986; Time: 3262.1560096740723 ms; Tokens/Sec : 160718.248436065\n",
            "\n",
            "Training\n",
            "Step: 16402; Training Loss: 3.0517830848693848; Gradient Norm 0.27158933877944946; Learning Rate: 0.00017182754675928335; Time: 3262.06111907959 ms; Tokens/Sec : 160722.92359375872\n",
            "\n",
            "Training\n",
            "Step: 16403; Training Loss: 3.0554637908935547; Gradient Norm 0.3175349831581116; Learning Rate: 0.00017178761312502358; Time: 3260.6329917907715 ms; Tokens/Sec : 160793.31875742812\n",
            "\n",
            "Training\n",
            "Step: 16404; Training Loss: 2.9915969371795654; Gradient Norm 0.27462634444236755; Learning Rate: 0.0001717476896324901; Time: 3261.73996925354 ms; Tokens/Sec : 160738.7483190406\n",
            "\n",
            "Training\n",
            "Step: 16405; Training Loss: 3.038120746612549; Gradient Norm 0.3005276918411255; Learning Rate: 0.0001717077762827534; Time: 3258.714199066162 ms; Tokens/Sec : 160887.99691309023\n",
            "\n",
            "Training\n",
            "Step: 16406; Training Loss: 3.040313482284546; Gradient Norm 0.30404070019721985; Learning Rate: 0.00017166787307688387; Time: 3260.178327560425 ms; Tokens/Sec : 160815.74298186385\n",
            "\n",
            "Training\n",
            "Step: 16407; Training Loss: 2.977919340133667; Gradient Norm 0.28362709283828735; Learning Rate: 0.00017162798001595157; Time: 3259.397268295288 ms; Tokens/Sec : 160854.2797467\n",
            "\n",
            "Training\n",
            "Step: 16408; Training Loss: 3.082296133041382; Gradient Norm 0.2944850027561188; Learning Rate: 0.00017158809710102587; Time: 3271.2385654449463 ms; Tokens/Sec : 160272.01609146094\n",
            "\n",
            "Training\n",
            "Step: 16409; Training Loss: 3.0167791843414307; Gradient Norm 0.2840628921985626; Learning Rate: 0.0001715482243331766; Time: 3261.197566986084 ms; Tokens/Sec : 160765.4823821464\n",
            "\n",
            "Training\n",
            "Step: 16410; Training Loss: 3.01358962059021; Gradient Norm 0.2526093125343323; Learning Rate: 0.00017150836171347277; Time: 3260.012149810791 ms; Tokens/Sec : 160823.94049679517\n",
            "\n",
            "Training\n",
            "Step: 16411; Training Loss: 2.99715256690979; Gradient Norm 0.314487099647522; Learning Rate: 0.0001714685092429835; Time: 3261.8422508239746 ms; Tokens/Sec : 160733.7080349485\n",
            "\n",
            "Training\n",
            "Step: 16412; Training Loss: 3.0219714641571045; Gradient Norm 0.2770536541938782; Learning Rate: 0.00017142866692277704; Time: 3260.90145111084 ms; Tokens/Sec : 160780.0811709287\n",
            "\n",
            "Training\n",
            "Step: 16413; Training Loss: 3.015193223953247; Gradient Norm 0.2792946696281433; Learning Rate: 0.00017138883475392204; Time: 3262.826442718506 ms; Tokens/Sec : 160685.22466771977\n",
            "\n",
            "Training\n",
            "Step: 16414; Training Loss: 3.0215675830841064; Gradient Norm 0.2883332371711731; Learning Rate: 0.0001713490127374867; Time: 3262.4435424804688 ms; Tokens/Sec : 160704.0836640436\n",
            "\n",
            "Training\n",
            "Step: 16415; Training Loss: 3.013737678527832; Gradient Norm 0.2714548408985138; Learning Rate: 0.00017130920087453854; Time: 3263.0860805511475 ms; Tokens/Sec : 160672.43923624774\n",
            "\n",
            "Training\n",
            "Step: 16416; Training Loss: 2.998833179473877; Gradient Norm 0.3865659534931183; Learning Rate: 0.00017126939916614517; Time: 3262.2008323669434 ms; Tokens/Sec : 160716.04016469893\n",
            "\n",
            "Training\n",
            "Step: 16417; Training Loss: 3.048680067062378; Gradient Norm 0.3176276683807373; Learning Rate: 0.00017122960761337412; Time: 3260.180711746216 ms; Tokens/Sec : 160815.6253765397\n",
            "\n",
            "Training\n",
            "Step: 16418; Training Loss: 3.073575973510742; Gradient Norm 0.359792560338974; Learning Rate: 0.00017118982621729218; Time: 3257.6112747192383 ms; Tokens/Sec : 160942.46851020813\n",
            "\n",
            "Training\n",
            "Step: 16419; Training Loss: 3.030245065689087; Gradient Norm 0.427366703748703; Learning Rate: 0.00017115005497896603; Time: 3261.4855766296387 ms; Tokens/Sec : 160751.28578118377\n",
            "\n",
            "Training\n",
            "Step: 16420; Training Loss: 3.063319683074951; Gradient Norm 0.325665146112442; Learning Rate: 0.0001711102938994623; Time: 3260.493278503418 ms; Tokens/Sec : 160800.2088078681\n",
            "\n",
            "Training\n",
            "Step: 16421; Training Loss: 3.055582284927368; Gradient Norm 0.3563605844974518; Learning Rate: 0.0001710705429798472; Time: 3262.4990940093994 ms; Tokens/Sec : 160701.3473084782\n",
            "\n",
            "Training\n",
            "Step: 16422; Training Loss: 3.1167383193969727; Gradient Norm 0.3739999532699585; Learning Rate: 0.00017103080222118647; Time: 3261.962890625 ms; Tokens/Sec : 160727.76349075668\n",
            "\n",
            "Training\n",
            "Step: 16423; Training Loss: 3.019329071044922; Gradient Norm 0.3238607347011566; Learning Rate: 0.00017099107162454576; Time: 3262.5837326049805 ms; Tokens/Sec : 160697.17836218933\n",
            "\n",
            "Training\n",
            "Step: 16424; Training Loss: 3.04315447807312; Gradient Norm 0.3759874403476715; Learning Rate: 0.00017095135119099073; Time: 3262.7930641174316 ms; Tokens/Sec : 160686.868488798\n",
            "\n",
            "Training\n",
            "Step: 16425; Training Loss: 3.0484044551849365; Gradient Norm 0.31507453322410583; Learning Rate: 0.0001709116409215862; Time: 3262.064218521118 ms; Tokens/Sec : 160722.7708833059\n",
            "\n",
            "Training\n",
            "Step: 16426; Training Loss: 2.9935216903686523; Gradient Norm 0.4059719443321228; Learning Rate: 0.000170871940817397; Time: 3262.199640274048 ms; Tokens/Sec : 160716.0988945349\n",
            "\n",
            "Training\n",
            "Step: 16427; Training Loss: 3.0233867168426514; Gradient Norm 0.29649028182029724; Learning Rate: 0.0001708322508794877; Time: 3271.8071937561035 ms; Tokens/Sec : 160244.16139207347\n",
            "\n",
            "Training\n",
            "Step: 16428; Training Loss: 3.025829792022705; Gradient Norm 0.32321518659591675; Learning Rate: 0.00017079257110892286; Time: 3262.582778930664 ms; Tokens/Sec : 160697.22533502715\n",
            "\n",
            "Training\n",
            "Step: 16429; Training Loss: 3.063000202178955; Gradient Norm 0.29760563373565674; Learning Rate: 0.00017075290150676614; Time: 3262.265205383301 ms; Tokens/Sec : 160712.86881729733\n",
            "\n",
            "Training\n",
            "Step: 16430; Training Loss: 3.024421215057373; Gradient Norm 0.2827812135219574; Learning Rate: 0.00017071324207408136; Time: 3259.460210800171 ms; Tokens/Sec : 160851.17353566084\n",
            "\n",
            "Training\n",
            "Step: 16431; Training Loss: 3.0318243503570557; Gradient Norm 0.3089967966079712; Learning Rate: 0.00017067359281193207; Time: 3260.9875202178955 ms; Tokens/Sec : 160775.83761037138\n",
            "\n",
            "Training\n",
            "Step: 16432; Training Loss: 3.038978338241577; Gradient Norm 0.2622905969619751; Learning Rate: 0.00017063395372138144; Time: 3260.603666305542 ms; Tokens/Sec : 160794.76491359327\n",
            "\n",
            "Training\n",
            "Step: 16433; Training Loss: 3.03314208984375; Gradient Norm 0.27322444319725037; Learning Rate: 0.00017059432480349236; Time: 3259.0177059173584 ms; Tokens/Sec : 160873.01368386453\n",
            "\n",
            "Training\n",
            "Step: 16434; Training Loss: 3.066843271255493; Gradient Norm 0.26037847995758057; Learning Rate: 0.00017055470605932744; Time: 3262.02654838562 ms; Tokens/Sec : 160724.62692232552\n",
            "\n",
            "Training\n",
            "Step: 16435; Training Loss: 3.0435543060302734; Gradient Norm 0.2698853313922882; Learning Rate: 0.0001705150974899491; Time: 3263.061285018921 ms; Tokens/Sec : 160673.66016294723\n",
            "\n",
            "Training\n",
            "Step: 16436; Training Loss: 3.0347049236297607; Gradient Norm 0.3097512722015381; Learning Rate: 0.00017047549909641948; Time: 3264.763116836548 ms; Tokens/Sec : 160589.90537359982\n",
            "\n",
            "Training\n",
            "Step: 16437; Training Loss: 2.986922025680542; Gradient Norm 0.2852361798286438; Learning Rate: 0.00017043591087980027; Time: 3261.9709968566895 ms; Tokens/Sec : 160727.3640707462\n",
            "\n",
            "Training\n",
            "Step: 16438; Training Loss: 3.0340206623077393; Gradient Norm 0.28909289836883545; Learning Rate: 0.0001703963328411532; Time: 3259.9780559539795 ms; Tokens/Sec : 160825.62244320865\n",
            "\n",
            "Training\n",
            "Step: 16439; Training Loss: 2.9693715572357178; Gradient Norm 0.31412893533706665; Learning Rate: 0.00017035676498153947; Time: 3260.5185508728027 ms; Tokens/Sec : 160798.96244100627\n",
            "\n",
            "Training\n",
            "Step: 16440; Training Loss: 2.9488441944122314; Gradient Norm 0.2711067795753479; Learning Rate: 0.00017031720730202002; Time: 3260.6890201568604 ms; Tokens/Sec : 160790.55584846245\n",
            "\n",
            "Training\n",
            "Step: 16441; Training Loss: 2.9711697101593018; Gradient Norm 0.2832406759262085; Learning Rate: 0.00017027765980365579; Time: 3258.8884830474854 ms; Tokens/Sec : 160879.39269088532\n",
            "\n",
            "Training\n",
            "Step: 16442; Training Loss: 2.99174165725708; Gradient Norm 0.27570778131484985; Learning Rate: 0.00017023812248750686; Time: 3260.110378265381 ms; Tokens/Sec : 160819.09480591264\n",
            "\n",
            "Training\n",
            "Step: 16443; Training Loss: 3.0361080169677734; Gradient Norm 0.2875056564807892; Learning Rate: 0.0001701985953546338; Time: 3259.7885131835938 ms; Tokens/Sec : 160834.97376581858\n",
            "\n",
            "Training\n",
            "Step: 16444; Training Loss: 2.965707302093506; Gradient Norm 0.3252749443054199; Learning Rate: 0.00017015907840609647; Time: 3264.5833492279053 ms; Tokens/Sec : 160598.74842037575\n",
            "\n",
            "Training\n",
            "Step: 16445; Training Loss: 3.0017428398132324; Gradient Norm 0.28597888350486755; Learning Rate: 0.00017011957164295437; Time: 3262.2218132019043 ms; Tokens/Sec : 160715.00652661198\n",
            "\n",
            "Training\n",
            "Step: 16446; Training Loss: 2.9528884887695312; Gradient Norm 0.3102412819862366; Learning Rate: 0.00017008007506626673; Time: 3277.1756649017334 ms; Tokens/Sec : 159981.65908989223\n",
            "\n",
            "Training\n",
            "Step: 16447; Training Loss: 3.000518321990967; Gradient Norm 0.29127004742622375; Learning Rate: 0.00017004058867709304; Time: 3258.183002471924 ms; Tokens/Sec : 160914.22722487728\n",
            "\n",
            "Training\n",
            "Step: 16448; Training Loss: 3.045842409133911; Gradient Norm 0.27740728855133057; Learning Rate: 0.00017000111247649196; Time: 3258.3391666412354 ms; Tokens/Sec : 160906.51500237992\n",
            "\n",
            "Training\n",
            "Step: 16449; Training Loss: 3.0440824031829834; Gradient Norm 0.2926631569862366; Learning Rate: 0.00016996164646552195; Time: 3263.3304595947266 ms; Tokens/Sec : 160660.40705700134\n",
            "\n",
            "Training\n",
            "Step: 16450; Training Loss: 3.004746913909912; Gradient Norm 0.3013833165168762; Learning Rate: 0.00016992219064524124; Time: 3262.212038040161 ms; Tokens/Sec : 160715.48810633918\n",
            "\n",
            "Training\n",
            "Step: 16451; Training Loss: 3.0468926429748535; Gradient Norm 0.3217816948890686; Learning Rate: 0.00016988274501670815; Time: 3262.5014781951904 ms; Tokens/Sec : 160701.2298704107\n",
            "\n",
            "Training\n",
            "Step: 16452; Training Loss: 3.107673168182373; Gradient Norm 0.36237141489982605; Learning Rate: 0.00016984330958098008; Time: 3261.920690536499 ms; Tokens/Sec : 160729.84285640882\n",
            "\n",
            "Training\n",
            "Step: 16453; Training Loss: 3.08256196975708; Gradient Norm 0.32330378890037537; Learning Rate: 0.0001698038843391146; Time: 3260.4053020477295 ms; Tokens/Sec : 160804.54772623384\n",
            "\n",
            "Training\n",
            "Step: 16454; Training Loss: 3.070363998413086; Gradient Norm 0.3518657684326172; Learning Rate: 0.00016976446929216883; Time: 3259.044885635376 ms; Tokens/Sec : 160871.67203828983\n",
            "\n",
            "Training\n",
            "Step: 16455; Training Loss: 3.0479464530944824; Gradient Norm 0.34065598249435425; Learning Rate: 0.00016972506444119993; Time: 3263.2696628570557 ms; Tokens/Sec : 160663.400260025\n",
            "\n",
            "Training\n",
            "Step: 16456; Training Loss: 3.0657784938812256; Gradient Norm 0.34569284319877625; Learning Rate: 0.00016968566978726424; Time: 3261.1310482025146 ms; Tokens/Sec : 160768.7615893202\n",
            "\n",
            "Training\n",
            "Step: 16457; Training Loss: 3.0835423469543457; Gradient Norm 0.3171680271625519; Learning Rate: 0.0001696462853314183; Time: 3260.225534439087 ms; Tokens/Sec : 160813.4144284599\n",
            "\n",
            "Training\n",
            "Step: 16458; Training Loss: 3.0474963188171387; Gradient Norm 0.3689493238925934; Learning Rate: 0.0001696069110747181; Time: 3261.7387771606445 ms; Tokens/Sec : 160738.80706547401\n",
            "\n",
            "Training\n",
            "Step: 16459; Training Loss: 2.9568285942077637; Gradient Norm 0.3631136417388916; Learning Rate: 0.00016956754701821948; Time: 3261.406660079956 ms; Tokens/Sec : 160755.1754944741\n",
            "\n",
            "Training\n",
            "Step: 16460; Training Loss: 3.066282272338867; Gradient Norm 0.31328725814819336; Learning Rate: 0.00016952819316297802; Time: 3260.660171508789 ms; Tokens/Sec : 160791.97844079495\n",
            "\n",
            "Training\n",
            "Step: 16461; Training Loss: 3.110941171646118; Gradient Norm 0.4086315631866455; Learning Rate: 0.000169488849510049; Time: 3262.8910541534424 ms; Tokens/Sec : 160682.04279533526\n",
            "\n",
            "Training\n",
            "Step: 16462; Training Loss: 3.0076515674591064; Gradient Norm 0.30941513180732727; Learning Rate: 0.00016944951606048732; Time: 3260.9994411468506 ms; Tokens/Sec : 160775.24987726304\n",
            "\n",
            "Training\n",
            "Step: 16463; Training Loss: 3.116831064224243; Gradient Norm 0.4144074320793152; Learning Rate: 0.00016941019281534783; Time: 3261.150598526001 ms; Tokens/Sec : 160767.79779411954\n",
            "\n",
            "Training\n",
            "Step: 16464; Training Loss: 3.045255184173584; Gradient Norm 0.37573397159576416; Learning Rate: 0.00016937087977568493; Time: 3263.6148929595947 ms; Tokens/Sec : 160646.40504338173\n",
            "\n",
            "Training\n",
            "Step: 16465; Training Loss: 3.021810531616211; Gradient Norm 0.3000480532646179; Learning Rate: 0.00016933157694255276; Time: 3266.4501667022705 ms; Tokens/Sec : 160506.96420980716\n",
            "\n",
            "Training\n",
            "Step: 16466; Training Loss: 3.029991626739502; Gradient Norm 0.33162596821784973; Learning Rate: 0.0001692922843170052; Time: 3263.3442878723145 ms; Tokens/Sec : 160659.72626560755\n",
            "\n",
            "Training\n",
            "Step: 16467; Training Loss: 3.0913729667663574; Gradient Norm 0.2939176559448242; Learning Rate: 0.00016925300190009598; Time: 3262.2106075286865 ms; Tokens/Sec : 160715.5585816633\n",
            "\n",
            "Training\n",
            "Step: 16468; Training Loss: 3.0345890522003174; Gradient Norm 0.33023902773857117; Learning Rate: 0.00016921372969287853; Time: 3261.6052627563477 ms; Tokens/Sec : 160745.38693775894\n",
            "\n",
            "Training\n",
            "Step: 16469; Training Loss: 3.0743632316589355; Gradient Norm 0.29632705450057983; Learning Rate: 0.0001691744676964055; Time: 3261.5180015563965 ms; Tokens/Sec : 160749.68764538775\n",
            "\n",
            "Training\n",
            "Step: 16470; Training Loss: 3.0710694789886475; Gradient Norm 0.31349194049835205; Learning Rate: 0.00016913521591173012; Time: 3261.3821029663086 ms; Tokens/Sec : 160756.38592704208\n",
            "\n",
            "Training\n",
            "Step: 16471; Training Loss: 3.0743088722229004; Gradient Norm 0.3069409132003784; Learning Rate: 0.00016909597433990484; Time: 3263.8068199157715 ms; Tokens/Sec : 160636.95829078826\n",
            "\n",
            "Training\n",
            "Step: 16472; Training Loss: 3.0284440517425537; Gradient Norm 0.2733328938484192; Learning Rate: 0.00016905674298198196; Time: 15283.293008804321 ms; Tokens/Sec : 34304.64885401143\n",
            "\n",
            "Training\n",
            "Step: 16473; Training Loss: 3.0384020805358887; Gradient Norm 0.29821059107780457; Learning Rate: 0.00016901752183901317; Time: 3245.457887649536 ms; Tokens/Sec : 161545.15576836094\n",
            "\n",
            "Training\n",
            "Step: 16474; Training Loss: 2.9973573684692383; Gradient Norm 0.3050488531589508; Learning Rate: 0.00016897831091205047; Time: 3239.945411682129 ms; Tokens/Sec : 161820.01033400063\n",
            "\n",
            "Training\n",
            "Step: 16475; Training Loss: 3.0585312843322754; Gradient Norm 0.3076295554637909; Learning Rate: 0.00016893911020214537; Time: 3248.157501220703 ms; Tokens/Sec : 161410.8921143649\n",
            "\n",
            "Training\n",
            "Step: 16476; Training Loss: 3.006474018096924; Gradient Norm 0.33470216393470764; Learning Rate: 0.00016889991971034862; Time: 3246.7434406280518 ms; Tokens/Sec : 161481.19171947305\n",
            "\n",
            "Training\n",
            "Step: 16477; Training Loss: 3.003974199295044; Gradient Norm 0.28680554032325745; Learning Rate: 0.00016886073943771148; Time: 3248.5430240631104 ms; Tokens/Sec : 161391.73657741726\n",
            "\n",
            "Training\n",
            "Step: 16478; Training Loss: 3.017361879348755; Gradient Norm 0.2935029864311218; Learning Rate: 0.0001688215693852846; Time: 3246.4206218719482 ms; Tokens/Sec : 161497.2491450247\n",
            "\n",
            "Training\n",
            "Step: 16479; Training Loss: 3.074537515640259; Gradient Norm 0.28153201937675476; Learning Rate: 0.00016878240955411802; Time: 3246.795892715454 ms; Tokens/Sec : 161478.58298585942\n",
            "\n",
            "Training\n",
            "Step: 16480; Training Loss: 3.0569541454315186; Gradient Norm 0.32218316197395325; Learning Rate: 0.00016874325994526194; Time: 3249.164342880249 ms; Tokens/Sec : 161360.874573442\n",
            "\n",
            "Training\n",
            "Step: 16481; Training Loss: 3.0135185718536377; Gradient Norm 0.2804725766181946; Learning Rate: 0.00016870412055976618; Time: 3249.3457794189453 ms; Tokens/Sec : 161351.86452632758\n",
            "\n",
            "Training\n",
            "Step: 16482; Training Loss: 3.030060291290283; Gradient Norm 0.27630648016929626; Learning Rate: 0.00016866499139868045; Time: 3249.413251876831 ms; Tokens/Sec : 161348.51413472142\n",
            "\n",
            "Training\n",
            "Step: 16483; Training Loss: 3.044342041015625; Gradient Norm 0.2850204110145569; Learning Rate: 0.0001686258724630536; Time: 3245.4535961151123 ms; Tokens/Sec : 161545.36938306116\n",
            "\n",
            "Training\n",
            "Step: 16484; Training Loss: 3.048656702041626; Gradient Norm 0.28206390142440796; Learning Rate: 0.00016858676375393475; Time: 3247.0648288726807 ms; Tokens/Sec : 161465.20862104956\n",
            "\n",
            "Training\n",
            "Step: 16485; Training Loss: 3.0152931213378906; Gradient Norm 0.28237172961235046; Learning Rate: 0.00016854766527237287; Time: 3249.1979598999023 ms; Tokens/Sec : 161359.2050932322\n",
            "\n",
            "Training\n",
            "Step: 16486; Training Loss: 3.0997347831726074; Gradient Norm 0.3300735354423523; Learning Rate: 0.00016850857701941602; Time: 3250.502824783325 ms; Tokens/Sec : 161294.42989638026\n",
            "\n",
            "Training\n",
            "Step: 16487; Training Loss: 3.0667078495025635; Gradient Norm 0.2851846516132355; Learning Rate: 0.00016846949899611244; Time: 3248.4021186828613 ms; Tokens/Sec : 161398.73723902894\n",
            "\n",
            "Training\n",
            "Step: 16488; Training Loss: 3.1389408111572266; Gradient Norm 0.32100528478622437; Learning Rate: 0.0001684304312035101; Time: 3248.250961303711 ms; Tokens/Sec : 161406.2479302932\n",
            "\n",
            "Training\n",
            "Step: 16489; Training Loss: 3.0047974586486816; Gradient Norm 0.33241620659828186; Learning Rate: 0.00016839137364265658; Time: 3246.1299896240234 ms; Tokens/Sec : 161511.7083036852\n",
            "\n",
            "Training\n",
            "Step: 16490; Training Loss: 3.116581916809082; Gradient Norm 0.31992557644844055; Learning Rate: 0.0001683523263145991; Time: 3247.312307357788 ms; Tokens/Sec : 161452.90331701812\n",
            "\n",
            "Training\n",
            "Step: 16491; Training Loss: 3.0019614696502686; Gradient Norm 0.3183895945549011; Learning Rate: 0.00016831328922038485; Time: 3249.267339706421 ms; Tokens/Sec : 161355.75967946384\n",
            "\n",
            "Training\n",
            "Step: 16492; Training Loss: 3.1006736755371094; Gradient Norm 0.2899199426174164; Learning Rate: 0.00016827426236106047; Time: 3250.732421875 ms; Tokens/Sec : 161283.0377769433\n",
            "\n",
            "Training\n",
            "Step: 16493; Training Loss: 3.07310152053833; Gradient Norm 0.30714404582977295; Learning Rate: 0.00016823524573767254; Time: 3254.4491291046143 ms; Tokens/Sec : 161098.84628746542\n",
            "\n",
            "Training\n",
            "Step: 16494; Training Loss: 3.05206561088562; Gradient Norm 0.29014062881469727; Learning Rate: 0.00016819623935126728; Time: 3256.258487701416 ms; Tokens/Sec : 161009.33079489443\n",
            "\n",
            "Training\n",
            "Step: 16495; Training Loss: 3.1319479942321777; Gradient Norm 0.33723586797714233; Learning Rate: 0.00016815724320289066; Time: 3255.7003498077393 ms; Tokens/Sec : 161036.9332764181\n",
            "\n",
            "Training\n",
            "Step: 16496; Training Loss: 3.0250940322875977; Gradient Norm 0.3007362484931946; Learning Rate: 0.0001681182572935883; Time: 3254.85897064209 ms; Tokens/Sec : 161078.56123074147\n",
            "\n",
            "Training\n",
            "Step: 16497; Training Loss: 3.043330430984497; Gradient Norm 0.3188868761062622; Learning Rate: 0.00016807928162440567; Time: 3259.739875793457 ms; Tokens/Sec : 160837.37352581928\n",
            "\n",
            "Training\n",
            "Step: 16498; Training Loss: 3.053602933883667; Gradient Norm 0.3012099862098694; Learning Rate: 0.0001680403161963878; Time: 3258.9359283447266 ms; Tokens/Sec : 160877.05052437025\n",
            "\n",
            "Training\n",
            "Step: 16499; Training Loss: 3.0527403354644775; Gradient Norm 0.30937090516090393; Learning Rate: 0.00016800136101057967; Time: 3259.382963180542 ms; Tokens/Sec : 160854.98572048557\n",
            "\n",
            "Evaluating HellaSwag\n",
            "Evaluating Validation set\n",
            "Training\n",
            "Step: 16500; Training Loss: 3.058403253555298; Gradient Norm 0.35136714577674866; Learning Rate: 0.0001679624160680258; Time: 3250.507593154907 ms; Tokens/Sec : 161294.19328355783; Validation_loss: 2.9307363033294678; HellaSwag Acc: 0.338\n",
            "\n",
            "Checkpointing\n",
            "Training\n",
            "Step: 16501; Training Loss: 3.0649170875549316; Gradient Norm 0.3337017893791199; Learning Rate: 0.00016792348136977049; Time: 3284.5077514648438 ms; Tokens/Sec : 159624.5281400767\n",
            "\n",
            "Training\n",
            "Step: 16502; Training Loss: 3.0424959659576416; Gradient Norm 0.3634796142578125; Learning Rate: 0.00016788455691685788; Time: 3252.9399394989014 ms; Tokens/Sec : 161173.58750889322\n",
            "\n",
            "Training\n",
            "Step: 16503; Training Loss: 3.0519754886627197; Gradient Norm 0.3375524878501892; Learning Rate: 0.00016784564271033137; Time: 3250.4525184631348 ms; Tokens/Sec : 161296.92620395255\n",
            "\n",
            "Training\n",
            "Step: 16504; Training Loss: 3.172513723373413; Gradient Norm 0.39208149909973145; Learning Rate: 0.0001678067387512348; Time: 3247.816562652588 ms; Tokens/Sec : 161427.8361742815\n",
            "\n",
            "Training\n",
            "Step: 16505; Training Loss: 3.0497665405273438; Gradient Norm 0.33479735255241394; Learning Rate: 0.00016776784504061141; Time: 3250.4971027374268 ms; Tokens/Sec : 161294.71383268348\n",
            "\n",
            "Training\n",
            "Step: 16506; Training Loss: 3.086045265197754; Gradient Norm 0.3251984119415283; Learning Rate: 0.0001677289615795038; Time: 3252.345085144043 ms; Tokens/Sec : 161203.0661797931\n",
            "\n",
            "Training\n",
            "Step: 16507; Training Loss: 3.0640745162963867; Gradient Norm 0.31036800146102905; Learning Rate: 0.0001676900883689548; Time: 3248.4023571014404 ms; Tokens/Sec : 161398.72539306487\n",
            "\n",
            "Training\n",
            "Step: 16508; Training Loss: 3.036803722381592; Gradient Norm 0.3238012492656708; Learning Rate: 0.00016765122541000685; Time: 3256.019115447998 ms; Tokens/Sec : 161021.16769294912\n",
            "\n",
            "Training\n",
            "Step: 16509; Training Loss: 3.047899007797241; Gradient Norm 0.32610610127449036; Learning Rate: 0.0001676123727037022; Time: 3256.2968730926514 ms; Tokens/Sec : 161007.43280880904\n",
            "\n",
            "Training\n",
            "Step: 16510; Training Loss: 3.069406509399414; Gradient Norm 0.31712883710861206; Learning Rate: 0.0001675735302510823; Time: 3255.2714347839355 ms; Tokens/Sec : 161058.15152548067\n",
            "\n",
            "Training\n",
            "Step: 16511; Training Loss: 3.0337493419647217; Gradient Norm 0.29508674144744873; Learning Rate: 0.0001675346980531888; Time: 3253.011703491211 ms; Tokens/Sec : 161170.03189300591\n",
            "\n",
            "Training\n",
            "Step: 16512; Training Loss: 3.0415561199188232; Gradient Norm 0.30549588799476624; Learning Rate: 0.00016749587611106334; Time: 3256.1657428741455 ms; Tokens/Sec : 161013.91679688348\n",
            "\n",
            "Training\n",
            "Step: 16513; Training Loss: 3.0173208713531494; Gradient Norm 0.30304238200187683; Learning Rate: 0.00016745706442574646; Time: 3255.5627822875977 ms; Tokens/Sec : 161043.7380757857\n",
            "\n",
            "Training\n",
            "Step: 16514; Training Loss: 3.034526824951172; Gradient Norm 0.332121342420578; Learning Rate: 0.00016741826299827918; Time: 3254.974126815796 ms; Tokens/Sec : 161072.86250932165\n",
            "\n",
            "Training\n",
            "Step: 16515; Training Loss: 3.0197720527648926; Gradient Norm 0.2991243302822113; Learning Rate: 0.00016737947182970172; Time: 3254.682779312134 ms; Tokens/Sec : 161087.28117300774\n",
            "\n",
            "Training\n",
            "Step: 16516; Training Loss: 2.9878928661346436; Gradient Norm 0.3466755151748657; Learning Rate: 0.00016734069092105467; Time: 3252.0499229431152 ms; Tokens/Sec : 161217.6972749292\n",
            "\n",
            "Training\n",
            "Step: 16517; Training Loss: 2.9982407093048096; Gradient Norm 0.27795353531837463; Learning Rate: 0.00016730192027337757; Time: 3254.3296813964844 ms; Tokens/Sec : 161104.75929870133\n",
            "\n",
            "Training\n",
            "Step: 16518; Training Loss: 3.0059969425201416; Gradient Norm 0.28699344396591187; Learning Rate: 0.00016726315988771013; Time: 3252.7222633361816 ms; Tokens/Sec : 161184.37344301867\n",
            "\n",
            "Training\n",
            "Step: 16519; Training Loss: 3.0009725093841553; Gradient Norm 0.34418439865112305; Learning Rate: 0.0001672244097650917; Time: 3255.429744720459 ms; Tokens/Sec : 161050.31934732173\n",
            "\n",
            "Training\n",
            "Step: 16520; Training Loss: 3.0584638118743896; Gradient Norm 0.340120404958725; Learning Rate: 0.00016718566990656146; Time: 3251.1281967163086 ms; Tokens/Sec : 161263.40404833597\n",
            "\n",
            "Training\n",
            "Step: 16521; Training Loss: 3.0087811946868896; Gradient Norm 0.31363630294799805; Learning Rate: 0.00016714694031315814; Time: 3251.866102218628 ms; Tokens/Sec : 161226.8105511164\n",
            "\n",
            "Training\n",
            "Step: 16522; Training Loss: 3.114065408706665; Gradient Norm 0.3259333670139313; Learning Rate: 0.00016710822098592025; Time: 3256.385326385498 ms; Tokens/Sec : 161003.05935905498\n",
            "\n",
            "Training\n",
            "Step: 16523; Training Loss: 3.0962040424346924; Gradient Norm 0.3690551817417145; Learning Rate: 0.00016706951192588612; Time: 3255.986213684082 ms; Tokens/Sec : 161022.79481299734\n",
            "\n",
            "Training\n",
            "Step: 16524; Training Loss: 3.065448760986328; Gradient Norm 0.34010049700737; Learning Rate: 0.00016703081313409367; Time: 3253.8979053497314 ms; Tokens/Sec : 161126.13709791523\n",
            "\n",
            "Training\n",
            "Step: 16525; Training Loss: 3.115229606628418; Gradient Norm 0.3265395760536194; Learning Rate: 0.00016699212461158063; Time: 3254.3234825134277 ms; Tokens/Sec : 161105.0661734076\n",
            "\n",
            "Training\n",
            "Step: 16526; Training Loss: 3.0721004009246826; Gradient Norm 0.27504923939704895; Learning Rate: 0.00016695344635938436; Time: 3254.213333129883 ms; Tokens/Sec : 161110.51929583945\n",
            "\n",
            "Training\n",
            "Step: 16527; Training Loss: 3.0283355712890625; Gradient Norm 0.31628793478012085; Learning Rate: 0.00016691477837854214; Time: 3252.6602745056152 ms; Tokens/Sec : 161187.4452765248\n",
            "\n",
            "Training\n",
            "Step: 16528; Training Loss: 3.031766176223755; Gradient Norm 0.2835681736469269; Learning Rate: 0.00016687612067009075; Time: 3252.9687881469727 ms; Tokens/Sec : 161172.15815607517\n",
            "\n",
            "Training\n",
            "Step: 16529; Training Loss: 3.1423919200897217; Gradient Norm 0.2875964641571045; Learning Rate: 0.00016683747323506694; Time: 3256.351947784424 ms; Tokens/Sec : 161004.70968954024\n",
            "\n",
            "Training\n",
            "Step: 16530; Training Loss: 3.007690668106079; Gradient Norm 0.31412896513938904; Learning Rate: 0.00016679883607450673; Time: 3251.251220703125 ms; Tokens/Sec : 161257.30200775317\n",
            "\n",
            "Training\n",
            "Step: 16531; Training Loss: 3.064894199371338; Gradient Norm 0.32049715518951416; Learning Rate: 0.00016676020918944643; Time: 3254.826784133911 ms; Tokens/Sec : 161080.15411318108\n",
            "\n",
            "Training\n",
            "Step: 16532; Training Loss: 3.056553363800049; Gradient Norm 0.2753470242023468; Learning Rate: 0.00016672159258092182; Time: 3254.152297973633 ms; Tokens/Sec : 161113.5410983915\n",
            "\n",
            "Training\n",
            "Step: 16533; Training Loss: 3.0916664600372314; Gradient Norm 0.3171593248844147; Learning Rate: 0.0001666829862499682; Time: 3258.887767791748 ms; Tokens/Sec : 160879.42800045008\n",
            "\n",
            "Training\n",
            "Step: 16534; Training Loss: 3.1187283992767334; Gradient Norm 0.3048724830150604; Learning Rate: 0.000166644390197621; Time: 3257.6024532318115 ms; Tokens/Sec : 160942.90433747153\n",
            "\n",
            "Training\n",
            "Step: 16535; Training Loss: 3.047090768814087; Gradient Norm 0.31545567512512207; Learning Rate: 0.0001666058044249151; Time: 3255.488395690918 ms; Tokens/Sec : 161047.41786024073\n",
            "\n",
            "Training\n",
            "Step: 16536; Training Loss: 3.1319165229797363; Gradient Norm 0.29354071617126465; Learning Rate: 0.00016656722893288524; Time: 3252.439260482788 ms; Tokens/Sec : 161198.39849743276\n",
            "\n",
            "Training\n",
            "Step: 16537; Training Loss: 3.0624783039093018; Gradient Norm 0.3231824040412903; Learning Rate: 0.00016652866372256556; Time: 3254.2624473571777 ms; Tokens/Sec : 161108.08777140273\n",
            "\n",
            "Training\n",
            "Step: 16538; Training Loss: 3.05216383934021; Gradient Norm 0.3139357566833496; Learning Rate: 0.00016649010879499042; Time: 3257.004737854004 ms; Tokens/Sec : 160972.44007862458\n",
            "\n",
            "Training\n",
            "Step: 16539; Training Loss: 3.0584778785705566; Gradient Norm 0.30205821990966797; Learning Rate: 0.00016645156415119375; Time: 3255.7640075683594 ms; Tokens/Sec : 161033.78462973313\n",
            "\n",
            "Training\n",
            "Step: 16540; Training Loss: 3.0032095909118652; Gradient Norm 0.2865135669708252; Learning Rate: 0.00016641302979220887; Time: 3256.361246109009 ms; Tokens/Sec : 161004.24995121968\n",
            "\n",
            "Training\n",
            "Step: 16541; Training Loss: 3.028742790222168; Gradient Norm 0.3385336399078369; Learning Rate: 0.0001663745057190691; Time: 3254.7266483306885 ms; Tokens/Sec : 161085.10994891115\n",
            "\n",
            "Training\n",
            "Step: 16542; Training Loss: 3.0448179244995117; Gradient Norm 0.2727559506893158; Learning Rate: 0.00016633599193280763; Time: 3257.233142852783 ms; Tokens/Sec : 160961.15230511647\n",
            "\n",
            "Training\n",
            "Step: 16543; Training Loss: 3.024045705795288; Gradient Norm 0.28965693712234497; Learning Rate: 0.00016629748843445725; Time: 3253.9007663726807 ms; Tokens/Sec : 161125.99542623895\n",
            "\n",
            "Training\n",
            "Step: 16544; Training Loss: 3.0574567317962646; Gradient Norm 0.32168692350387573; Learning Rate: 0.00016625899522505012; Time: 3256.9921016693115 ms; Tokens/Sec : 160973.06460500343\n",
            "\n",
            "Training\n",
            "Step: 16545; Training Loss: 3.0136075019836426; Gradient Norm 0.2894216477870941; Learning Rate: 0.0001662205123056186; Time: 3255.669116973877 ms; Tokens/Sec : 161038.47816307642\n",
            "\n",
            "Training\n",
            "Step: 16546; Training Loss: 2.949345588684082; Gradient Norm 0.2781170904636383; Learning Rate: 0.00016618203967719481; Time: 3257.2181224823 ms; Tokens/Sec : 160961.89456309556\n",
            "\n",
            "Training\n",
            "Step: 16547; Training Loss: 2.9504566192626953; Gradient Norm 0.3174162805080414; Learning Rate: 0.00016614357734081003; Time: 3256.115198135376 ms; Tokens/Sec : 161016.41621900696\n",
            "\n",
            "Training\n",
            "Step: 16548; Training Loss: 3.0290255546569824; Gradient Norm 0.26201727986335754; Learning Rate: 0.00016610512529749584; Time: 3254.7500133514404 ms; Tokens/Sec : 161083.95355996533\n",
            "\n",
            "Training\n",
            "Step: 16549; Training Loss: 2.9876697063446045; Gradient Norm 0.3219456672668457; Learning Rate: 0.00016606668354828322; Time: 3255.6490898132324 ms; Tokens/Sec : 161039.4687930194\n",
            "\n",
            "Training\n",
            "Step: 16550; Training Loss: 3.0431320667266846; Gradient Norm 0.27747321128845215; Learning Rate: 0.00016602825209420307; Time: 3255.5389404296875 ms; Tokens/Sec : 161044.9174755689\n",
            "\n",
            "Training\n",
            "Step: 16551; Training Loss: 3.0408754348754883; Gradient Norm 0.33346834778785706; Learning Rate: 0.00016598983093628588; Time: 3257.685661315918 ms; Tokens/Sec : 160938.79352012672\n",
            "\n",
            "Training\n",
            "Step: 16552; Training Loss: 3.039745807647705; Gradient Norm 0.313670814037323; Learning Rate: 0.00016595142007556194; Time: 3255.8302879333496 ms; Tokens/Sec : 161030.5063943593\n",
            "\n",
            "Training\n",
            "Step: 16553; Training Loss: 3.0370543003082275; Gradient Norm 0.3003298044204712; Learning Rate: 0.0001659130195130612; Time: 3255.622148513794 ms; Tokens/Sec : 161040.80144538268\n",
            "\n",
            "Training\n",
            "Step: 16554; Training Loss: 3.0224826335906982; Gradient Norm 0.3200886845588684; Learning Rate: 0.0001658746292498134; Time: 3255.795478820801 ms; Tokens/Sec : 161032.22804089927\n",
            "\n",
            "Training\n",
            "Step: 16555; Training Loss: 3.113614320755005; Gradient Norm 0.34052297472953796; Learning Rate: 0.00016583624928684793; Time: 3260.768175125122 ms; Tokens/Sec : 160786.6526665552\n",
            "\n",
            "Training\n",
            "Step: 16556; Training Loss: 3.0738770961761475; Gradient Norm 0.3151143789291382; Learning Rate: 0.000165797879625194; Time: 3257.1544647216797 ms; Tokens/Sec : 160965.04039908954\n",
            "\n",
            "Training\n",
            "Step: 16557; Training Loss: 2.9993929862976074; Gradient Norm 0.322002112865448; Learning Rate: 0.00016575952026588042; Time: 3257.0741176605225 ms; Tokens/Sec : 160969.01116164448\n",
            "\n",
            "Training\n",
            "Step: 16558; Training Loss: 3.029078245162964; Gradient Norm 0.3016827702522278; Learning Rate: 0.00016572117120993584; Time: 3255.790948867798 ms; Tokens/Sec : 161032.4520934986\n",
            "\n",
            "Training\n",
            "Step: 16559; Training Loss: 2.930448055267334; Gradient Norm 0.33729803562164307; Learning Rate: 0.00016568283245838858; Time: 3255.79833984375 ms; Tokens/Sec : 161032.08653431566\n",
            "\n",
            "Training\n",
            "Step: 16560; Training Loss: 3.037278175354004; Gradient Norm 0.3507704436779022; Learning Rate: 0.00016564450401226666; Time: 3256.5696239471436 ms; Tokens/Sec : 160993.9477862395\n",
            "\n",
            "Training\n",
            "Step: 16561; Training Loss: 3.0562493801116943; Gradient Norm 0.3105838894844055; Learning Rate: 0.0001656061858725979; Time: 3258.117914199829 ms; Tokens/Sec : 160917.44185040076\n",
            "\n",
            "Training\n",
            "Step: 16562; Training Loss: 3.0390238761901855; Gradient Norm 0.35854628682136536; Learning Rate: 0.00016556787804040978; Time: 3256.093978881836 ms; Tokens/Sec : 161017.46552783588\n",
            "\n",
            "Training\n",
            "Step: 16563; Training Loss: 2.9959332942962646; Gradient Norm 0.3037322461605072; Learning Rate: 0.00016552958051672958; Time: 3259.053945541382 ms; Tokens/Sec : 160871.22482807116\n",
            "\n",
            "Training\n",
            "Step: 16564; Training Loss: 3.0741147994995117; Gradient Norm 0.3170011341571808; Learning Rate: 0.00016549129330258402; Time: 3257.5628757476807 ms; Tokens/Sec : 160944.85969965035\n",
            "\n",
            "Training\n",
            "Step: 16565; Training Loss: 3.0293054580688477; Gradient Norm 0.3453295826911926; Learning Rate: 0.000165453016399; Time: 3256.225347518921 ms; Tokens/Sec : 161010.96946483786\n",
            "\n",
            "Training\n",
            "Step: 16566; Training Loss: 3.072970390319824; Gradient Norm 0.3065606355667114; Learning Rate: 0.00016541474980700393; Time: 3258.7947845458984 ms; Tokens/Sec : 160884.018375848\n",
            "\n",
            "Training\n",
            "Step: 16567; Training Loss: 3.0857858657836914; Gradient Norm 0.297141432762146; Learning Rate: 0.00016537649352762178; Time: 3255.2363872528076 ms; Tokens/Sec : 161059.88555948235\n",
            "\n",
            "Training\n",
            "Step: 16568; Training Loss: 3.0452077388763428; Gradient Norm 0.2928306758403778; Learning Rate: 0.0001653382475618793; Time: 3255.0601959228516 ms; Tokens/Sec : 161068.6034798068\n",
            "\n",
            "Training\n",
            "Step: 16569; Training Loss: 3.1327457427978516; Gradient Norm 0.30431875586509705; Learning Rate: 0.00016530001191080227; Time: 3255.251407623291 ms; Tokens/Sec : 161059.14239748096\n",
            "\n",
            "Training\n",
            "Step: 16570; Training Loss: 3.056579351425171; Gradient Norm 0.28226467967033386; Learning Rate: 0.00016526178657541604; Time: 3259.5763206481934 ms; Tokens/Sec : 160845.44383232636\n",
            "\n",
            "Training\n",
            "Step: 16571; Training Loss: 3.0716171264648438; Gradient Norm 0.2919299006462097; Learning Rate: 0.00016522357155674533; Time: 3255.4619312286377 ms; Tokens/Sec : 161048.72705488204\n",
            "\n",
            "Training\n",
            "Step: 16572; Training Loss: 3.0853519439697266; Gradient Norm 0.2722271978855133; Learning Rate: 0.00016518536685581497; Time: 3257.024049758911 ms; Tokens/Sec : 160971.48562314376\n",
            "\n",
            "Training\n",
            "Step: 16573; Training Loss: 3.0479235649108887; Gradient Norm 0.30419641733169556; Learning Rate: 0.00016514717247364966; Time: 3257.120370864868 ms; Tokens/Sec : 160966.7252981458\n",
            "\n",
            "Training\n",
            "Step: 16574; Training Loss: 3.0383663177490234; Gradient Norm 0.28829652070999146; Learning Rate: 0.0001651089884112732; Time: 3257.6396465301514 ms; Tokens/Sec : 160941.06681149988\n",
            "\n",
            "Training\n",
            "Step: 16575; Training Loss: 3.0149972438812256; Gradient Norm 0.28927773237228394; Learning Rate: 0.00016507081466970977; Time: 3258.617877960205 ms; Tokens/Sec : 160892.75258263428\n",
            "\n",
            "Training\n",
            "Step: 16576; Training Loss: 3.034518003463745; Gradient Norm 0.31118789315223694; Learning Rate: 0.00016503265124998288; Time: 3257.22074508667 ms; Tokens/Sec : 160961.76496200275\n",
            "\n",
            "Training\n",
            "Step: 16577; Training Loss: 3.0474417209625244; Gradient Norm 0.2839696705341339; Learning Rate: 0.00016499449815311583; Time: 3258.111000061035 ms; Tokens/Sec : 160917.78333831424\n",
            "\n",
            "Training\n",
            "Step: 16578; Training Loss: 3.030905246734619; Gradient Norm 0.26916491985321045; Learning Rate: 0.0001649563553801318; Time: 3259.1910362243652 ms; Tokens/Sec : 160864.45813479085\n",
            "\n",
            "Training\n",
            "Step: 16579; Training Loss: 2.9836483001708984; Gradient Norm 0.29785653948783875; Learning Rate: 0.00016491822293205347; Time: 3256.417989730835 ms; Tokens/Sec : 161001.44442554686\n",
            "\n",
            "Training\n",
            "Step: 16580; Training Loss: 3.028172731399536; Gradient Norm 0.2884778082370758; Learning Rate: 0.00016488010080990362; Time: 3255.8157444000244 ms; Tokens/Sec : 161031.22570795688\n",
            "\n",
            "Training\n",
            "Step: 16581; Training Loss: 3.0096092224121094; Gradient Norm 0.3179451525211334; Learning Rate: 0.00016484198901470416; Time: 3256.2386989593506 ms; Tokens/Sec : 161010.30927725148\n",
            "\n",
            "Training\n",
            "Step: 16582; Training Loss: 3.0154683589935303; Gradient Norm 0.2971416115760803; Learning Rate: 0.00016480388754747723; Time: 3259.0699195861816 ms; Tokens/Sec : 160870.43633190022\n",
            "\n",
            "Training\n",
            "Step: 16583; Training Loss: 3.0418105125427246; Gradient Norm 0.31106486916542053; Learning Rate: 0.00016476579640924447; Time: 3257.6379776000977 ms; Tokens/Sec : 160941.1492636892\n",
            "\n",
            "Training\n",
            "Step: 16584; Training Loss: 3.0300283432006836; Gradient Norm 0.28837960958480835; Learning Rate: 0.0001647277156010273; Time: 3255.8321952819824 ms; Tokens/Sec : 161030.41205862645\n",
            "\n",
            "Training\n",
            "Step: 16585; Training Loss: 3.0260846614837646; Gradient Norm 0.28480207920074463; Learning Rate: 0.00016468964512384688; Time: 3259.507894515991 ms; Tokens/Sec : 160848.8204253459\n",
            "\n",
            "Training\n",
            "Step: 16586; Training Loss: 3.051201105117798; Gradient Norm 0.299906462430954; Learning Rate: 0.000164651584978724; Time: 3257.758140563965 ms; Tokens/Sec : 160935.21292198758\n",
            "\n",
            "Training\n",
            "Step: 16587; Training Loss: 3.0462024211883545; Gradient Norm 0.3047206401824951; Learning Rate: 0.0001646135351666793; Time: 3257.4994564056396 ms; Tokens/Sec : 160947.99308992212\n",
            "\n",
            "Training\n",
            "Step: 16588; Training Loss: 3.0407767295837402; Gradient Norm 0.38551583886146545; Learning Rate: 0.00016457549568873304; Time: 3256.88099861145 ms; Tokens/Sec : 160978.55593235575\n",
            "\n",
            "Training\n",
            "Step: 16589; Training Loss: 3.0806784629821777; Gradient Norm 0.38848820328712463; Learning Rate: 0.0001645374665459053; Time: 3256.9408416748047 ms; Tokens/Sec : 160975.59811077113\n",
            "\n",
            "Training\n",
            "Step: 16590; Training Loss: 3.0775306224823; Gradient Norm 0.377941370010376; Learning Rate: 0.00016449944773921574; Time: 3257.579565048218 ms; Tokens/Sec : 160944.0351435406\n",
            "\n",
            "Training\n",
            "Step: 16591; Training Loss: 3.0917670726776123; Gradient Norm 0.3562478721141815; Learning Rate: 0.0001644614392696839; Time: 3258.1422328948975 ms; Tokens/Sec : 160916.2407664947\n",
            "\n",
            "Training\n",
            "Step: 16592; Training Loss: 3.094013214111328; Gradient Norm 0.41497451066970825; Learning Rate: 0.000164423441138329; Time: 3260.2550983428955 ms; Tokens/Sec : 160811.95617682868\n",
            "\n",
            "Training\n",
            "Step: 16593; Training Loss: 3.0770435333251953; Gradient Norm 0.2919422388076782; Learning Rate: 0.00016438545334616998; Time: 3259.3581676483154 ms; Tokens/Sec : 160856.20942306044\n",
            "\n",
            "Training\n",
            "Step: 16594; Training Loss: 3.0330088138580322; Gradient Norm 0.35022950172424316; Learning Rate: 0.00016434747589422522; Time: 3258.646011352539 ms; Tokens/Sec : 160891.363521375\n",
            "\n",
            "Training\n",
            "Step: 16595; Training Loss: 3.119690179824829; Gradient Norm 0.30647969245910645; Learning Rate: 0.00016430950878351336; Time: 3257.169723510742 ms; Tokens/Sec : 160964.28632981886\n",
            "\n",
            "Training\n",
            "Step: 16596; Training Loss: 3.1771860122680664; Gradient Norm 0.31815874576568604; Learning Rate: 0.00016427155201505242; Time: 3261.0249519348145 ms; Tokens/Sec : 160773.99214284826\n",
            "\n",
            "Training\n",
            "Step: 16597; Training Loss: 3.0185658931732178; Gradient Norm 0.4289597272872925; Learning Rate: 0.00016423360558986025; Time: 3259.0713500976562 ms; Tokens/Sec : 160870.36572067993\n",
            "\n",
            "Training\n",
            "Step: 16598; Training Loss: 3.0379526615142822; Gradient Norm 0.3257198929786682; Learning Rate: 0.00016419566950895412; Time: 3260.4470252990723 ms; Tokens/Sec : 160802.48994442978\n",
            "\n",
            "Training\n",
            "Step: 16599; Training Loss: 2.9968974590301514; Gradient Norm 0.3746126890182495; Learning Rate: 0.00016415774377335162; Time: 3259.2594623565674 ms; Tokens/Sec : 160861.08088520207\n",
            "\n",
            "Training\n",
            "Step: 16600; Training Loss: 3.0622446537017822; Gradient Norm 0.3067072033882141; Learning Rate: 0.00016411982838406962; Time: 3258.234977722168 ms; Tokens/Sec : 160911.66032676064\n",
            "\n",
            "Training\n",
            "Step: 16601; Training Loss: 3.0576138496398926; Gradient Norm 0.3065145015716553; Learning Rate: 0.0001640819233421247; Time: 3258.6894035339355 ms; Tokens/Sec : 160889.22111798316\n",
            "\n",
            "Training\n",
            "Step: 16602; Training Loss: 3.0393331050872803; Gradient Norm 0.3193512260913849; Learning Rate: 0.0001640440286485333; Time: 3258.3727836608887 ms; Tokens/Sec : 160904.8549107218\n",
            "\n",
            "Training\n",
            "Step: 16603; Training Loss: 3.0667476654052734; Gradient Norm 0.30597439408302307; Learning Rate: 0.00016400614430431165; Time: 3259.631872177124 ms; Tokens/Sec : 160842.7026607227\n",
            "\n",
            "Training\n",
            "Step: 16604; Training Loss: 3.0692901611328125; Gradient Norm 0.3010341227054596; Learning Rate: 0.0001639682703104757; Time: 3256.7801475524902 ms; Tokens/Sec : 160983.5408736475\n",
            "\n",
            "Training\n",
            "Step: 16605; Training Loss: 3.009807825088501; Gradient Norm 0.3232971429824829; Learning Rate: 0.00016393040666804083; Time: 3257.9505443573 ms; Tokens/Sec : 160925.70862011873\n",
            "\n",
            "Training\n",
            "Step: 16606; Training Loss: 3.0026402473449707; Gradient Norm 0.278054803609848; Learning Rate: 0.00016389255337802237; Time: 3260.226249694824 ms; Tokens/Sec : 160813.3791478663\n",
            "\n",
            "Training\n",
            "Step: 16607; Training Loss: 3.1516802310943604; Gradient Norm 0.31075572967529297; Learning Rate: 0.0001638547104414356; Time: 3260.185480117798 ms; Tokens/Sec : 160815.39016640742\n",
            "\n",
            "Training\n",
            "Step: 16608; Training Loss: 3.046013593673706; Gradient Norm 0.33133620023727417; Learning Rate: 0.00016381687785929503; Time: 3261.0325813293457 ms; Tokens/Sec : 160773.61600179912\n",
            "\n",
            "Training\n",
            "Step: 16609; Training Loss: 3.052955150604248; Gradient Norm 0.2817172706127167; Learning Rate: 0.00016377905563261512; Time: 3259.9048614501953 ms; Tokens/Sec : 160829.23345399907\n",
            "\n",
            "Training\n",
            "Step: 16610; Training Loss: 3.0855915546417236; Gradient Norm 0.31648939847946167; Learning Rate: 0.00016374124376241023; Time: 3260.1969242095947 ms; Tokens/Sec : 160814.8256648972\n",
            "\n",
            "Training\n",
            "Step: 16611; Training Loss: 3.005915880203247; Gradient Norm 0.31997379660606384; Learning Rate: 0.00016370344224969418; Time: 3258.9573860168457 ms; Tokens/Sec : 160875.9912754778\n",
            "\n",
            "Training\n",
            "Step: 16612; Training Loss: 3.054133892059326; Gradient Norm 0.3022775948047638; Learning Rate: 0.00016366565109548066; Time: 3258.0833435058594 ms; Tokens/Sec : 160919.14930446196\n",
            "\n",
            "Training\n",
            "Step: 16613; Training Loss: 3.0352470874786377; Gradient Norm 0.32366377115249634; Learning Rate: 0.00016362787030078295; Time: 3258.0485343933105 ms; Tokens/Sec : 160920.86857067922\n",
            "\n",
            "Training\n",
            "Step: 16614; Training Loss: 3.027226686477661; Gradient Norm 0.31031322479248047; Learning Rate: 0.00016359009986661424; Time: 3257.5228214263916 ms; Tokens/Sec : 160946.83866878538\n",
            "\n",
            "Training\n",
            "Step: 16615; Training Loss: 3.061061143875122; Gradient Norm 0.2873029112815857; Learning Rate: 0.00016355233979398732; Time: 3259.1419219970703 ms; Tokens/Sec : 160866.88231077016\n",
            "\n",
            "Training\n",
            "Step: 16616; Training Loss: 3.006554365158081; Gradient Norm 0.2685425877571106; Learning Rate: 0.0001635145900839147; Time: 3255.5317878723145 ms; Tokens/Sec : 161045.2712988724\n",
            "\n",
            "Training\n",
            "Step: 16617; Training Loss: 3.0706987380981445; Gradient Norm 0.3095115125179291; Learning Rate: 0.00016347685073740865; Time: 3261.1684799194336 ms; Tokens/Sec : 160766.916284237\n",
            "\n",
            "Training\n",
            "Step: 16618; Training Loss: 3.045192003250122; Gradient Norm 0.30416545271873474; Learning Rate: 0.00016343912175548108; Time: 3259.5248222351074 ms; Tokens/Sec : 160847.9850877428\n",
            "\n",
            "Training\n",
            "Step: 16619; Training Loss: 3.0084590911865234; Gradient Norm 0.3051631450653076; Learning Rate: 0.0001634014031391438; Time: 3258.9452266693115 ms; Tokens/Sec : 160876.59151480425\n",
            "\n",
            "Training\n",
            "Step: 16620; Training Loss: 3.0668230056762695; Gradient Norm 0.2861526906490326; Learning Rate: 0.00016336369488940814; Time: 3260.331630706787 ms; Tokens/Sec : 160808.18130956293\n",
            "\n",
            "Training\n",
            "Step: 16621; Training Loss: 2.980682849884033; Gradient Norm 0.3147434890270233; Learning Rate: 0.00016332599700728532; Time: 3260.258197784424 ms; Tokens/Sec : 160811.80329714096\n",
            "\n",
            "Training\n",
            "Step: 16622; Training Loss: 3.1406493186950684; Gradient Norm 0.3408172130584717; Learning Rate: 0.00016328830949378616; Time: 3261.0552310943604 ms; Tokens/Sec : 160772.49934342786\n",
            "\n",
            "Training\n",
            "Step: 16623; Training Loss: 3.0142219066619873; Gradient Norm 0.40423184633255005; Learning Rate: 0.00016325063234992122; Time: 3259.004831314087 ms; Tokens/Sec : 160873.6492080001\n",
            "\n",
            "Training\n",
            "Step: 16624; Training Loss: 2.971944808959961; Gradient Norm 0.3354361355304718; Learning Rate: 0.000163212965576701; Time: 3261.31534576416 ms; Tokens/Sec : 160759.67651547474\n",
            "\n",
            "Training\n",
            "Step: 16625; Training Loss: 3.0965850353240967; Gradient Norm 0.3961702883243561; Learning Rate: 0.0001631753091751351; Time: 3260.263204574585 ms; Tokens/Sec : 160811.5563382594\n",
            "\n",
            "Training\n",
            "Step: 16626; Training Loss: 3.065828800201416; Gradient Norm 0.4618363082408905; Learning Rate: 0.00016313766314623367; Time: 3261.030912399292 ms; Tokens/Sec : 160773.69828250323\n",
            "\n",
            "Training\n",
            "Step: 16627; Training Loss: 3.0567080974578857; Gradient Norm 0.3686462342739105; Learning Rate: 0.00016310002749100614; Time: 3262.9542350769043 ms; Tokens/Sec : 160678.93149216758\n",
            "\n",
            "Training\n",
            "Step: 16628; Training Loss: 3.029148578643799; Gradient Norm 0.3600730895996094; Learning Rate: 0.00016306240221046156; Time: 3260.511875152588 ms; Tokens/Sec : 160799.29166811085\n",
            "\n",
            "Training\n",
            "Step: 16629; Training Loss: 3.0932457447052; Gradient Norm 0.3489813208580017; Learning Rate: 0.00016302478730560879; Time: 3263.286828994751 ms; Tokens/Sec : 160662.5551090481\n",
            "\n",
            "Training\n",
            "Step: 16630; Training Loss: 3.087522268295288; Gradient Norm 0.34502771496772766; Learning Rate: 0.0001629871827774567; Time: 3261.568546295166 ms; Tokens/Sec : 160747.19649707858\n",
            "\n",
            "Training\n",
            "Step: 16631; Training Loss: 3.023803472518921; Gradient Norm 0.3271823823451996; Learning Rate: 0.0001629495886270136; Time: 3261.326789855957 ms; Tokens/Sec : 160759.11240503323\n",
            "\n",
            "Training\n",
            "Step: 16632; Training Loss: 3.042576789855957; Gradient Norm 0.3737586736679077; Learning Rate: 0.00016291200485528737; Time: 3260.2312564849854 ms; Tokens/Sec : 160813.13218414466\n",
            "\n",
            "Training\n",
            "Step: 16633; Training Loss: 3.0142481327056885; Gradient Norm 0.3250484764575958; Learning Rate: 0.00016287443146328602; Time: 3260.648250579834 ms; Tokens/Sec : 160792.5662962164\n",
            "\n",
            "Training\n",
            "Step: 16634; Training Loss: 3.071035385131836; Gradient Norm 0.3080572187900543; Learning Rate: 0.0001628368684520172; Time: 3260.380744934082 ms; Tokens/Sec : 160805.7589024315\n",
            "\n",
            "Training\n",
            "Step: 16635; Training Loss: 2.9900612831115723; Gradient Norm 0.30011627078056335; Learning Rate: 0.00016279931582248775; Time: 3259.2647075653076 ms; Tokens/Sec : 160860.82200781006\n",
            "\n",
            "Training\n",
            "Step: 16636; Training Loss: 3.017561912536621; Gradient Norm 0.3388306498527527; Learning Rate: 0.00016276177357570483; Time: 3257.8234672546387 ms; Tokens/Sec : 160931.98580886165\n",
            "\n",
            "Training\n",
            "Step: 16637; Training Loss: 3.0337352752685547; Gradient Norm 0.3205370604991913; Learning Rate: 0.00016272424171267537; Time: 3256.1752796173096 ms; Tokens/Sec : 161013.44521650515\n",
            "\n",
            "Training\n",
            "Step: 16638; Training Loss: 3.0229058265686035; Gradient Norm 0.2708488702774048; Learning Rate: 0.00016268672023440553; Time: 3262.5110149383545 ms; Tokens/Sec : 160700.7601198571\n",
            "\n",
            "Training\n",
            "Step: 16639; Training Loss: 3.057032823562622; Gradient Norm 0.29700422286987305; Learning Rate: 0.0001626492091419014; Time: 3257.2948932647705 ms; Tokens/Sec : 160958.10087201186\n",
            "\n",
            "Training\n",
            "Step: 16640; Training Loss: 3.0800483226776123; Gradient Norm 0.2811957001686096; Learning Rate: 0.0001626117084361689; Time: 3257.925510406494 ms; Tokens/Sec : 160926.94517579197\n",
            "\n",
            "Training\n",
            "Step: 16641; Training Loss: 3.016831636428833; Gradient Norm 0.31816184520721436; Learning Rate: 0.0001625742181182138; Time: 3261.7268562316895 ms; Tokens/Sec : 160739.39453216997\n",
            "\n",
            "Training\n",
            "Step: 16642; Training Loss: 3.0092239379882812; Gradient Norm 0.2948751747608185; Learning Rate: 0.00016253673818904116; Time: 3258.552312850952 ms; Tokens/Sec : 160895.98989475582\n",
            "\n",
            "Training\n",
            "Step: 16643; Training Loss: 3.039482355117798; Gradient Norm 0.2806412875652313; Learning Rate: 0.00016249926864965605; Time: 3261.094093322754 ms; Tokens/Sec : 160770.5834288881\n",
            "\n",
            "Training\n",
            "Step: 16644; Training Loss: 3.0346474647521973; Gradient Norm 0.27944961190223694; Learning Rate: 0.00016246180950106327; Time: 3257.8909397125244 ms; Tokens/Sec : 160928.65283153494\n",
            "\n",
            "Training\n",
            "Step: 16645; Training Loss: 3.0139715671539307; Gradient Norm 0.2938166856765747; Learning Rate: 0.00016242436074426724; Time: 3258.8138580322266 ms; Tokens/Sec : 160883.07673902597\n",
            "\n",
            "Training\n",
            "Step: 16646; Training Loss: 3.058190107345581; Gradient Norm 0.2616539001464844; Learning Rate: 0.00016238692238027214; Time: 3257.54976272583 ms; Tokens/Sec : 160945.50757109228\n",
            "\n",
            "Training\n",
            "Step: 16647; Training Loss: 2.998281955718994; Gradient Norm 0.2701348662376404; Learning Rate: 0.0001623494944100819; Time: 3260.4732513427734 ms; Tokens/Sec : 160801.19650853766\n",
            "\n",
            "Training\n",
            "Step: 16648; Training Loss: 3.0337376594543457; Gradient Norm 0.31290555000305176; Learning Rate: 0.00016231207683470018; Time: 3258.7356567382812 ms; Tokens/Sec : 160886.93752004663\n",
            "\n",
            "Training\n",
            "Step: 16649; Training Loss: 3.0055689811706543; Gradient Norm 0.2941233813762665; Learning Rate: 0.00016227466965513024; Time: 3258.8138580322266 ms; Tokens/Sec : 160883.07673902597\n",
            "\n",
            "Training\n",
            "Step: 16650; Training Loss: 3.0351643562316895; Gradient Norm 0.2694765627384186; Learning Rate: 0.00016223727287237525; Time: 3259.037971496582 ms; Tokens/Sec : 160872.01333197166\n",
            "\n",
            "Training\n",
            "Step: 16651; Training Loss: 3.036059617996216; Gradient Norm 0.2844058871269226; Learning Rate: 0.0001621998864874379; Time: 3258.5527896881104 ms; Tokens/Sec : 160895.96635019738\n",
            "\n",
            "Training\n",
            "Step: 16652; Training Loss: 3.060061454772949; Gradient Norm 0.31573736667633057; Learning Rate: 0.00016216251050132078; Time: 3261.972665786743 ms; Tokens/Sec : 160727.28183746105\n",
            "\n",
            "Training\n",
            "Step: 16653; Training Loss: 3.0167717933654785; Gradient Norm 0.2601645588874817; Learning Rate: 0.0001621251449150261; Time: 3258.842706680298 ms; Tokens/Sec : 160881.65253427625\n",
            "\n",
            "Training\n",
            "Step: 16654; Training Loss: 3.0078134536743164; Gradient Norm 0.2876698672771454; Learning Rate: 0.00016208778972955594; Time: 3257.4539184570312 ms; Tokens/Sec : 160950.24308074976\n",
            "\n",
            "Training\n",
            "Step: 16655; Training Loss: 3.003852605819702; Gradient Norm 0.2689451277256012; Learning Rate: 0.00016205044494591156; Time: 3261.7509365081787 ms; Tokens/Sec : 160738.20785386793\n",
            "\n",
            "Training\n",
            "Step: 16656; Training Loss: 3.02644944190979; Gradient Norm 0.2832702696323395; Learning Rate: 0.00016201311056509475; Time: 3257.608652114868 ms; Tokens/Sec : 160942.59808022907\n",
            "\n",
            "Training\n",
            "Step: 16657; Training Loss: 3.057709217071533; Gradient Norm 0.3991006910800934; Learning Rate: 0.00016197578658810657; Time: 3257.516860961914 ms; Tokens/Sec : 160947.13316239987\n",
            "\n",
            "Training\n",
            "Step: 16658; Training Loss: 3.0803511142730713; Gradient Norm 0.33257806301116943; Learning Rate: 0.0001619384730159479; Time: 3259.3255043029785 ms; Tokens/Sec : 160857.8214442934\n",
            "\n",
            "Training\n",
            "Step: 16659; Training Loss: 3.0973963737487793; Gradient Norm 0.30873796343803406; Learning Rate: 0.000161901169849619; Time: 3259.7951889038086 ms; Tokens/Sec : 160834.64439258393\n",
            "\n",
            "Training\n",
            "Step: 16660; Training Loss: 3.032789945602417; Gradient Norm 0.3443154990673065; Learning Rate: 0.00016186387709012047; Time: 3255.70011138916 ms; Tokens/Sec : 161036.94506933377\n",
            "\n",
            "Training\n",
            "Step: 16661; Training Loss: 3.0961804389953613; Gradient Norm 0.3158704340457916; Learning Rate: 0.00016182659473845229; Time: 3255.389928817749 ms; Tokens/Sec : 161052.28911560963\n",
            "\n",
            "Training\n",
            "Step: 16662; Training Loss: 3.187227249145508; Gradient Norm 0.40450671315193176; Learning Rate: 0.00016178932279561396; Time: 25776.09395980835 ms; Tokens/Sec : 20340.08724586051\n",
            "\n",
            "Training\n",
            "Step: 16663; Training Loss: 3.009014129638672; Gradient Norm 0.3934190273284912; Learning Rate: 0.00016175206126260502; Time: 3238.7514114379883 ms; Tokens/Sec : 161879.66700637236\n",
            "\n",
            "Training\n",
            "Step: 16664; Training Loss: 3.0464401245117188; Gradient Norm 0.319522887468338; Learning Rate: 0.00016171481014042472; Time: 3243.0896759033203 ms; Tokens/Sec : 161663.12140411793\n",
            "\n",
            "Training\n",
            "Step: 16665; Training Loss: 2.97908878326416; Gradient Norm 0.3768021762371063; Learning Rate: 0.000161677569430072; Time: 3245.734453201294 ms; Tokens/Sec : 161531.39067889258\n",
            "\n",
            "Training\n",
            "Step: 16666; Training Loss: 3.0972838401794434; Gradient Norm 0.33206892013549805; Learning Rate: 0.0001616403391325453; Time: 3246.899127960205 ms; Tokens/Sec : 161473.44876998774\n",
            "\n",
            "Training\n",
            "Step: 16667; Training Loss: 3.0310745239257812; Gradient Norm 0.2868165075778961; Learning Rate: 0.00016160311924884286; Time: 3250.3111362457275 ms; Tokens/Sec : 161303.94230675988\n",
            "\n",
            "Training\n",
            "Step: 16668; Training Loss: 3.0195741653442383; Gradient Norm 0.3290477693080902; Learning Rate: 0.00016156590977996308; Time: 3251.163959503174 ms; Tokens/Sec : 161261.63015171926\n",
            "\n",
            "Training\n",
            "Step: 16669; Training Loss: 3.1004674434661865; Gradient Norm 0.31398263573646545; Learning Rate: 0.00016152871072690344; Time: 3252.511501312256 ms; Tokens/Sec : 161194.81815466945\n",
            "\n",
            "Training\n",
            "Step: 16670; Training Loss: 3.0447864532470703; Gradient Norm 0.3479022979736328; Learning Rate: 0.00016149152209066146; Time: 3252.0592212677 ms; Tokens/Sec : 161217.2363194619\n",
            "\n",
            "Training\n",
            "Step: 16671; Training Loss: 3.035914421081543; Gradient Norm 0.31147369742393494; Learning Rate: 0.00016145434387223446; Time: 3250.3268718719482 ms; Tokens/Sec : 161303.16139497957\n",
            "\n",
            "Training\n",
            "Step: 16672; Training Loss: 3.0363166332244873; Gradient Norm 0.34039461612701416; Learning Rate: 0.00016141717607261928; Time: 3249.7706413269043 ms; Tokens/Sec : 161330.77003425985\n",
            "\n",
            "Training\n",
            "Step: 16673; Training Loss: 3.0531234741210938; Gradient Norm 0.2996468245983124; Learning Rate: 0.00016138001869281263; Time: 3253.7355422973633 ms; Tokens/Sec : 161134.17737380593\n",
            "\n",
            "Training\n",
            "Step: 16674; Training Loss: 3.0657081604003906; Gradient Norm 0.29579970240592957; Learning Rate: 0.00016134287173381082; Time: 3252.4826526641846 ms; Tokens/Sec : 161196.24790943725\n",
            "\n",
            "Training\n",
            "Step: 16675; Training Loss: 3.0342624187469482; Gradient Norm 0.2589349150657654; Learning Rate: 0.00016130573519660999; Time: 3251.446008682251 ms; Tokens/Sec : 161247.64138786483\n",
            "\n",
            "Training\n",
            "Step: 16676; Training Loss: 3.029183864593506; Gradient Norm 0.35163572430610657; Learning Rate: 0.00016126860908220592; Time: 3253.7450790405273 ms; Tokens/Sec : 161133.70508872299\n",
            "\n",
            "Training\n",
            "Step: 16677; Training Loss: 3.0265684127807617; Gradient Norm 0.2842263877391815; Learning Rate: 0.0001612314933915942; Time: 3253.338575363159 ms; Tokens/Sec : 161153.83869675337\n",
            "\n",
            "Training\n",
            "Step: 16678; Training Loss: 3.0729424953460693; Gradient Norm 0.28129836916923523; Learning Rate: 0.00016119438812577; Time: 3254.0338039398193 ms; Tokens/Sec : 161119.40796841713\n",
            "\n",
            "Training\n",
            "Step: 16679; Training Loss: 3.051276445388794; Gradient Norm 0.28519463539123535; Learning Rate: 0.00016115729328572837; Time: 3254.6427249908447 ms; Tokens/Sec : 161089.2636461272\n",
            "\n",
            "Training\n",
            "Step: 16680; Training Loss: 3.057633638381958; Gradient Norm 0.3178568184375763; Learning Rate: 0.00016112020887246398; Time: 3255.6142807006836 ms; Tokens/Sec : 161041.19063120743\n",
            "\n",
            "Training\n",
            "Step: 16681; Training Loss: 3.0123109817504883; Gradient Norm 0.32029834389686584; Learning Rate: 0.0001610831348869713; Time: 3255.927562713623 ms; Tokens/Sec : 161025.69541290315\n",
            "\n",
            "Training\n",
            "Step: 16682; Training Loss: 3.017685651779175; Gradient Norm 0.34902557730674744; Learning Rate: 0.0001610460713302442; Time: 3254.9142837524414 ms; Tokens/Sec : 161075.82390635874\n",
            "\n",
            "Training\n",
            "Step: 16683; Training Loss: 2.9513092041015625; Gradient Norm 0.3125763535499573; Learning Rate: 0.00016100901820327685; Time: 3254.8065185546875 ms; Tokens/Sec : 161081.15705532403\n",
            "\n",
            "Training\n",
            "Step: 16684; Training Loss: 3.0472114086151123; Gradient Norm 0.2967377305030823; Learning Rate: 0.00016097197550706273; Time: 3255.594491958618 ms; Tokens/Sec : 161042.16950083972\n",
            "\n",
            "Training\n",
            "Step: 16685; Training Loss: 3.0463061332702637; Gradient Norm 0.28087809681892395; Learning Rate: 0.0001609349432425952; Time: 3254.2943954467773 ms; Tokens/Sec : 161106.50613956555\n",
            "\n",
            "Training\n",
            "Step: 16686; Training Loss: 3.049670696258545; Gradient Norm 0.2915547490119934; Learning Rate: 0.00016089792141086697; Time: 3252.699136734009 ms; Tokens/Sec : 161185.51945951892\n",
            "\n",
            "Training\n",
            "Step: 16687; Training Loss: 3.005917549133301; Gradient Norm 0.3126160204410553; Learning Rate: 0.00016086091001287113; Time: 3254.1890144348145 ms; Tokens/Sec : 161111.72328170924\n",
            "\n",
            "Training\n",
            "Step: 16688; Training Loss: 2.9617090225219727; Gradient Norm 0.2622739374637604; Learning Rate: 0.00016082390904960012; Time: 3254.1215419769287 ms; Tokens/Sec : 161115.0638465357\n",
            "\n",
            "Training\n",
            "Step: 16689; Training Loss: 3.0419399738311768; Gradient Norm 0.2853638827800751; Learning Rate: 0.0001607869185220458; Time: 3253.6048889160156 ms; Tokens/Sec : 161140.64795823256\n",
            "\n",
            "Training\n",
            "Step: 16690; Training Loss: 3.0525834560394287; Gradient Norm 0.29535973072052; Learning Rate: 0.00016074993843120036; Time: 3254.624366760254 ms; Tokens/Sec : 161090.17229594802\n",
            "\n",
            "Training\n",
            "Step: 16691; Training Loss: 3.0936241149902344; Gradient Norm 0.3469853401184082; Learning Rate: 0.00016071296877805534; Time: 3256.3045024871826 ms; Tokens/Sec : 161007.0555746694\n",
            "\n",
            "Training\n",
            "Step: 16692; Training Loss: 3.043900489807129; Gradient Norm 0.33332395553588867; Learning Rate: 0.0001606760095636022; Time: 3258.228540420532 ms; Tokens/Sec : 160911.9782408914\n",
            "\n",
            "Training\n",
            "Step: 16693; Training Loss: 3.1031341552734375; Gradient Norm 0.35435643792152405; Learning Rate: 0.0001606390607888316; Time: 3256.9544315338135 ms; Tokens/Sec : 160974.92642938037\n",
            "\n",
            "Training\n",
            "Step: 16694; Training Loss: 3.059597969055176; Gradient Norm 0.3449825346469879; Learning Rate: 0.0001606021224547347; Time: 3259.242057800293 ms; Tokens/Sec : 160861.93989342698\n",
            "\n",
            "Training\n",
            "Step: 16695; Training Loss: 3.079026937484741; Gradient Norm 0.334585577249527; Learning Rate: 0.00016056519456230202; Time: 3262.3531818389893 ms; Tokens/Sec : 160708.53484491791\n",
            "\n",
            "Training\n",
            "Step: 16696; Training Loss: 3.049151659011841; Gradient Norm 0.3767724335193634; Learning Rate: 0.00016052827711252347; Time: 3258.981466293335 ms; Tokens/Sec : 160874.8025794418\n",
            "\n",
            "Training\n",
            "Step: 16697; Training Loss: 3.0621683597564697; Gradient Norm 0.3173074424266815; Learning Rate: 0.0001604913701063891; Time: 3261.382818222046 ms; Tokens/Sec : 160756.35067146685\n",
            "\n",
            "Training\n",
            "Step: 16698; Training Loss: 3.035975694656372; Gradient Norm 0.3790300190448761; Learning Rate: 0.00016045447354488883; Time: 3261.636972427368 ms; Tokens/Sec : 160743.82416931447\n",
            "\n",
            "Training\n",
            "Step: 16699; Training Loss: 2.9905691146850586; Gradient Norm 0.3413858115673065; Learning Rate: 0.0001604175874290117; Time: 3262.3703479766846 ms; Tokens/Sec : 160707.68921902517\n",
            "\n",
            "Training\n",
            "Step: 16700; Training Loss: 3.1047379970550537; Gradient Norm 0.3234870731830597; Learning Rate: 0.00016038071175974696; Time: 3260.291814804077 ms; Tokens/Sec : 160810.14515920146\n",
            "\n",
            "Training\n",
            "Step: 16701; Training Loss: 2.968334436416626; Gradient Norm 0.38776257634162903; Learning Rate: 0.0001603438465380833; Time: 3260.502576828003 ms; Tokens/Sec : 160799.75023668172\n",
            "\n",
            "Training\n",
            "Step: 16702; Training Loss: 3.0832126140594482; Gradient Norm 0.2759993076324463; Learning Rate: 0.00016030699176500956; Time: 3259.9143981933594 ms; Tokens/Sec : 160828.76295480636\n",
            "\n",
            "Training\n",
            "Step: 16703; Training Loss: 3.0544533729553223; Gradient Norm 0.2681281566619873; Learning Rate: 0.0001602701474415137; Time: 3258.4989070892334 ms; Tokens/Sec : 160898.62692890645\n",
            "\n",
            "Training\n",
            "Step: 16704; Training Loss: 3.062108039855957; Gradient Norm 0.3138972520828247; Learning Rate: 0.00016023331356858383; Time: 3260.1048946380615 ms; Tokens/Sec : 160819.36531008666\n",
            "\n",
            "Training\n",
            "Step: 16705; Training Loss: 3.00423002243042; Gradient Norm 0.3003576695919037; Learning Rate: 0.00016019649014720758; Time: 3260.8845233917236 ms; Tokens/Sec : 160780.9158033832\n",
            "\n",
            "Training\n",
            "Step: 16706; Training Loss: 3.016169786453247; Gradient Norm 0.3059998154640198; Learning Rate: 0.0001601596771783724; Time: 3260.86688041687 ms; Tokens/Sec : 160781.78571122012\n",
            "\n",
            "Training\n",
            "Step: 16707; Training Loss: 3.0585362911224365; Gradient Norm 0.265227347612381; Learning Rate: 0.0001601228746630654; Time: 3262.2289657592773 ms; Tokens/Sec : 160714.6541530303\n",
            "\n",
            "Training\n",
            "Step: 16708; Training Loss: 3.026839017868042; Gradient Norm 0.2887347638607025; Learning Rate: 0.00016008608260227353; Time: 3263.7646198272705 ms; Tokens/Sec : 160639.0353075606\n",
            "\n",
            "Training\n",
            "Step: 16709; Training Loss: 3.0841104984283447; Gradient Norm 0.3224853575229645; Learning Rate: 0.00016004930099698325; Time: 3262.340784072876 ms; Tokens/Sec : 160709.14558026387\n",
            "\n",
            "Training\n",
            "Step: 16710; Training Loss: 3.012730121612549; Gradient Norm 0.2767992913722992; Learning Rate: 0.0001600125298481809; Time: 3261.857748031616 ms; Tokens/Sec : 160732.94438311545\n",
            "\n",
            "Training\n",
            "Step: 16711; Training Loss: 3.0193703174591064; Gradient Norm 0.290506511926651; Learning Rate: 0.0001599757691568525; Time: 3262.6771926879883 ms; Tokens/Sec : 160692.57515729291\n",
            "\n",
            "Training\n",
            "Step: 16712; Training Loss: 2.9669153690338135; Gradient Norm 0.2999081611633301; Learning Rate: 0.00015993901892398377; Time: 3262.04514503479 ms; Tokens/Sec : 160723.71064454058\n",
            "\n",
            "Training\n",
            "Step: 16713; Training Loss: 3.037588119506836; Gradient Norm 0.27673566341400146; Learning Rate: 0.00015990227915056016; Time: 3267.9378986358643 ms; Tokens/Sec : 160433.89325692315\n",
            "\n",
            "Training\n",
            "Step: 16714; Training Loss: 3.023837089538574; Gradient Norm 0.3340218663215637; Learning Rate: 0.0001598655498375669; Time: 3260.4427337646484 ms; Tokens/Sec : 160802.7015995568\n",
            "\n",
            "Training\n",
            "Step: 16715; Training Loss: 2.9691364765167236; Gradient Norm 0.2961961627006531; Learning Rate: 0.00015982883098598894; Time: 3259.857654571533 ms; Tokens/Sec : 160831.562465543\n",
            "\n",
            "Training\n",
            "Step: 16716; Training Loss: 2.976837635040283; Gradient Norm 0.28024089336395264; Learning Rate: 0.00015979212259681054; Time: 3261.106252670288 ms; Tokens/Sec : 160769.98398034342\n",
            "\n",
            "Training\n",
            "Step: 16717; Training Loss: 3.00927996635437; Gradient Norm 0.3424745202064514; Learning Rate: 0.00015975542467101643; Time: 3263.1194591522217 ms; Tokens/Sec : 160670.79571037623\n",
            "\n",
            "Training\n",
            "Step: 16718; Training Loss: 2.985447406768799; Gradient Norm 0.3001326024532318; Learning Rate: 0.00015971873720959046; Time: 3262.6664638519287 ms; Tokens/Sec : 160693.1035730271\n",
            "\n",
            "Training\n",
            "Step: 16719; Training Loss: 3.0121796131134033; Gradient Norm 0.31296807527542114; Learning Rate: 0.00015968206021351662; Time: 3261.2550258636475 ms; Tokens/Sec : 160762.64991302168\n",
            "\n",
            "Training\n",
            "Step: 16720; Training Loss: 3.0434157848358154; Gradient Norm 0.3357532322406769; Learning Rate: 0.000159645393683778; Time: 3262.2199058532715 ms; Tokens/Sec : 160715.1004931614\n",
            "\n",
            "Training\n",
            "Step: 16721; Training Loss: 2.982185125350952; Gradient Norm 0.3431795835494995; Learning Rate: 0.00015960873762135817; Time: 3261.935234069824 ms; Tokens/Sec : 160729.12623279178\n",
            "\n",
            "Training\n",
            "Step: 16722; Training Loss: 2.980708599090576; Gradient Norm 0.2723540663719177; Learning Rate: 0.00015957209202724002; Time: 3261.904716491699 ms; Tokens/Sec : 160730.62997495875\n",
            "\n",
            "Training\n",
            "Step: 16723; Training Loss: 3.015937566757202; Gradient Norm 0.31112146377563477; Learning Rate: 0.000159535456902406; Time: 3261.524438858032 ms; Tokens/Sec : 160749.3703722087\n",
            "\n",
            "Training\n",
            "Step: 16724; Training Loss: 3.0039217472076416; Gradient Norm 0.327103853225708; Learning Rate: 0.00015949883224783849; Time: 3261.1258029937744 ms; Tokens/Sec : 160769.02017048647\n",
            "\n",
            "Training\n",
            "Step: 16725; Training Loss: 3.0475170612335205; Gradient Norm 0.2963137626647949; Learning Rate: 0.00015946221806451986; Time: 3261.4450454711914 ms; Tokens/Sec : 160753.28349561518\n",
            "\n",
            "Training\n",
            "Step: 16726; Training Loss: 3.0385613441467285; Gradient Norm 0.36583009362220764; Learning Rate: 0.00015942561435343164; Time: 3262.3627185821533 ms; Tokens/Sec : 160708.06505165662\n",
            "\n",
            "Training\n",
            "Step: 16727; Training Loss: 3.063652515411377; Gradient Norm 0.2750099301338196; Learning Rate: 0.00015938902111555545; Time: 3261.9869709014893 ms; Tokens/Sec : 160726.57698418296\n",
            "\n",
            "Training\n",
            "Step: 16728; Training Loss: 3.133829116821289; Gradient Norm 0.33665430545806885; Learning Rate: 0.00015935243835187243; Time: 3262.2218132019043 ms; Tokens/Sec : 160715.00652661198\n",
            "\n",
            "Training\n",
            "Step: 16729; Training Loss: 3.0931270122528076; Gradient Norm 0.33851584792137146; Learning Rate: 0.0001593158660633638; Time: 3262.1521949768066 ms; Tokens/Sec : 160718.43637685568\n",
            "\n",
            "Training\n",
            "Step: 16730; Training Loss: 3.1240575313568115; Gradient Norm 0.30899229645729065; Learning Rate: 0.00015927930425101004; Time: 3262.4592781066895 ms; Tokens/Sec : 160703.30854957408\n",
            "\n",
            "Training\n",
            "Step: 16731; Training Loss: 3.021829128265381; Gradient Norm 0.36808809638023376; Learning Rate: 0.0001592427529157916; Time: 3262.096881866455 ms; Tokens/Sec : 160721.16156771567\n",
            "\n",
            "Training\n",
            "Step: 16732; Training Loss: 3.0476999282836914; Gradient Norm 0.3341430723667145; Learning Rate: 0.00015920621205868854; Time: 3263.36932182312 ms; Tokens/Sec : 160658.4938131061\n",
            "\n",
            "Training\n",
            "Step: 16733; Training Loss: 3.107090950012207; Gradient Norm 0.34936320781707764; Learning Rate: 0.00015916968168068078; Time: 3263.6542320251465 ms; Tokens/Sec : 160644.46866194872\n",
            "\n",
            "Training\n",
            "Step: 16734; Training Loss: 3.0277366638183594; Gradient Norm 0.2979387640953064; Learning Rate: 0.00015913316178274784; Time: 3266.700506210327 ms; Tokens/Sec : 160494.6639593301\n",
            "\n",
            "Training\n",
            "Step: 16735; Training Loss: 3.145266532897949; Gradient Norm 0.3495924472808838; Learning Rate: 0.00015909665236586894; Time: 3260.101795196533 ms; Tokens/Sec : 160819.51820415276\n",
            "\n",
            "Training\n",
            "Step: 16736; Training Loss: 3.08852481842041; Gradient Norm 0.36172032356262207; Learning Rate: 0.00015906015343102335; Time: 3262.742519378662 ms; Tokens/Sec : 160689.35776760048\n",
            "\n",
            "Training\n",
            "Step: 16737; Training Loss: 3.0057547092437744; Gradient Norm 0.3342430889606476; Learning Rate: 0.00015902366497918944; Time: 3262.188196182251 ms; Tokens/Sec : 160716.6627031438\n",
            "\n",
            "Training\n",
            "Step: 16738; Training Loss: 3.030585289001465; Gradient Norm 0.31969794631004333; Learning Rate: 0.0001589871870113458; Time: 3263.213634490967 ms; Tokens/Sec : 160666.15880078118\n",
            "\n",
            "Training\n",
            "Step: 16739; Training Loss: 3.0483927726745605; Gradient Norm 0.3416694402694702; Learning Rate: 0.00015895071952847062; Time: 3262.9354000091553 ms; Tokens/Sec : 160679.8590001288\n",
            "\n",
            "Training\n",
            "Step: 16740; Training Loss: 3.074838161468506; Gradient Norm 0.3340641260147095; Learning Rate: 0.00015891426253154166; Time: 3261.232376098633 ms; Tokens/Sec : 160763.76643457662\n",
            "\n",
            "Training\n",
            "Step: 16741; Training Loss: 3.0533270835876465; Gradient Norm 0.2886495590209961; Learning Rate: 0.00015887781602153656; Time: 3265.1121616363525 ms; Tokens/Sec : 160572.73810074764\n",
            "\n",
            "Training\n",
            "Step: 16742; Training Loss: 3.0399625301361084; Gradient Norm 0.29930973052978516; Learning Rate: 0.0001588413799994328; Time: 3261.972188949585 ms; Tokens/Sec : 160727.3053326768\n",
            "\n",
            "Training\n",
            "Step: 16743; Training Loss: 3.0461299419403076; Gradient Norm 0.33637723326683044; Learning Rate: 0.00015880495446620702; Time: 3267.270565032959 ms; Tokens/Sec : 160466.66156486835\n",
            "\n",
            "Training\n",
            "Step: 16744; Training Loss: 3.0425057411193848; Gradient Norm 0.3048415184020996; Learning Rate: 0.00015876853942283625; Time: 3258.4877014160156 ms; Tokens/Sec : 160899.18024615047\n",
            "\n",
            "Training\n",
            "Step: 16745; Training Loss: 3.0144028663635254; Gradient Norm 0.3148990571498871; Learning Rate: 0.00015873213487029695; Time: 3263.30304145813 ms; Tokens/Sec : 160661.7569190676\n",
            "\n",
            "Training\n",
            "Step: 16746; Training Loss: 3.0233452320098877; Gradient Norm 0.3242647349834442; Learning Rate: 0.00015869574080956526; Time: 3263.2901668548584 ms; Tokens/Sec : 160662.3907751685\n",
            "\n",
            "Training\n",
            "Step: 16747; Training Loss: 3.0775678157806396; Gradient Norm 0.29202330112457275; Learning Rate: 0.00015865935724161708; Time: 3261.8701457977295 ms; Tokens/Sec : 160732.3334668735\n",
            "\n",
            "Training\n",
            "Step: 16748; Training Loss: 3.0394034385681152; Gradient Norm 0.30818602442741394; Learning Rate: 0.000158622984167428; Time: 3261.2948417663574 ms; Tokens/Sec : 160760.68722325002\n",
            "\n",
            "Training\n",
            "Step: 16749; Training Loss: 3.043085813522339; Gradient Norm 0.3297079801559448; Learning Rate: 0.0001585866215879735; Time: 3260.8048915863037 ms; Tokens/Sec : 160784.8422188015\n",
            "\n",
            "Evaluating HellaSwag\n",
            "Evaluating Validation set\n",
            "Training\n",
            "Step: 16750; Training Loss: 3.136789321899414; Gradient Norm 0.500853419303894; Learning Rate: 0.00015855026950422837; Time: 3255.8164596557617 ms; Tokens/Sec : 161031.19033172808; Validation_loss: 2.9313793182373047; HellaSwag Acc: 0.337\n",
            "\n",
            "Checkpointing\n",
            "Training\n",
            "Step: 16751; Training Loss: 2.9816553592681885; Gradient Norm 0.3634907901287079; Learning Rate: 0.0001585139279171676; Time: 3275.731086730957 ms; Tokens/Sec : 160052.21006197354\n",
            "\n",
            "Training\n",
            "Step: 16752; Training Loss: 3.005413055419922; Gradient Norm 0.3756612539291382; Learning Rate: 0.00015847759682776562; Time: 3245.713710784912 ms; Tokens/Sec : 161532.42297923166\n",
            "\n",
            "Training\n",
            "Step: 16753; Training Loss: 2.967681884765625; Gradient Norm 0.3686780035495758; Learning Rate: 0.0001584412762369968; Time: 3243.0484294891357 ms; Tokens/Sec : 161665.17750170908\n",
            "\n",
            "Training\n",
            "Step: 16754; Training Loss: 2.9984922409057617; Gradient Norm 0.353989839553833; Learning Rate: 0.0001584049661458347; Time: 3248.115062713623 ms; Tokens/Sec : 161413.0010412827\n",
            "\n",
            "Training\n",
            "Step: 16755; Training Loss: 2.9985110759735107; Gradient Norm 0.314400851726532; Learning Rate: 0.00015836866655525333; Time: 3247.9825019836426 ms; Tokens/Sec : 161419.58883085154\n",
            "\n",
            "Training\n",
            "Step: 16756; Training Loss: 2.9971182346343994; Gradient Norm 0.2922317087650299; Learning Rate: 0.000158332377466226; Time: 3245.2595233917236 ms; Tokens/Sec : 161555.03010497292\n",
            "\n",
            "Training\n",
            "Step: 16757; Training Loss: 2.983997106552124; Gradient Norm 0.3374019265174866; Learning Rate: 0.00015829609887972563; Time: 3242.83766746521 ms; Tokens/Sec : 161675.6846203202\n",
            "\n",
            "Training\n",
            "Step: 16758; Training Loss: 3.041050910949707; Gradient Norm 0.33119848370552063; Learning Rate: 0.00015825983079672504; Time: 3248.9261627197266 ms; Tokens/Sec : 161372.70400787145\n",
            "\n",
            "Training\n",
            "Step: 16759; Training Loss: 3.050873041152954; Gradient Norm 0.30153724551200867; Learning Rate: 0.00015822357321819706; Time: 3249.2806911468506 ms; Tokens/Sec : 161355.09666139365\n",
            "\n",
            "Training\n",
            "Step: 16760; Training Loss: 2.9958131313323975; Gradient Norm 0.331617146730423; Learning Rate: 0.00015818732614511358; Time: 3239.0058040618896 ms; Tokens/Sec : 161866.95292194732\n",
            "\n",
            "Training\n",
            "Step: 16761; Training Loss: 3.123218536376953; Gradient Norm 0.32652831077575684; Learning Rate: 0.00015815108957844667; Time: 3246.1230754852295 ms; Tokens/Sec : 161512.05231848135\n",
            "\n",
            "Training\n",
            "Step: 16762; Training Loss: 3.0229079723358154; Gradient Norm 0.3195955157279968; Learning Rate: 0.000158114863519168; Time: 3246.2689876556396 ms; Tokens/Sec : 161504.7927308776\n",
            "\n",
            "Training\n",
            "Step: 16763; Training Loss: 3.1121232509613037; Gradient Norm 0.3832828104496002; Learning Rate: 0.00015807864796824915; Time: 3244.0669536590576 ms; Tokens/Sec : 161614.4202599282\n",
            "\n",
            "Training\n",
            "Step: 16764; Training Loss: 3.1070754528045654; Gradient Norm 0.3574568033218384; Learning Rate: 0.000158042442926661; Time: 3241.169214248657 ms; Tokens/Sec : 161758.91023990748\n",
            "\n",
            "Training\n",
            "Step: 16765; Training Loss: 3.037923812866211; Gradient Norm 0.318668007850647; Learning Rate: 0.00015800624839537448; Time: 3247.586488723755 ms; Tokens/Sec : 161439.27246292864\n",
            "\n",
            "Training\n",
            "Step: 16766; Training Loss: 3.01214599609375; Gradient Norm 0.3691394329071045; Learning Rate: 0.00015797006437536017; Time: 3249.286651611328 ms; Tokens/Sec : 161354.80067294292\n",
            "\n",
            "Training\n",
            "Step: 16767; Training Loss: 3.1171326637268066; Gradient Norm 0.29521796107292175; Learning Rate: 0.00015793389086758827; Time: 3242.9966926574707 ms; Tokens/Sec : 161667.75661136204\n",
            "\n",
            "Training\n",
            "Step: 16768; Training Loss: 3.085988998413086; Gradient Norm 0.3495820462703705; Learning Rate: 0.0001578977278730288; Time: 3248.267650604248 ms; Tokens/Sec : 161405.41863983133\n",
            "\n",
            "Training\n",
            "Step: 16769; Training Loss: 3.1094586849212646; Gradient Norm 0.3388056755065918; Learning Rate: 0.0001578615753926515; Time: 3246.1466789245605 ms; Tokens/Sec : 161510.8779291807\n",
            "\n",
            "Training\n",
            "Step: 16770; Training Loss: 3.1074635982513428; Gradient Norm 0.27750661969184875; Learning Rate: 0.0001578254334274258; Time: 3243.130683898926 ms; Tokens/Sec : 161661.07724333066\n",
            "\n",
            "Training\n",
            "Step: 16771; Training Loss: 3.0886499881744385; Gradient Norm 0.34686869382858276; Learning Rate: 0.00015778930197832074; Time: 3245.967388153076 ms; Tokens/Sec : 161519.79897072064\n",
            "\n",
            "Training\n",
            "Step: 16772; Training Loss: 3.0602035522460938; Gradient Norm 0.32254600524902344; Learning Rate: 0.00015775318104630526; Time: 3247.2658157348633 ms; Tokens/Sec : 161455.21486400167\n",
            "\n",
            "Training\n",
            "Step: 16773; Training Loss: 3.1032843589782715; Gradient Norm 0.3223312199115753; Learning Rate: 0.000157717070632348; Time: 3244.3418502807617 ms; Tokens/Sec : 161600.72649392008\n",
            "\n",
            "Training\n",
            "Step: 16774; Training Loss: 3.0811898708343506; Gradient Norm 0.3224707543849945; Learning Rate: 0.00015768097073741712; Time: 3241.4305210113525 ms; Tokens/Sec : 161745.87010318454\n",
            "\n",
            "Training\n",
            "Step: 16775; Training Loss: 3.051403045654297; Gradient Norm 0.36883676052093506; Learning Rate: 0.0001576448813624807; Time: 3243.603467941284 ms; Tokens/Sec : 161637.5137040921\n",
            "\n",
            "Training\n",
            "Step: 16776; Training Loss: 3.058622360229492; Gradient Norm 0.2950853705406189; Learning Rate: 0.00015760880250850661; Time: 3238.098382949829 ms; Tokens/Sec : 161912.31333817175\n",
            "\n",
            "Training\n",
            "Step: 16777; Training Loss: 3.0212090015411377; Gradient Norm 0.2816137373447418; Learning Rate: 0.00015757273417646195; Time: 3243.3629035949707 ms; Tokens/Sec : 161649.50256379723\n",
            "\n",
            "Training\n",
            "Step: 16778; Training Loss: 3.059417486190796; Gradient Norm 0.36116164922714233; Learning Rate: 0.00015753667636731424; Time: 3239.2194271087646 ms; Tokens/Sec : 161856.27797001842\n",
            "\n",
            "Training\n",
            "Step: 16779; Training Loss: 3.075254440307617; Gradient Norm 0.2918432354927063; Learning Rate: 0.00015750062908203015; Time: 3243.7820434570312 ms; Tokens/Sec : 161628.61529415363\n",
            "\n",
            "Training\n",
            "Step: 16780; Training Loss: 3.047515392303467; Gradient Norm 0.3018975853919983; Learning Rate: 0.00015746459232157656; Time: 3245.1179027557373 ms; Tokens/Sec : 161562.0805502251\n",
            "\n",
            "Training\n",
            "Step: 16781; Training Loss: 3.04021954536438; Gradient Norm 0.2843196988105774; Learning Rate: 0.00015742856608691938; Time: 3244.5478439331055 ms; Tokens/Sec : 161590.46659778876\n",
            "\n",
            "Training\n",
            "Step: 16782; Training Loss: 3.0252432823181152; Gradient Norm 0.2835908830165863; Learning Rate: 0.00015739255037902495; Time: 3239.654302597046 ms; Tokens/Sec : 161834.55116791575\n",
            "\n",
            "Training\n",
            "Step: 16783; Training Loss: 3.0526556968688965; Gradient Norm 0.2762620449066162; Learning Rate: 0.00015735654519885903; Time: 3240.243434906006 ms; Tokens/Sec : 161805.12684696133\n",
            "\n",
            "Training\n",
            "Step: 16784; Training Loss: 3.1095826625823975; Gradient Norm 0.2911302447319031; Learning Rate: 0.00015732055054738689; Time: 3238.960027694702 ms; Tokens/Sec : 161869.24059484512\n",
            "\n",
            "Training\n",
            "Step: 16785; Training Loss: 3.0409255027770996; Gradient Norm 0.27428656816482544; Learning Rate: 0.00015728456642557378; Time: 3245.4280853271484 ms; Tokens/Sec : 161546.63921543967\n",
            "\n",
            "Training\n",
            "Step: 16786; Training Loss: 2.9946823120117188; Gradient Norm 0.29011160135269165; Learning Rate: 0.00015724859283438488; Time: 3241.4443492889404 ms; Tokens/Sec : 161745.18008153077\n",
            "\n",
            "Training\n",
            "Step: 16787; Training Loss: 2.989245891571045; Gradient Norm 0.2698432207107544; Learning Rate: 0.00015721262977478454; Time: 3241.502046585083 ms; Tokens/Sec : 161742.30108919306\n",
            "\n",
            "Training\n",
            "Step: 16788; Training Loss: 3.0084285736083984; Gradient Norm 0.250274658203125; Learning Rate: 0.0001571766772477371; Time: 3240.3833866119385 ms; Tokens/Sec : 161798.13850612968\n",
            "\n",
            "Training\n",
            "Step: 16789; Training Loss: 2.977569580078125; Gradient Norm 0.29667219519615173; Learning Rate: 0.00015714073525420664; Time: 3240.9119606018066 ms; Tokens/Sec : 161771.75016585292\n",
            "\n",
            "Training\n",
            "Step: 16790; Training Loss: 2.9628403186798096; Gradient Norm 0.2886098027229309; Learning Rate: 0.00015710480379515723; Time: 3237.945079803467 ms; Tokens/Sec : 161919.97920848697\n",
            "\n",
            "Training\n",
            "Step: 16791; Training Loss: 3.0373313426971436; Gradient Norm 0.2705388069152832; Learning Rate: 0.00015706888287155202; Time: 3239.76469039917 ms; Tokens/Sec : 161829.03701422916\n",
            "\n",
            "Training\n",
            "Step: 16792; Training Loss: 2.9951539039611816; Gradient Norm 0.27098533511161804; Learning Rate: 0.00015703297248435423; Time: 3248.643159866333 ms; Tokens/Sec : 161386.76185708624\n",
            "\n",
            "Training\n",
            "Step: 16793; Training Loss: 2.9829843044281006; Gradient Norm 0.28847554326057434; Learning Rate: 0.00015699707263452714; Time: 3241.455078125 ms; Tokens/Sec : 161744.6447239587\n",
            "\n",
            "Training\n",
            "Step: 16794; Training Loss: 2.995116710662842; Gradient Norm 0.29252833127975464; Learning Rate: 0.00015696118332303308; Time: 3241.9445514678955 ms; Tokens/Sec : 161720.22429026788\n",
            "\n",
            "Training\n",
            "Step: 16795; Training Loss: 2.960334062576294; Gradient Norm 0.25590863823890686; Learning Rate: 0.00015692530455083448; Time: 3241.8735027313232 ms; Tokens/Sec : 161723.76854256654\n",
            "\n",
            "Training\n",
            "Step: 16796; Training Loss: 3.1358084678649902; Gradient Norm 0.31791046261787415; Learning Rate: 0.00015688943631889335; Time: 3238.867998123169 ms; Tokens/Sec : 161873.8399662505\n",
            "\n",
            "Training\n",
            "Step: 16797; Training Loss: 3.0510079860687256; Gradient Norm 0.2989281117916107; Learning Rate: 0.00015685357862817178; Time: 3240.278482437134 ms; Tokens/Sec : 161803.37672880004\n",
            "\n",
            "Training\n",
            "Step: 16798; Training Loss: 3.101992607116699; Gradient Norm 0.3045000433921814; Learning Rate: 0.00015681773147963103; Time: 3243.288993835449 ms; Tokens/Sec : 161653.18631688983\n",
            "\n",
            "Training\n",
            "Step: 16799; Training Loss: 2.997629165649414; Gradient Norm 0.3124758303165436; Learning Rate: 0.00015678189487423246; Time: 3244.126558303833 ms; Tokens/Sec : 161611.4509028649\n",
            "\n",
            "Training\n",
            "Step: 16800; Training Loss: 3.072844982147217; Gradient Norm 0.32199230790138245; Learning Rate: 0.0001567460688129369; Time: 3239.7232055664062 ms; Tokens/Sec : 161831.10924389536\n",
            "\n",
            "Training\n",
            "Step: 16801; Training Loss: 3.0540008544921875; Gradient Norm 0.33867818117141724; Learning Rate: 0.00015671025329670518; Time: 3243.2034015655518 ms; Tokens/Sec : 161657.4525504373\n",
            "\n",
            "Training\n",
            "Step: 16802; Training Loss: 3.12152361869812; Gradient Norm 0.3025686740875244; Learning Rate: 0.00015667444832649755; Time: 3238.5499477386475 ms; Tokens/Sec : 161889.7372159073\n",
            "\n",
            "Training\n",
            "Step: 16803; Training Loss: 3.059901714324951; Gradient Norm 0.37384167313575745; Learning Rate: 0.00015663865390327423; Time: 3242.936134338379 ms; Tokens/Sec : 161670.77558157488\n",
            "\n",
            "Training\n",
            "Step: 16804; Training Loss: 3.0244295597076416; Gradient Norm 0.30808714032173157; Learning Rate: 0.00015660287002799495; Time: 3240.504503250122 ms; Tokens/Sec : 161792.09116177927\n",
            "\n",
            "Training\n",
            "Step: 16805; Training Loss: 3.0264034271240234; Gradient Norm 0.3685159683227539; Learning Rate: 0.0001565670967016193; Time: 3236.736297607422 ms; Tokens/Sec : 161980.44937659916\n",
            "\n",
            "Training\n",
            "Step: 16806; Training Loss: 3.097543478012085; Gradient Norm 0.3473741114139557; Learning Rate: 0.00015653133392510657; Time: 3235.874891281128 ms; Tokens/Sec : 162023.56939468297\n",
            "\n",
            "Training\n",
            "Step: 16807; Training Loss: 3.071342945098877; Gradient Norm 0.3095574975013733; Learning Rate: 0.00015649558169941564; Time: 3240.767240524292 ms; Tokens/Sec : 161778.9742638785\n",
            "\n",
            "Training\n",
            "Step: 16808; Training Loss: 3.0614781379699707; Gradient Norm 0.37237539887428284; Learning Rate: 0.00015645984002550528; Time: 3241.0659790039062 ms; Tokens/Sec : 161764.06262520215\n",
            "\n",
            "Training\n",
            "Step: 16809; Training Loss: 3.0204434394836426; Gradient Norm 0.3248671293258667; Learning Rate: 0.0001564241089043339; Time: 3240.7615184783936 ms; Tokens/Sec : 161779.25990869096\n",
            "\n",
            "Training\n",
            "Step: 16810; Training Loss: 3.005340814590454; Gradient Norm 0.33721113204956055; Learning Rate: 0.00015638838833685967; Time: 3240.581512451172 ms; Tokens/Sec : 161788.24633342712\n",
            "\n",
            "Training\n",
            "Step: 16811; Training Loss: 3.1146910190582275; Gradient Norm 0.31184303760528564; Learning Rate: 0.0001563526783240402; Time: 3240.267753601074 ms; Tokens/Sec : 161803.9124752367\n",
            "\n",
            "Training\n",
            "Step: 16812; Training Loss: 3.046978712081909; Gradient Norm 0.30828869342803955; Learning Rate: 0.00015631697886683328; Time: 3243.135452270508 ms; Tokens/Sec : 161660.83955357084\n",
            "\n",
            "Training\n",
            "Step: 16813; Training Loss: 2.9557716846466064; Gradient Norm 0.2763119339942932; Learning Rate: 0.00015628128996619627; Time: 3239.8855686187744 ms; Tokens/Sec : 161822.99926830875\n",
            "\n",
            "Training\n",
            "Step: 16814; Training Loss: 3.0283095836639404; Gradient Norm 0.32197490334510803; Learning Rate: 0.0001562456116230859; Time: 3240.3194904327393 ms; Tokens/Sec : 161801.3290195598\n",
            "\n",
            "Training\n",
            "Step: 16815; Training Loss: 3.0893595218658447; Gradient Norm 0.30329564213752747; Learning Rate: 0.00015620994383845887; Time: 3239.473819732666 ms; Tokens/Sec : 161843.56755914955\n",
            "\n",
            "Training\n",
            "Step: 16816; Training Loss: 3.0239009857177734; Gradient Norm 0.2793745994567871; Learning Rate: 0.00015617428661327188; Time: 3242.9044246673584 ms; Tokens/Sec : 161672.3564259156\n",
            "\n",
            "Training\n",
            "Step: 16817; Training Loss: 3.006141185760498; Gradient Norm 0.31505000591278076; Learning Rate: 0.00015613863994848104; Time: 3239.2046451568604 ms; Tokens/Sec : 161857.01659322332\n",
            "\n",
            "Training\n",
            "Step: 16818; Training Loss: 3.0479421615600586; Gradient Norm 0.3160113990306854; Learning Rate: 0.00015610300384504198; Time: 3242.8436279296875 ms; Tokens/Sec : 161675.38745453433\n",
            "\n",
            "Training\n",
            "Step: 16819; Training Loss: 3.077760696411133; Gradient Norm 0.30765581130981445; Learning Rate: 0.0001560673783039103; Time: 3238.295316696167 ms; Tokens/Sec : 161902.46680000104\n",
            "\n",
            "Training\n",
            "Step: 16820; Training Loss: 3.0141043663024902; Gradient Norm 0.28002026677131653; Learning Rate: 0.0001560317633260416; Time: 3242.814779281616 ms; Tokens/Sec : 161676.8257470894\n",
            "\n",
            "Training\n",
            "Step: 16821; Training Loss: 2.9660565853118896; Gradient Norm 0.2860855162143707; Learning Rate: 0.00015599615891239059; Time: 3237.307548522949 ms; Tokens/Sec : 161951.86652538192\n",
            "\n",
            "Training\n",
            "Step: 16822; Training Loss: 2.9618842601776123; Gradient Norm 0.2777079939842224; Learning Rate: 0.00015596056506391213; Time: 3239.6810054779053 ms; Tokens/Sec : 161833.2172561104\n",
            "\n",
            "Training\n",
            "Step: 16823; Training Loss: 2.9837543964385986; Gradient Norm 0.31327760219573975; Learning Rate: 0.0001559249817815605; Time: 3238.2941246032715 ms; Tokens/Sec : 161902.5264001402\n",
            "\n",
            "Training\n",
            "Step: 16824; Training Loss: 3.0170347690582275; Gradient Norm 0.3050500452518463; Learning Rate: 0.0001558894090662902; Time: 3241.6722774505615 ms; Tokens/Sec : 161733.80746937517\n",
            "\n",
            "Training\n",
            "Step: 16825; Training Loss: 2.9729647636413574; Gradient Norm 0.3081117868423462; Learning Rate: 0.00015585384691905484; Time: 3235.9254360198975 ms; Tokens/Sec : 162021.03860738533\n",
            "\n",
            "Training\n",
            "Step: 16826; Training Loss: 3.0374581813812256; Gradient Norm 0.3023645281791687; Learning Rate: 0.00015581829534080802; Time: 3239.9563789367676 ms; Tokens/Sec : 161819.46257315096\n",
            "\n",
            "Training\n",
            "Step: 16827; Training Loss: 3.0240895748138428; Gradient Norm 0.31263959407806396; Learning Rate: 0.00015578275433250318; Time: 3238.4376525878906 ms; Tokens/Sec : 161895.35085877986\n",
            "\n",
            "Training\n",
            "Step: 16828; Training Loss: 2.970348596572876; Gradient Norm 0.2911382019519806; Learning Rate: 0.00015574722389509327; Time: 3235.513687133789 ms; Tokens/Sec : 162041.6572752766\n",
            "\n",
            "Training\n",
            "Step: 16829; Training Loss: 2.9750959873199463; Gradient Norm 0.3382667899131775; Learning Rate: 0.000155711704029531; Time: 3240.2851581573486 ms; Tokens/Sec : 161803.0433772522\n",
            "\n",
            "Training\n",
            "Step: 16830; Training Loss: 3.0543267726898193; Gradient Norm 0.28924688696861267; Learning Rate: 0.0001556761947367689; Time: 3239.887237548828 ms; Tokens/Sec : 161822.91591007833\n",
            "\n",
            "Training\n",
            "Step: 16831; Training Loss: 3.0134944915771484; Gradient Norm 0.29372936487197876; Learning Rate: 0.00015564069601775913; Time: 3241.4333820343018 ms; Tokens/Sec : 161745.72733960074\n",
            "\n",
            "Training\n",
            "Step: 16832; Training Loss: 3.1176838874816895; Gradient Norm 0.35168009996414185; Learning Rate: 0.00015560520787345357; Time: 3237.2000217437744 ms; Tokens/Sec : 161957.24591574143\n",
            "\n",
            "Training\n",
            "Step: 16833; Training Loss: 3.0800185203552246; Gradient Norm 0.29459622502326965; Learning Rate: 0.00015556973030480387; Time: 3239.4821643829346 ms; Tokens/Sec : 161843.15066289855\n",
            "\n",
            "Training\n",
            "Step: 16834; Training Loss: 3.097628593444824; Gradient Norm 0.2746020257472992; Learning Rate: 0.0001555342633127613; Time: 3239.548683166504 ms; Tokens/Sec : 161839.82748100997\n",
            "\n",
            "Training\n",
            "Step: 16835; Training Loss: 3.1256473064422607; Gradient Norm 0.29879170656204224; Learning Rate: 0.00015549880689827696; Time: 3243.5383796691895 ms; Tokens/Sec : 161640.75729341994\n",
            "\n",
            "Training\n",
            "Step: 16836; Training Loss: 3.0585012435913086; Gradient Norm 0.3432484269142151; Learning Rate: 0.00015546336106230156; Time: 3237.48779296875 ms; Tokens/Sec : 161942.84998963107\n",
            "\n",
            "Training\n",
            "Step: 16837; Training Loss: 3.032099962234497; Gradient Norm 0.30785760283470154; Learning Rate: 0.0001554279258057857; Time: 3240.6609058380127 ms; Tokens/Sec : 161784.28266144765\n",
            "\n",
            "Training\n",
            "Step: 16838; Training Loss: 3.100620985031128; Gradient Norm 0.3016678988933563; Learning Rate: 0.00015539250112967928; Time: 3240.4773235321045 ms; Tokens/Sec : 161793.44820365187\n",
            "\n",
            "Training\n",
            "Step: 16839; Training Loss: 3.0829758644104004; Gradient Norm 0.32436463236808777; Learning Rate: 0.0001553570870349325; Time: 3238.6770248413086 ms; Tokens/Sec : 161883.3850916917\n",
            "\n",
            "Training\n",
            "Step: 16840; Training Loss: 3.075849771499634; Gradient Norm 0.3200770914554596; Learning Rate: 0.00015532168352249488; Time: 3240.4277324676514 ms; Tokens/Sec : 161795.924268536\n",
            "\n",
            "Training\n",
            "Step: 16841; Training Loss: 3.055396795272827; Gradient Norm 0.320866197347641; Learning Rate: 0.00015528629059331588; Time: 3237.013578414917 ms; Tokens/Sec : 161966.57422015833\n",
            "\n",
            "Training\n",
            "Step: 16842; Training Loss: 3.16957426071167; Gradient Norm 0.2983807325363159; Learning Rate: 0.00015525090824834422; Time: 3238.748788833618 ms; Tokens/Sec : 161879.79808980913\n",
            "\n",
            "Training\n",
            "Step: 16843; Training Loss: 3.0191311836242676; Gradient Norm 0.3109505772590637; Learning Rate: 0.00015521553648852903; Time: 3240.692138671875 ms; Tokens/Sec : 161782.72343230594\n",
            "\n",
            "Training\n",
            "Step: 16844; Training Loss: 3.0510196685791016; Gradient Norm 0.32188180088996887; Learning Rate: 0.00015518017531481875; Time: 3240.040063858032 ms; Tokens/Sec : 161815.2830418126\n",
            "\n",
            "Training\n",
            "Step: 16845; Training Loss: 3.0594570636749268; Gradient Norm 0.3270682692527771; Learning Rate: 0.00015514482472816143; Time: 3237.5199794769287 ms; Tokens/Sec : 161941.23999960822\n",
            "\n",
            "Training\n",
            "Step: 16846; Training Loss: 3.0225679874420166; Gradient Norm 0.2723974883556366; Learning Rate: 0.00015510948472950497; Time: 3236.116647720337 ms; Tokens/Sec : 162011.46530652148\n",
            "\n",
            "Training\n",
            "Step: 16847; Training Loss: 3.0479509830474854; Gradient Norm 0.34502220153808594; Learning Rate: 0.00015507415531979728; Time: 3239.2590045928955 ms; Tokens/Sec : 161854.3003991407\n",
            "\n",
            "Training\n",
            "Step: 16848; Training Loss: 3.046327590942383; Gradient Norm 0.30177760124206543; Learning Rate: 0.00015503883649998544; Time: 3242.173194885254 ms; Tokens/Sec : 161708.8195125108\n",
            "\n",
            "Training\n",
            "Step: 16849; Training Loss: 3.0318992137908936; Gradient Norm 0.33126792311668396; Learning Rate: 0.00015500352827101652; Time: 3240.8342361450195 ms; Tokens/Sec : 161775.62991424144\n",
            "\n",
            "Training\n",
            "Step: 16850; Training Loss: 3.071336030960083; Gradient Norm 0.2625432312488556; Learning Rate: 0.00015496823063383753; Time: 3243.0899143218994 ms; Tokens/Sec : 161663.1095193128\n",
            "\n",
            "Training\n",
            "Step: 16851; Training Loss: 3.048732280731201; Gradient Norm 0.31377527117729187; Learning Rate: 0.00015493294358939492; Time: 3238.649368286133 ms; Tokens/Sec : 161884.76750030182\n",
            "\n",
            "Training\n",
            "Step: 16852; Training Loss: 3.053340435028076; Gradient Norm 0.2855035662651062; Learning Rate: 0.00015489766713863476; Time: 3235.8336448669434 ms; Tokens/Sec : 162025.63467120344\n",
            "\n",
            "Training\n",
            "Step: 16853; Training Loss: 3.0109784603118896; Gradient Norm 0.2720365822315216; Learning Rate: 0.00015486240128250296; Time: 3235.556125640869 ms; Tokens/Sec : 162039.53188917527\n",
            "\n",
            "Training\n",
            "Step: 16854; Training Loss: 3.0244035720825195; Gradient Norm 0.2753749489784241; Learning Rate: 0.00015482714602194547; Time: 3239.60542678833 ms; Tokens/Sec : 161836.99275987662\n",
            "\n",
            "Training\n",
            "Step: 16855; Training Loss: 3.061066150665283; Gradient Norm 0.3391743004322052; Learning Rate: 0.00015479190135790736; Time: 24077.54635810852 ms; Tokens/Sec : 21774.97624559394\n",
            "\n",
            "Training\n",
            "Step: 16856; Training Loss: 3.0151262283325195; Gradient Norm 0.2827100455760956; Learning Rate: 0.00015475666729133385; Time: 3232.802391052246 ms; Tokens/Sec : 162177.5588421751\n",
            "\n",
            "Training\n",
            "Step: 16857; Training Loss: 2.9601330757141113; Gradient Norm 0.33652809262275696; Learning Rate: 0.00015472144382316965; Time: 3231.0898303985596 ms; Tokens/Sec : 162263.51711655394\n",
            "\n",
            "Training\n",
            "Step: 16858; Training Loss: 2.9659411907196045; Gradient Norm 0.26949724555015564; Learning Rate: 0.00015468623095435933; Time: 3228.494644165039 ms; Tokens/Sec : 162393.95067529764\n",
            "\n",
            "Training\n",
            "Step: 16859; Training Loss: 2.98803973197937; Gradient Norm 0.2809976637363434; Learning Rate: 0.0001546510286858471; Time: 3229.6221256256104 ms; Tokens/Sec : 162337.25792253177\n",
            "\n",
            "Training\n",
            "Step: 16860; Training Loss: 3.0506935119628906; Gradient Norm 0.2625686824321747; Learning Rate: 0.000154615837018577; Time: 3229.973793029785 ms; Tokens/Sec : 162319.5832521621\n",
            "\n",
            "Training\n",
            "Step: 16861; Training Loss: 2.961137294769287; Gradient Norm 0.2819177508354187; Learning Rate: 0.00015458065595349253; Time: 3227.9953956604004 ms; Tokens/Sec : 162419.06686262123\n",
            "\n",
            "Training\n",
            "Step: 16862; Training Loss: 3.062030076980591; Gradient Norm 0.3235092759132385; Learning Rate: 0.00015454548549153715; Time: 3228.2238006591797 ms; Tokens/Sec : 162407.5753028474\n",
            "\n",
            "Training\n",
            "Step: 16863; Training Loss: 2.9839510917663574; Gradient Norm 0.3042020797729492; Learning Rate: 0.000154510325633654; Time: 3240.734577178955 ms; Tokens/Sec : 161780.60483323826\n",
            "\n",
            "Training\n",
            "Step: 16864; Training Loss: 3.0134694576263428; Gradient Norm 0.33131033182144165; Learning Rate: 0.00015447517638078574; Time: 3229.2826175689697 ms; Tokens/Sec : 162354.32512088033\n",
            "\n",
            "Training\n",
            "Step: 16865; Training Loss: 2.9779133796691895; Gradient Norm 0.2846924364566803; Learning Rate: 0.00015444003773387504; Time: 3231.719493865967 ms; Tokens/Sec : 162231.90193181552\n",
            "\n",
            "Training\n",
            "Step: 16866; Training Loss: 3.0223793983459473; Gradient Norm 0.3898451626300812; Learning Rate: 0.00015440490969386415; Time: 3231.731653213501 ms; Tokens/Sec : 162231.2915364336\n",
            "\n",
            "Training\n",
            "Step: 16867; Training Loss: 3.0766265392303467; Gradient Norm 0.4113561809062958; Learning Rate: 0.00015436979226169493; Time: 3241.8243885040283 ms; Tokens/Sec : 161726.2186869838\n",
            "\n",
            "Training\n",
            "Step: 16868; Training Loss: 3.0934906005859375; Gradient Norm 0.2900770902633667; Learning Rate: 0.00015433468543830906; Time: 3234.7612380981445 ms; Tokens/Sec : 162079.35034743135\n",
            "\n",
            "Training\n",
            "Step: 16869; Training Loss: 3.1324753761291504; Gradient Norm 0.33611392974853516; Learning Rate: 0.00015429958922464794; Time: 3239.8455142974854 ms; Tokens/Sec : 161824.99989160267\n",
            "\n",
            "Training\n",
            "Step: 16870; Training Loss: 3.0746355056762695; Gradient Norm 0.3481290936470032; Learning Rate: 0.00015426450362165272; Time: 3237.9117012023926 ms; Tokens/Sec : 161921.64839001218\n",
            "\n",
            "Training\n",
            "Step: 16871; Training Loss: 3.1156094074249268; Gradient Norm 0.3041352927684784; Learning Rate: 0.00015422942863026423; Time: 3240.7467365264893 ms; Tokens/Sec : 161779.9978291252\n",
            "\n",
            "Training\n",
            "Step: 16872; Training Loss: 3.0482590198516846; Gradient Norm 0.34525880217552185; Learning Rate: 0.0001541943642514228; Time: 3232.487916946411 ms; Tokens/Sec : 162193.3363621887\n",
            "\n",
            "Training\n",
            "Step: 16873; Training Loss: 3.069453716278076; Gradient Norm 0.3233341574668884; Learning Rate: 0.0001541593104860689; Time: 3234.0033054351807 ms; Tokens/Sec : 162117.33584775965\n",
            "\n",
            "Training\n",
            "Step: 16874; Training Loss: 3.1219143867492676; Gradient Norm 0.3488183319568634; Learning Rate: 0.00015412426733514247; Time: 3241.2798404693604 ms; Tokens/Sec : 161753.3893414397\n",
            "\n",
            "Training\n",
            "Step: 16875; Training Loss: 3.049466609954834; Gradient Norm 0.3039979338645935; Learning Rate: 0.00015408923479958312; Time: 3231.2915325164795 ms; Tokens/Sec : 162253.3883817325\n",
            "\n",
            "Training\n",
            "Step: 16876; Training Loss: 3.111359119415283; Gradient Norm 0.3268507122993469; Learning Rate: 0.00015405421288033016; Time: 3233.8268756866455 ms; Tokens/Sec : 162126.18057627985\n",
            "\n",
            "Training\n",
            "Step: 16877; Training Loss: 3.021458387374878; Gradient Norm 0.32756495475769043; Learning Rate: 0.00015401920157832286; Time: 3231.736898422241 ms; Tokens/Sec : 162231.02823003984\n",
            "\n",
            "Training\n",
            "Step: 16878; Training Loss: 3.019834280014038; Gradient Norm 0.3174495995044708; Learning Rate: 0.00015398420089450014; Time: 3231.3170433044434 ms; Tokens/Sec : 162252.1074143338\n",
            "\n",
            "Training\n",
            "Step: 16879; Training Loss: 3.092153310775757; Gradient Norm 0.3036411702632904; Learning Rate: 0.00015394921082980025; Time: 3228.5821437835693 ms; Tokens/Sec : 162389.54954560575\n",
            "\n",
            "Training\n",
            "Step: 16880; Training Loss: 3.052896738052368; Gradient Norm 0.2877134680747986; Learning Rate: 0.00015391423138516147; Time: 3229.156970977783 ms; Tokens/Sec : 162360.64233236903\n",
            "\n",
            "Training\n",
            "Step: 16881; Training Loss: 3.072871685028076; Gradient Norm 0.27775874733924866; Learning Rate: 0.00015387926256152208; Time: 3229.3734550476074 ms; Tokens/Sec : 162349.7583348628\n",
            "\n",
            "Training\n",
            "Step: 16882; Training Loss: 3.0884485244750977; Gradient Norm 0.33214518427848816; Learning Rate: 0.00015384430435981953; Time: 3236.7641925811768 ms; Tokens/Sec : 161979.05340206553\n",
            "\n",
            "Training\n",
            "Step: 16883; Training Loss: 3.102187156677246; Gradient Norm 0.41468924283981323; Learning Rate: 0.0001538093567809912; Time: 3242.7818775177 ms; Tokens/Sec : 161678.46614504163\n",
            "\n",
            "Training\n",
            "Step: 16884; Training Loss: 3.050231456756592; Gradient Norm 0.379827082157135; Learning Rate: 0.0001537744198259742; Time: 3239.1796112060547 ms; Tokens/Sec : 161858.26750273662\n",
            "\n",
            "Training\n",
            "Step: 16885; Training Loss: 3.008113145828247; Gradient Norm 0.3694610297679901; Learning Rate: 0.00015373949349570558; Time: 3240.699052810669 ms; Tokens/Sec : 161782.3782635056\n",
            "\n",
            "Training\n",
            "Step: 16886; Training Loss: 3.0872488021850586; Gradient Norm 0.32644951343536377; Learning Rate: 0.00015370457779112165; Time: 3241.7871952056885 ms; Tokens/Sec : 161728.07418555257\n",
            "\n",
            "Training\n",
            "Step: 16887; Training Loss: 3.048966646194458; Gradient Norm 0.2831580936908722; Learning Rate: 0.00015366967271315877; Time: 3241.896867752075 ms; Tokens/Sec : 161722.60296594206\n",
            "\n",
            "Training\n",
            "Step: 16888; Training Loss: 3.0713424682617188; Gradient Norm 0.33336907625198364; Learning Rate: 0.0001536347782627529; Time: 3237.7798557281494 ms; Tokens/Sec : 161928.24199349157\n",
            "\n",
            "Training\n",
            "Step: 16889; Training Loss: 3.019089698791504; Gradient Norm 0.32692593336105347; Learning Rate: 0.00015359989444083973; Time: 3243.971347808838 ms; Tokens/Sec : 161619.1833365402\n",
            "\n",
            "Training\n",
            "Step: 16890; Training Loss: 2.974653959274292; Gradient Norm 0.2838789224624634; Learning Rate: 0.00015356502124835462; Time: 3242.3856258392334 ms; Tokens/Sec : 161698.22485698242\n",
            "\n",
            "Training\n",
            "Step: 16891; Training Loss: 3.0030877590179443; Gradient Norm 0.36304280161857605; Learning Rate: 0.00015353015868623278; Time: 3240.510940551758 ms; Tokens/Sec : 161791.7697604595\n",
            "\n",
            "Training\n",
            "Step: 16892; Training Loss: 3.006377696990967; Gradient Norm 0.2732994556427002; Learning Rate: 0.00015349530675540896; Time: 3242.845058441162 ms; Tokens/Sec : 161675.31613490827\n",
            "\n",
            "Training\n",
            "Step: 16893; Training Loss: 3.034604072570801; Gradient Norm 0.31948864459991455; Learning Rate: 0.0001534604654568178; Time: 3263.0646228790283 ms; Tokens/Sec : 160673.4958063492\n",
            "\n",
            "Training\n",
            "Step: 16894; Training Loss: 2.9568614959716797; Gradient Norm 0.3063884675502777; Learning Rate: 0.0001534256347913935; Time: 3241.8487071990967 ms; Tokens/Sec : 161725.005499401\n",
            "\n",
            "Training\n",
            "Step: 16895; Training Loss: 3.0488388538360596; Gradient Norm 0.2868473529815674; Learning Rate: 0.00015339081476007006; Time: 3241.474390029907 ms; Tokens/Sec : 161743.6810892597\n",
            "\n",
            "Training\n",
            "Step: 16896; Training Loss: 3.0281002521514893; Gradient Norm 0.32935288548469543; Learning Rate: 0.00015335600536378114; Time: 3243.232250213623 ms; Tokens/Sec : 161656.01460255167\n",
            "\n",
            "Training\n",
            "Step: 16897; Training Loss: 3.0240137577056885; Gradient Norm 0.275413453578949; Learning Rate: 0.00015332120660346024; Time: 3240.224838256836 ms; Tokens/Sec : 161806.05549646195\n",
            "\n",
            "Training\n",
            "Step: 16898; Training Loss: 2.9439215660095215; Gradient Norm 0.3151794970035553; Learning Rate: 0.00015328641848004048; Time: 3246.671438217163 ms; Tokens/Sec : 161484.7729365251\n",
            "\n",
            "Training\n",
            "Step: 16899; Training Loss: 2.9667370319366455; Gradient Norm 0.3036256432533264; Learning Rate: 0.0001532516409944545; Time: 3242.1247959136963 ms; Tokens/Sec : 161711.2335283334\n",
            "\n",
            "Training\n",
            "Step: 16900; Training Loss: 2.975008964538574; Gradient Norm 0.2823224663734436; Learning Rate: 0.0001532168741476351; Time: 3243.0126667022705 ms; Tokens/Sec : 161666.96028761857\n",
            "\n",
            "Training\n",
            "Step: 16901; Training Loss: 3.024529457092285; Gradient Norm 0.3219301700592041; Learning Rate: 0.00015318211794051453; Time: 3242.2468662261963 ms; Tokens/Sec : 161705.14511445683\n",
            "\n",
            "Training\n",
            "Step: 16902; Training Loss: 3.062771797180176; Gradient Norm 0.3671095669269562; Learning Rate: 0.00015314737237402454; Time: 3240.2517795562744 ms; Tokens/Sec : 161804.71014872706\n",
            "\n",
            "Training\n",
            "Step: 16903; Training Loss: 3.1004958152770996; Gradient Norm 0.3326658308506012; Learning Rate: 0.00015311263744909704; Time: 3239.5718097686768 ms; Tokens/Sec : 161838.6721415004\n",
            "\n",
            "Training\n",
            "Step: 16904; Training Loss: 3.046469211578369; Gradient Norm 0.32744863629341125; Learning Rate: 0.00015307791316666343; Time: 3243.7198162078857 ms; Tokens/Sec : 161631.7159639657\n",
            "\n",
            "Training\n",
            "Step: 16905; Training Loss: 3.0192153453826904; Gradient Norm 0.31794416904449463; Learning Rate: 0.00015304319952765487; Time: 3240.814208984375 ms; Tokens/Sec : 161776.62963416358\n",
            "\n",
            "Training\n",
            "Step: 16906; Training Loss: 3.0853664875030518; Gradient Norm 0.3126310110092163; Learning Rate: 0.00015300849653300197; Time: 3241.6815757751465 ms; Tokens/Sec : 161733.34355785174\n",
            "\n",
            "Training\n",
            "Step: 16907; Training Loss: 3.043785333633423; Gradient Norm 0.3187611699104309; Learning Rate: 0.0001529738041836355; Time: 3241.6412830352783 ms; Tokens/Sec : 161735.35386034084\n",
            "\n",
            "Training\n",
            "Step: 16908; Training Loss: 3.091181516647339; Gradient Norm 0.3097299635410309; Learning Rate: 0.0001529391224804858; Time: 3244.015693664551 ms; Tokens/Sec : 161616.97399427387\n",
            "\n",
            "Training\n",
            "Step: 16909; Training Loss: 3.1555585861206055; Gradient Norm 0.3082776367664337; Learning Rate: 0.0001529044514244826; Time: 3241.3904666900635 ms; Tokens/Sec : 161747.86881982014\n",
            "\n",
            "Training\n",
            "Step: 16910; Training Loss: 3.0948774814605713; Gradient Norm 0.33747851848602295; Learning Rate: 0.0001528697910165557; Time: 3244.5120811462402 ms; Tokens/Sec : 161592.24773629953\n",
            "\n",
            "Training\n",
            "Step: 16911; Training Loss: 3.123908758163452; Gradient Norm 0.3353181481361389; Learning Rate: 0.00015283514125763464; Time: 3246.0649013519287 ms; Tokens/Sec : 161514.9468458389\n",
            "\n",
            "Training\n",
            "Step: 16912; Training Loss: 3.0528512001037598; Gradient Norm 0.3052873909473419; Learning Rate: 0.00015280050214864854; Time: 3240.198850631714 ms; Tokens/Sec : 161807.3532424666\n",
            "\n",
            "Training\n",
            "Step: 16913; Training Loss: 3.064441442489624; Gradient Norm 0.3040914833545685; Learning Rate: 0.0001527658736905261; Time: 3238.435745239258 ms; Tokens/Sec : 161895.4462106412\n",
            "\n",
            "Training\n",
            "Step: 16914; Training Loss: 3.0708446502685547; Gradient Norm 0.30837610363960266; Learning Rate: 0.0001527312558841958; Time: 3240.852117538452 ms; Tokens/Sec : 161774.73731760902\n",
            "\n",
            "Training\n",
            "Step: 16915; Training Loss: 3.023589611053467; Gradient Norm 0.2662755846977234; Learning Rate: 0.0001526966487305863; Time: 3242.133617401123 ms; Tokens/Sec : 161710.79352993058\n",
            "\n",
            "Training\n",
            "Step: 16916; Training Loss: 3.0798873901367188; Gradient Norm 0.27134081721305847; Learning Rate: 0.00015266205223062526; Time: 3241.7960166931152 ms; Tokens/Sec : 161727.634095502\n",
            "\n",
            "Training\n",
            "Step: 16917; Training Loss: 3.065439462661743; Gradient Norm 0.3024197220802307; Learning Rate: 0.00015262746638524042; Time: 3243.055582046509 ms; Tokens/Sec : 161664.8209492455\n",
            "\n",
            "Training\n",
            "Step: 16918; Training Loss: 3.1200194358825684; Gradient Norm 0.2854454815387726; Learning Rate: 0.00015259289119535921; Time: 3241.3854598999023 ms; Tokens/Sec : 161748.11866287282\n",
            "\n",
            "Training\n",
            "Step: 16919; Training Loss: 3.0150279998779297; Gradient Norm 0.25880002975463867; Learning Rate: 0.00015255832666190888; Time: 3242.4747943878174 ms; Tokens/Sec : 161693.77813127646\n",
            "\n",
            "Training\n",
            "Step: 16920; Training Loss: 3.02584171295166; Gradient Norm 0.3087451457977295; Learning Rate: 0.00015252377278581616; Time: 3241.359233856201 ms; Tokens/Sec : 161749.4273771876\n",
            "\n",
            "Training\n",
            "Step: 16921; Training Loss: 3.060532808303833; Gradient Norm 0.2733776569366455; Learning Rate: 0.00015248922956800765; Time: 3245.427131652832 ms; Tokens/Sec : 161546.68668619604\n",
            "\n",
            "Training\n",
            "Step: 16922; Training Loss: 3.010772705078125; Gradient Norm 0.2686085104942322; Learning Rate: 0.00015245469700940955; Time: 3239.1841411590576 ms; Tokens/Sec : 161858.04114624902\n",
            "\n",
            "Training\n",
            "Step: 16923; Training Loss: 3.087592601776123; Gradient Norm 0.2807718813419342; Learning Rate: 0.000152420175110948; Time: 3242.8371906280518 ms; Tokens/Sec : 161675.70839363022\n",
            "\n",
            "Training\n",
            "Step: 16924; Training Loss: 3.1634445190429688; Gradient Norm 0.3626232147216797; Learning Rate: 0.0001523856638735485; Time: 3239.5918369293213 ms; Tokens/Sec : 161837.67165463397\n",
            "\n",
            "Training\n",
            "Step: 16925; Training Loss: 3.007143020629883; Gradient Norm 0.30691176652908325; Learning Rate: 0.00015235116329813666; Time: 3241.5719032287598 ms; Tokens/Sec : 161738.81550422628\n",
            "\n",
            "Training\n",
            "Step: 16926; Training Loss: 3.013434648513794; Gradient Norm 0.32986652851104736; Learning Rate: 0.00015231667338563745; Time: 3242.9444789886475 ms; Tokens/Sec : 161670.3595750445\n",
            "\n",
            "Training\n",
            "Step: 16927; Training Loss: 3.0430428981781006; Gradient Norm 0.2809004485607147; Learning Rate: 0.00015228219413697584; Time: 3242.809772491455 ms; Tokens/Sec : 161677.07537071742\n",
            "\n",
            "Training\n",
            "Step: 16928; Training Loss: 2.9926629066467285; Gradient Norm 0.28896990418434143; Learning Rate: 0.00015224772555307633; Time: 3240.194320678711 ms; Tokens/Sec : 161807.57945720348\n",
            "\n",
            "Training\n",
            "Step: 16929; Training Loss: 3.0674993991851807; Gradient Norm 0.2872505187988281; Learning Rate: 0.00015221326763486322; Time: 3242.45023727417 ms; Tokens/Sec : 161695.0027398888\n",
            "\n",
            "Training\n",
            "Step: 16930; Training Loss: 3.0685982704162598; Gradient Norm 0.3144879639148712; Learning Rate: 0.0001521788203832604; Time: 3242.4159049987793 ms; Tokens/Sec : 161696.7148451603\n",
            "\n",
            "Training\n",
            "Step: 16931; Training Loss: 3.0135741233825684; Gradient Norm 0.3267577886581421; Learning Rate: 0.00015214438379919165; Time: 3246.600866317749 ms; Tokens/Sec : 161488.28315771391\n",
            "\n",
            "Training\n",
            "Step: 16932; Training Loss: 3.025045871734619; Gradient Norm 0.2992846369743347; Learning Rate: 0.0001521099578835805; Time: 3241.718292236328 ms; Tokens/Sec : 161731.51172809507\n",
            "\n",
            "Training\n",
            "Step: 16933; Training Loss: 3.009535312652588; Gradient Norm 0.30394452810287476; Learning Rate: 0.0001520755426373498; Time: 3240.572690963745 ms; Tokens/Sec : 161788.6867534136\n",
            "\n",
            "Training\n",
            "Step: 16934; Training Loss: 3.0431668758392334; Gradient Norm 0.32730230689048767; Learning Rate: 0.00015204113806142254; Time: 3242.0873641967773 ms; Tokens/Sec : 161713.10057521897\n",
            "\n",
            "Training\n",
            "Step: 16935; Training Loss: 3.029169797897339; Gradient Norm 0.32796385884284973; Learning Rate: 0.00015200674415672143; Time: 3242.3486709594727 ms; Tokens/Sec : 161700.06782301213\n",
            "\n",
            "Training\n",
            "Step: 16936; Training Loss: 2.9584250450134277; Gradient Norm 0.3709476590156555; Learning Rate: 0.0001519723609241685; Time: 3241.1954402923584 ms; Tokens/Sec : 161757.6013721372\n",
            "\n",
            "Training\n",
            "Step: 16937; Training Loss: 3.0620477199554443; Gradient Norm 0.383146196603775; Learning Rate: 0.00015193798836468571; Time: 3243.419408798218 ms; Tokens/Sec : 161646.68638838298\n",
            "\n",
            "Training\n",
            "Step: 16938; Training Loss: 3.075338840484619; Gradient Norm 0.31854626536369324; Learning Rate: 0.0001519036264791949; Time: 3242.9022789001465 ms; Tokens/Sec : 161672.4634014615\n",
            "\n",
            "Training\n",
            "Step: 16939; Training Loss: 3.1167004108428955; Gradient Norm 0.37946388125419617; Learning Rate: 0.00015186927526861764; Time: 3246.0062503814697 ms; Tokens/Sec : 161517.8652038596\n",
            "\n",
            "Training\n",
            "Step: 16940; Training Loss: 3.1257684230804443; Gradient Norm 0.3945859968662262; Learning Rate: 0.00015183493473387474; Time: 3245.957851409912 ms; Tokens/Sec : 161520.27352181132\n",
            "\n",
            "Training\n",
            "Step: 16941; Training Loss: 3.050541400909424; Gradient Norm 0.34215331077575684; Learning Rate: 0.00015180060487588707; Time: 3247.9515075683594 ms; Tokens/Sec : 161421.1292189267\n",
            "\n",
            "Training\n",
            "Step: 16942; Training Loss: 3.11179780960083; Gradient Norm 0.34905895590782166; Learning Rate: 0.0001517662856955754; Time: 3242.0971393585205 ms; Tokens/Sec : 161712.6129983062\n",
            "\n",
            "Training\n",
            "Step: 16943; Training Loss: 3.0324976444244385; Gradient Norm 0.3020963668823242; Learning Rate: 0.0001517319771938598; Time: 3246.9913959503174 ms; Tokens/Sec : 161468.86026673726\n",
            "\n",
            "Training\n",
            "Step: 16944; Training Loss: 3.1212141513824463; Gradient Norm 0.39075949788093567; Learning Rate: 0.00015169767937166034; Time: 3243.1294918060303 ms; Tokens/Sec : 161661.1366658798\n",
            "\n",
            "Training\n",
            "Step: 16945; Training Loss: 3.0670177936553955; Gradient Norm 0.2983627915382385; Learning Rate: 0.00015166339222989663; Time: 3241.2683963775635 ms; Tokens/Sec : 161753.96045139103\n",
            "\n",
            "Training\n",
            "Step: 16946; Training Loss: 3.0555684566497803; Gradient Norm 0.3688569962978363; Learning Rate: 0.0001516291157694882; Time: 3247.08890914917 ms; Tokens/Sec : 161464.01120176856\n",
            "\n",
            "Training\n",
            "Step: 16947; Training Loss: 3.0851480960845947; Gradient Norm 0.3044222295284271; Learning Rate: 0.0001515948499913541; Time: 3244.2755699157715 ms; Tokens/Sec : 161604.02798755214\n",
            "\n",
            "Training\n",
            "Step: 16948; Training Loss: 3.0703978538513184; Gradient Norm 0.29977160692214966; Learning Rate: 0.00015156059489641302; Time: 3241.3759231567383 ms; Tokens/Sec : 161748.59455653696\n",
            "\n",
            "Training\n",
            "Step: 16949; Training Loss: 3.0407872200012207; Gradient Norm 0.28942281007766724; Learning Rate: 0.0001515263504855839; Time: 3244.863748550415 ms; Tokens/Sec : 161574.7349127421\n",
            "\n",
            "Training\n",
            "Step: 16950; Training Loss: 3.0843212604522705; Gradient Norm 0.2855132520198822; Learning Rate: 0.0001514921167597846; Time: 3243.577480316162 ms; Tokens/Sec : 161638.80874795566\n",
            "\n",
            "Training\n",
            "Step: 16951; Training Loss: 3.045022964477539; Gradient Norm 0.29093125462532043; Learning Rate: 0.0001514578937199333; Time: 3243.378162384033 ms; Tokens/Sec : 161648.7420679382\n",
            "\n",
            "Training\n",
            "Step: 16952; Training Loss: 3.1282236576080322; Gradient Norm 0.30923786759376526; Learning Rate: 0.00015142368136694757; Time: 3248.1207847595215 ms; Tokens/Sec : 161412.71668837164\n",
            "\n",
            "Training\n",
            "Step: 16953; Training Loss: 3.037062406539917; Gradient Norm 0.30821600556373596; Learning Rate: 0.00015138947970174492; Time: 3248.4567165374756 ms; Tokens/Sec : 161396.02455865187\n",
            "\n",
            "Training\n",
            "Step: 16954; Training Loss: 3.1025078296661377; Gradient Norm 0.2997370958328247; Learning Rate: 0.00015135528872524243; Time: 3245.8579540252686 ms; Tokens/Sec : 161525.24461208092\n",
            "\n",
            "Training\n",
            "Step: 16955; Training Loss: 3.072599411010742; Gradient Norm 0.3223973214626312; Learning Rate: 0.00015132110843835687; Time: 3242.438554763794 ms; Tokens/Sec : 161695.58532719628\n",
            "\n",
            "Training\n",
            "Step: 16956; Training Loss: 3.0145061016082764; Gradient Norm 0.29710420966148376; Learning Rate: 0.00015128693884200483; Time: 3244.32373046875 ms; Tokens/Sec : 161601.62904712633\n",
            "\n",
            "Training\n",
            "Step: 16957; Training Loss: 3.0600688457489014; Gradient Norm 0.3034311830997467; Learning Rate: 0.00015125277993710264; Time: 3244.4379329681396 ms; Tokens/Sec : 161595.9407552484\n",
            "\n",
            "Training\n",
            "Step: 16958; Training Loss: 3.071385622024536; Gradient Norm 0.33267080783843994; Learning Rate: 0.0001512186317245661; Time: 3244.9557781219482 ms; Tokens/Sec : 161570.1525225213\n",
            "\n",
            "Training\n",
            "Step: 16959; Training Loss: 3.0681025981903076; Gradient Norm 0.3140907883644104; Learning Rate: 0.00015118449420531107; Time: 3241.9538497924805 ms; Tokens/Sec : 161719.7604566641\n",
            "\n",
            "Training\n",
            "Step: 16960; Training Loss: 3.044182538986206; Gradient Norm 0.3088254928588867; Learning Rate: 0.00015115036738025278; Time: 3246.9890117645264 ms; Tokens/Sec : 161468.97882943056\n",
            "\n",
            "Training\n",
            "Step: 16961; Training Loss: 3.0130717754364014; Gradient Norm 0.2975972592830658; Learning Rate: 0.0001511162512503064; Time: 3242.7451610565186 ms; Tokens/Sec : 161680.29677336154\n",
            "\n",
            "Training\n",
            "Step: 16962; Training Loss: 3.010671377182007; Gradient Norm 0.3777759075164795; Learning Rate: 0.00015108214581638693; Time: 3246.3223934173584 ms; Tokens/Sec : 161502.13579005914\n",
            "\n",
            "Training\n",
            "Step: 16963; Training Loss: 2.9924492835998535; Gradient Norm 0.3023920953273773; Learning Rate: 0.0001510480510794085; Time: 3247.1749782562256 ms; Tokens/Sec : 161459.7314621922\n",
            "\n",
            "Training\n",
            "Step: 16964; Training Loss: 2.978961706161499; Gradient Norm 0.29102787375450134; Learning Rate: 0.00015101396704028566; Time: 3245.337963104248 ms; Tokens/Sec : 161551.12532517422\n",
            "\n",
            "Training\n",
            "Step: 16965; Training Loss: 2.9365017414093018; Gradient Norm 0.3358619213104248; Learning Rate: 0.00015097989369993234; Time: 3245.643377304077 ms; Tokens/Sec : 161535.92340619024\n",
            "\n",
            "Training\n",
            "Step: 16966; Training Loss: 2.9716312885284424; Gradient Norm 0.2822306156158447; Learning Rate: 0.00015094583105926227; Time: 3241.6508197784424 ms; Tokens/Sec : 161734.87804458642\n",
            "\n",
            "Training\n",
            "Step: 16967; Training Loss: 2.9767184257507324; Gradient Norm 0.32383084297180176; Learning Rate: 0.00015091177911918856; Time: 3242.9351806640625 ms; Tokens/Sec : 161670.82312531467\n",
            "\n",
            "Training\n",
            "Step: 16968; Training Loss: 3.0167791843414307; Gradient Norm 0.2829993665218353; Learning Rate: 0.00015087773788062457; Time: 3249.79567527771 ms; Tokens/Sec : 161329.52726487865\n",
            "\n",
            "Training\n",
            "Step: 16969; Training Loss: 3.0133111476898193; Gradient Norm 0.2894223928451538; Learning Rate: 0.00015084370734448312; Time: 3244.9283599853516 ms; Tokens/Sec : 161571.51771522214\n",
            "\n",
            "Training\n",
            "Step: 16970; Training Loss: 3.0083184242248535; Gradient Norm 0.3139219582080841; Learning Rate: 0.00015080968751167656; Time: 3245.9142208099365 ms; Tokens/Sec : 161522.44462861284\n",
            "\n",
            "Training\n",
            "Step: 16971; Training Loss: 3.022291898727417; Gradient Norm 0.2752152681350708; Learning Rate: 0.00015077567838311714; Time: 3245.960474014282 ms; Tokens/Sec : 161520.1430199834\n",
            "\n",
            "Training\n",
            "Step: 16972; Training Loss: 3.0869646072387695; Gradient Norm 0.3983522653579712; Learning Rate: 0.00015074167995971703; Time: 3250.2756118774414 ms; Tokens/Sec : 161305.7053020676\n",
            "\n",
            "Training\n",
            "Step: 16973; Training Loss: 3.104684352874756; Gradient Norm 0.30881255865097046; Learning Rate: 0.00015070769224238777; Time: 3244.7891235351562 ms; Tokens/Sec : 161578.45087596786\n",
            "\n",
            "Training\n",
            "Step: 16974; Training Loss: 3.0690085887908936; Gradient Norm 0.32839980721473694; Learning Rate: 0.00015067371523204063; Time: 3247.9476928710938 ms; Tokens/Sec : 161421.3188071832\n",
            "\n",
            "Training\n",
            "Step: 16975; Training Loss: 3.0676872730255127; Gradient Norm 0.32051530480384827; Learning Rate: 0.00015063974892958671; Time: 3245.568037033081 ms; Tokens/Sec : 161539.67318438197\n",
            "\n",
            "Training\n",
            "Step: 16976; Training Loss: 3.1450366973876953; Gradient Norm 0.3411135673522949; Learning Rate: 0.0001506057933359371; Time: 3248.09193611145 ms; Tokens/Sec : 161414.1503111722\n",
            "\n",
            "Training\n",
            "Step: 16977; Training Loss: 3.049171209335327; Gradient Norm 0.3248983919620514; Learning Rate: 0.00015057184845200197; Time: 3246.3252544403076 ms; Tokens/Sec : 161501.99345641088\n",
            "\n",
            "Training\n",
            "Step: 16978; Training Loss: 3.1634132862091064; Gradient Norm 0.30355918407440186; Learning Rate: 0.0001505379142786917; Time: 3241.6129112243652 ms; Tokens/Sec : 161736.76942876412\n",
            "\n",
            "Training\n",
            "Step: 16979; Training Loss: 3.0486011505126953; Gradient Norm 0.32263755798339844; Learning Rate: 0.00015050399081691624; Time: 3245.5804347991943 ms; Tokens/Sec : 161539.0561203078\n",
            "\n",
            "Training\n",
            "Step: 16980; Training Loss: 3.124716281890869; Gradient Norm 0.30151477456092834; Learning Rate: 0.0001504700780675852; Time: 3248.1038570404053 ms; Tokens/Sec : 161413.55790196892\n",
            "\n",
            "Training\n",
            "Step: 16981; Training Loss: 3.0956060886383057; Gradient Norm 0.30818456411361694; Learning Rate: 0.000150436176031608; Time: 3247.10750579834 ms; Tokens/Sec : 161463.0864742797\n",
            "\n",
            "Training\n",
            "Step: 16982; Training Loss: 2.9977149963378906; Gradient Norm 0.42809444665908813; Learning Rate: 0.00015040228470989368; Time: 3242.441415786743 ms; Tokens/Sec : 161695.44265236545\n",
            "\n",
            "Training\n",
            "Step: 16983; Training Loss: 3.1102802753448486; Gradient Norm 0.3661934435367584; Learning Rate: 0.00015036840410335102; Time: 3245.9301948547363 ms; Tokens/Sec : 161521.64973574338\n",
            "\n",
            "Training\n",
            "Step: 16984; Training Loss: 3.015456199645996; Gradient Norm 0.3252468705177307; Learning Rate: 0.0001503345342128886; Time: 3245.4137802124023 ms; Tokens/Sec : 161547.35127971479\n",
            "\n",
            "Training\n",
            "Step: 16985; Training Loss: 3.0708062648773193; Gradient Norm 0.30410128831863403; Learning Rate: 0.00015030067503941452; Time: 3243.004560470581 ms; Tokens/Sec : 161667.3643912244\n",
            "\n",
            "Training\n",
            "Step: 16986; Training Loss: 3.0793979167938232; Gradient Norm 0.2954874634742737; Learning Rate: 0.0001502668265838368; Time: 3245.195150375366 ms; Tokens/Sec : 161558.23477653\n",
            "\n",
            "Training\n",
            "Step: 16987; Training Loss: 2.994997501373291; Gradient Norm 0.2982921898365021; Learning Rate: 0.00015023298884706308; Time: 3250.159978866577 ms; Tokens/Sec : 161311.44417784447\n",
            "\n",
            "Training\n",
            "Step: 16988; Training Loss: 3.0270538330078125; Gradient Norm 0.28672847151756287; Learning Rate: 0.00015019916183000063; Time: 3246.2005615234375 ms; Tokens/Sec : 161508.19706406322\n",
            "\n",
            "Training\n",
            "Step: 16989; Training Loss: 3.1369450092315674; Gradient Norm 0.3901779055595398; Learning Rate: 0.00015016534553355672; Time: 3242.1414852142334 ms; Tokens/Sec : 161710.40110094278\n",
            "\n",
            "Training\n",
            "Step: 16990; Training Loss: 3.0542569160461426; Gradient Norm 0.3221845328807831; Learning Rate: 0.00015013153995863782; Time: 3244.5120811462402 ms; Tokens/Sec : 161592.24773629953\n",
            "\n",
            "Training\n",
            "Step: 16991; Training Loss: 3.0784506797790527; Gradient Norm 0.37338775396347046; Learning Rate: 0.00015009774510615063; Time: 3242.4564361572266 ms; Tokens/Sec : 161694.6936136345\n",
            "\n",
            "Training\n",
            "Step: 16992; Training Loss: 3.040259838104248; Gradient Norm 0.3414764404296875; Learning Rate: 0.00015006396097700135; Time: 3243.983268737793 ms; Tokens/Sec : 161618.58942139245\n",
            "\n",
            "Training\n",
            "Step: 16993; Training Loss: 3.0432751178741455; Gradient Norm 0.3027113974094391; Learning Rate: 0.00015003018757209598; Time: 3250.8673667907715 ms; Tokens/Sec : 161276.34284802355\n",
            "\n",
            "Training\n",
            "Step: 16994; Training Loss: 3.0540428161621094; Gradient Norm 0.322995662689209; Learning Rate: 0.00014999642489233985; Time: 3244.1649436950684 ms; Tokens/Sec : 161609.53869467613\n",
            "\n",
            "Training\n",
            "Step: 16995; Training Loss: 2.9933133125305176; Gradient Norm 0.3018069565296173; Learning Rate: 0.00014996267293863858; Time: 3247.462749481201 ms; Tokens/Sec : 161445.4238416615\n",
            "\n",
            "Training\n",
            "Step: 16996; Training Loss: 3.037078619003296; Gradient Norm 0.30131882429122925; Learning Rate: 0.00014992893171189724; Time: 3244.44842338562 ms; Tokens/Sec : 161595.41826000097\n",
            "\n",
            "Training\n",
            "Step: 16997; Training Loss: 2.9764206409454346; Gradient Norm 0.2795192003250122; Learning Rate: 0.00014989520121302038; Time: 3245.774030685425 ms; Tokens/Sec : 161529.42103898828\n",
            "\n",
            "Training\n",
            "Step: 16998; Training Loss: 2.956427574157715; Gradient Norm 0.28818583488464355; Learning Rate: 0.0001498614814429125; Time: 3246.410131454468 ms; Tokens/Sec : 161497.77100563282\n",
            "\n",
            "Training\n",
            "Step: 16999; Training Loss: 2.9908852577209473; Gradient Norm 0.32113194465637207; Learning Rate: 0.00014982777240247797; Time: 3244.501829147339 ms; Tokens/Sec : 161592.7583365807\n",
            "\n",
            "Evaluating HellaSwag\n",
            "Evaluating Validation set\n",
            "Training\n",
            "Step: 17000; Training Loss: 3.020967960357666; Gradient Norm 0.29052355885505676; Learning Rate: 0.0001497940740926207; Time: 3234.2426776885986 ms; Tokens/Sec : 162105.33724534564; Validation_loss: 2.9253153800964355; HellaSwag Acc: 0.335\n",
            "\n",
            "Checkpointing\n",
            "Training\n",
            "Step: 17001; Training Loss: 3.0095789432525635; Gradient Norm 0.3352819085121155; Learning Rate: 0.00014976038651424417; Time: 3270.0278759002686 ms; Tokens/Sec : 160331.35492940064\n",
            "\n",
            "Training\n",
            "Step: 17002; Training Loss: 3.0102808475494385; Gradient Norm 0.29197436571121216; Learning Rate: 0.0001497267096682516; Time: 3233.1020832061768 ms; Tokens/Sec : 162162.52580558122\n",
            "\n",
            "Training\n",
            "Step: 17003; Training Loss: 3.01350474357605; Gradient Norm 0.30048713088035583; Learning Rate: 0.00014969304355554638; Time: 3233.7639331817627 ms; Tokens/Sec : 162129.3362265139\n",
            "\n",
            "Training\n",
            "Step: 17004; Training Loss: 3.076423406600952; Gradient Norm 0.34302181005477905; Learning Rate: 0.000149659388177031; Time: 3237.879514694214 ms; Tokens/Sec : 161923.2579905043\n",
            "\n",
            "Training\n",
            "Step: 17005; Training Loss: 3.0235588550567627; Gradient Norm 0.35709014534950256; Learning Rate: 0.00014962574353360797; Time: 3235.617160797119 ms; Tokens/Sec : 162036.47525186127\n",
            "\n",
            "Training\n",
            "Step: 17006; Training Loss: 3.0309083461761475; Gradient Norm 0.29795166850090027; Learning Rate: 0.0001495921096261796; Time: 3239.2053604125977 ms; Tokens/Sec : 161856.9808532356\n",
            "\n",
            "Training\n",
            "Step: 17007; Training Loss: 3.0383713245391846; Gradient Norm 0.2948514521121979; Learning Rate: 0.00014955848645564754; Time: 3238.8854026794434 ms; Tokens/Sec : 161872.9701169021\n",
            "\n",
            "Training\n",
            "Step: 17008; Training Loss: 3.0959839820861816; Gradient Norm 0.2728225588798523; Learning Rate: 0.00014952487402291361; Time: 3238.917112350464 ms; Tokens/Sec : 161871.38534691528\n",
            "\n",
            "Training\n",
            "Step: 17009; Training Loss: 3.095027446746826; Gradient Norm 0.3076009154319763; Learning Rate: 0.00014949127232887887; Time: 3241.990327835083 ms; Tokens/Sec : 161717.94082745025\n",
            "\n",
            "Training\n",
            "Step: 17010; Training Loss: 3.1060867309570312; Gradient Norm 0.3304319977760315; Learning Rate: 0.0001494576813744448; Time: 3241.196393966675 ms; Tokens/Sec : 161757.55377734467\n",
            "\n",
            "Training\n",
            "Step: 17011; Training Loss: 2.9978208541870117; Gradient Norm 0.34863385558128357; Learning Rate: 0.00014942410116051166; Time: 3238.133192062378 ms; Tokens/Sec : 161910.5728217681\n",
            "\n",
            "Training\n",
            "Step: 17012; Training Loss: 3.074286937713623; Gradient Norm 0.3300352394580841; Learning Rate: 0.00014939053168798016; Time: 3237.5893592834473 ms; Tokens/Sec : 161937.76968554064\n",
            "\n",
            "Training\n",
            "Step: 17013; Training Loss: 3.1253364086151123; Gradient Norm 0.31020402908325195; Learning Rate: 0.00014935697295775038; Time: 3240.558385848999 ms; Tokens/Sec : 161789.40095308327\n",
            "\n",
            "Training\n",
            "Step: 17014; Training Loss: 3.102144241333008; Gradient Norm 0.3194272518157959; Learning Rate: 0.0001493234249707222; Time: 3240.579128265381 ms; Tokens/Sec : 161788.36536561945\n",
            "\n",
            "Training\n",
            "Step: 17015; Training Loss: 3.0199332237243652; Gradient Norm 0.29885220527648926; Learning Rate: 0.00014928988772779518; Time: 3240.528106689453 ms; Tokens/Sec : 161790.9126965161\n",
            "\n",
            "Training\n",
            "Step: 17016; Training Loss: 3.0380945205688477; Gradient Norm 0.3312619924545288; Learning Rate: 0.00014925636122986873; Time: 3241.0027980804443 ms; Tokens/Sec : 161767.21609451284\n",
            "\n",
            "Training\n",
            "Step: 17017; Training Loss: 3.132934808731079; Gradient Norm 0.37297165393829346; Learning Rate: 0.00014922284547784172; Time: 3236.569881439209 ms; Tokens/Sec : 161988.777998164\n",
            "\n",
            "Training\n",
            "Step: 17018; Training Loss: 3.0440280437469482; Gradient Norm 0.29086342453956604; Learning Rate: 0.00014918934047261298; Time: 3239.917516708374 ms; Tokens/Sec : 161821.40356852527\n",
            "\n",
            "Training\n",
            "Step: 17019; Training Loss: 3.0788676738739014; Gradient Norm 0.3322645127773285; Learning Rate: 0.00014915584621508085; Time: 3237.2007369995117 ms; Tokens/Sec : 161957.21013147634\n",
            "\n",
            "Training\n",
            "Step: 17020; Training Loss: 3.046124219894409; Gradient Norm 0.3115365505218506; Learning Rate: 0.00014912236270614353; Time: 3238.980293273926 ms; Tokens/Sec : 161868.22781501256\n",
            "\n",
            "Training\n",
            "Step: 17021; Training Loss: 3.0336031913757324; Gradient Norm 0.29048892855644226; Learning Rate: 0.00014908888994669887; Time: 3245.554208755493 ms; Tokens/Sec : 161540.3614537186\n",
            "\n",
            "Training\n",
            "Step: 17022; Training Loss: 3.052940845489502; Gradient Norm 0.3855174779891968; Learning Rate: 0.00014905542793764446; Time: 3236.3390922546387 ms; Tokens/Sec : 162000.3297104284\n",
            "\n",
            "Training\n",
            "Step: 17023; Training Loss: 3.0601744651794434; Gradient Norm 0.2971958518028259; Learning Rate: 0.00014902197667987762; Time: 3241.511583328247 ms; Tokens/Sec : 161741.82523256118\n",
            "\n",
            "Training\n",
            "Step: 17024; Training Loss: 3.0322091579437256; Gradient Norm 0.3221467137336731; Learning Rate: 0.0001489885361742951; Time: 3237.8649711608887 ms; Tokens/Sec : 161923.98530196404\n",
            "\n",
            "Training\n",
            "Step: 17025; Training Loss: 3.0488646030426025; Gradient Norm 0.3020201623439789; Learning Rate: 0.0001489551064217939; Time: 3237.73455619812 ms; Tokens/Sec : 161930.50755082292\n",
            "\n",
            "Training\n",
            "Step: 17026; Training Loss: 2.991839647293091; Gradient Norm 0.3033328354358673; Learning Rate: 0.00014892168742327032; Time: 3237.812280654907 ms; Tokens/Sec : 161926.62037033014\n",
            "\n",
            "Training\n",
            "Step: 17027; Training Loss: 3.0312273502349854; Gradient Norm 0.31902459263801575; Learning Rate: 0.00014888827917962057; Time: 3240.5519485473633 ms; Tokens/Sec : 161789.72234499178\n",
            "\n",
            "Training\n",
            "Step: 17028; Training Loss: 2.9926087856292725; Gradient Norm 0.2645484209060669; Learning Rate: 0.00014885488169174017; Time: 3236.8407249450684 ms; Tokens/Sec : 161975.2235442161\n",
            "\n",
            "Training\n",
            "Step: 17029; Training Loss: 3.0666379928588867; Gradient Norm 0.31356754899024963; Learning Rate: 0.00014882149496052506; Time: 3239.2749786376953 ms; Tokens/Sec : 161853.5022366313\n",
            "\n",
            "Training\n",
            "Step: 17030; Training Loss: 2.983597755432129; Gradient Norm 0.2890448570251465; Learning Rate: 0.0001487881189868704; Time: 3240.135669708252 ms; Tokens/Sec : 161810.50839985596\n",
            "\n",
            "Training\n",
            "Step: 17031; Training Loss: 3.0019044876098633; Gradient Norm 0.27880945801734924; Learning Rate: 0.00014875475377167102; Time: 3239.2354011535645 ms; Tokens/Sec : 161855.47978800468\n",
            "\n",
            "Training\n",
            "Step: 17032; Training Loss: 3.005504608154297; Gradient Norm 0.3199898302555084; Learning Rate: 0.00014872139931582166; Time: 3238.4538650512695 ms; Tokens/Sec : 161894.5403724934\n",
            "\n",
            "Training\n",
            "Step: 17033; Training Loss: 2.992116689682007; Gradient Norm 0.2938253581523895; Learning Rate: 0.00014868805562021686; Time: 3236.531972885132 ms; Tokens/Sec : 161990.6753254273\n",
            "\n",
            "Training\n",
            "Step: 17034; Training Loss: 2.9297657012939453; Gradient Norm 0.3415202796459198; Learning Rate: 0.00014865472268575053; Time: 3240.453004837036 ms; Tokens/Sec : 161794.662418308\n",
            "\n",
            "Training\n",
            "Step: 17035; Training Loss: 3.0085527896881104; Gradient Norm 0.284351110458374; Learning Rate: 0.00014862140051331664; Time: 3233.7284088134766 ms; Tokens/Sec : 162131.1173106131\n",
            "\n",
            "Training\n",
            "Step: 17036; Training Loss: 3.02034592628479; Gradient Norm 0.3144378364086151; Learning Rate: 0.0001485880891038085; Time: 3237.100601196289 ms; Tokens/Sec : 161962.22008245476\n",
            "\n",
            "Training\n",
            "Step: 17037; Training Loss: 2.9883978366851807; Gradient Norm 0.2927308976650238; Learning Rate: 0.00014855478845811974; Time: 3235.4683876037598 ms; Tokens/Sec : 162043.92600735504\n",
            "\n",
            "Training\n",
            "Step: 17038; Training Loss: 2.984884262084961; Gradient Norm 0.3042828440666199; Learning Rate: 0.000148521498577143; Time: 3238.9886379241943 ms; Tokens/Sec : 161867.8107917063\n",
            "\n",
            "Training\n",
            "Step: 17039; Training Loss: 3.066981315612793; Gradient Norm 0.27849966287612915; Learning Rate: 0.00014848821946177097; Time: 3238.8200759887695 ms; Tokens/Sec : 161876.23507920295\n",
            "\n",
            "Training\n",
            "Step: 17040; Training Loss: 2.9807145595550537; Gradient Norm 0.258781760931015; Learning Rate: 0.00014845495111289614; Time: 3236.8693351745605 ms; Tokens/Sec : 161973.79186816193\n",
            "\n",
            "Training\n",
            "Step: 17041; Training Loss: 3.0333151817321777; Gradient Norm 0.3157225251197815; Learning Rate: 0.00014842169353141053; Time: 3237.156391143799 ms; Tokens/Sec : 161959.42878581502\n",
            "\n",
            "Training\n",
            "Step: 17042; Training Loss: 3.0757334232330322; Gradient Norm 0.3380852937698364; Learning Rate: 0.00014838844671820594; Time: 3240.266799926758 ms; Tokens/Sec : 161803.96009731386\n",
            "\n",
            "Training\n",
            "Step: 17043; Training Loss: 3.0937747955322266; Gradient Norm 0.313751757144928; Learning Rate: 0.0001483552106741739; Time: 3234.724283218384 ms; Tokens/Sec : 162081.2020115546\n",
            "\n",
            "Training\n",
            "Step: 17044; Training Loss: 3.0619678497314453; Gradient Norm 0.3468036651611328; Learning Rate: 0.00014832198540020558; Time: 3241.2869930267334 ms; Tokens/Sec : 161753.03239976807\n",
            "\n",
            "Training\n",
            "Step: 17045; Training Loss: 3.087337017059326; Gradient Norm 0.30669930577278137; Learning Rate: 0.00014828877089719198; Time: 3236.4423274993896 ms; Tokens/Sec : 161995.16226358552\n",
            "\n",
            "Training\n",
            "Step: 17046; Training Loss: 3.127500295639038; Gradient Norm 0.31380218267440796; Learning Rate: 0.00014825556716602372; Time: 3236.414909362793 ms; Tokens/Sec : 161996.53464803292\n",
            "\n",
            "Training\n",
            "Step: 17047; Training Loss: 3.069272756576538; Gradient Norm 0.32634612917900085; Learning Rate: 0.00014822237420759117; Time: 8151.13091468811 ms; Tokens/Sec : 64320.89062086436\n",
            "\n",
            "Training\n",
            "Step: 17048; Training Loss: 3.0717389583587646; Gradient Norm 0.31087762117385864; Learning Rate: 0.00014818919202278438; Time: 3235.133171081543 ms; Tokens/Sec : 162060.71659941107\n",
            "\n",
            "Training\n",
            "Step: 17049; Training Loss: 3.0577621459960938; Gradient Norm 0.29028502106666565; Learning Rate: 0.00014815602061249309; Time: 3234.330654144287 ms; Tokens/Sec : 162100.92784675778\n",
            "\n",
            "Training\n",
            "Step: 17050; Training Loss: 3.132596492767334; Gradient Norm 0.34697288274765015; Learning Rate: 0.00014812285997760694; Time: 3236.1488342285156 ms; Tokens/Sec : 162009.85395190827\n",
            "\n",
            "Training\n",
            "Step: 17051; Training Loss: 3.0672810077667236; Gradient Norm 0.29942139983177185; Learning Rate: 0.00014808971011901484; Time: 3237.1435165405273 ms; Tokens/Sec : 161960.07292265387\n",
            "\n",
            "Training\n",
            "Step: 17052; Training Loss: 3.0573902130126953; Gradient Norm 0.290010005235672; Learning Rate: 0.00014805657103760593; Time: 3237.065315246582 ms; Tokens/Sec : 161963.98556760742\n",
            "\n",
            "Training\n",
            "Step: 17053; Training Loss: 3.042405605316162; Gradient Norm 0.28441354632377625; Learning Rate: 0.00014802344273426882; Time: 3233.8902950286865 ms; Tokens/Sec : 162123.00114384346\n",
            "\n",
            "Training\n",
            "Step: 17054; Training Loss: 3.0867698192596436; Gradient Norm 0.3076222240924835; Learning Rate: 0.0001479903252098918; Time: 3233.940601348877 ms; Tokens/Sec : 162120.47920154114\n",
            "\n",
            "Training\n",
            "Step: 17055; Training Loss: 3.0593338012695312; Gradient Norm 0.3019368052482605; Learning Rate: 0.0001479572184653628; Time: 3239.6106719970703 ms; Tokens/Sec : 161836.7307318446\n",
            "\n",
            "Training\n",
            "Step: 17056; Training Loss: 3.055529832839966; Gradient Norm 0.2801457941532135; Learning Rate: 0.00014792412250156975; Time: 3238.051176071167 ms; Tokens/Sec : 161914.6738243142\n",
            "\n",
            "Training\n",
            "Step: 17057; Training Loss: 3.0575711727142334; Gradient Norm 0.28623756766319275; Learning Rate: 0.0001478910373194001; Time: 3235.7659339904785 ms; Tokens/Sec : 162029.02518150522\n",
            "\n",
            "Training\n",
            "Step: 17058; Training Loss: 3.0852177143096924; Gradient Norm 0.29563233256340027; Learning Rate: 0.00014785796291974098; Time: 3240.9491539001465 ms; Tokens/Sec : 161769.89366496963\n",
            "\n",
            "Training\n",
            "Step: 17059; Training Loss: 3.0326435565948486; Gradient Norm 0.294411838054657; Learning Rate: 0.00014782489930347908; Time: 3240.6039237976074 ms; Tokens/Sec : 161787.1274393805\n",
            "\n",
            "Training\n",
            "Step: 17060; Training Loss: 3.04899525642395; Gradient Norm 0.2805253863334656; Learning Rate: 0.00014779184647150133; Time: 3237.9720211029053 ms; Tokens/Sec : 161918.63196563974\n",
            "\n",
            "Training\n",
            "Step: 17061; Training Loss: 3.076533079147339; Gradient Norm 0.30117174983024597; Learning Rate: 0.000147758804424694; Time: 3240.962028503418 ms; Tokens/Sec : 161769.25103997623\n",
            "\n",
            "Training\n",
            "Step: 17062; Training Loss: 3.012758255004883; Gradient Norm 0.265598326921463; Learning Rate: 0.00014772577316394278; Time: 3234.8904609680176 ms; Tokens/Sec : 162072.87582872607\n",
            "\n",
            "Training\n",
            "Step: 17063; Training Loss: 3.0291666984558105; Gradient Norm 0.2801092863082886; Learning Rate: 0.0001476927526901337; Time: 3240.0825023651123 ms; Tokens/Sec : 161813.16358990665\n",
            "\n",
            "Training\n",
            "Step: 17064; Training Loss: 3.028261184692383; Gradient Norm 0.26783832907676697; Learning Rate: 0.00014765974300415223; Time: 3238.499164581299 ms; Tokens/Sec : 161892.2758214713\n",
            "\n",
            "Training\n",
            "Step: 17065; Training Loss: 3.0167958736419678; Gradient Norm 0.2514769732952118; Learning Rate: 0.00014762674410688335; Time: 3239.43829536438 ms; Tokens/Sec : 161845.3423700811\n",
            "\n",
            "Training\n",
            "Step: 17066; Training Loss: 2.9744858741760254; Gradient Norm 0.27843159437179565; Learning Rate: 0.00014759375599921186; Time: 3239.0363216400146 ms; Tokens/Sec : 161865.42784260545\n",
            "\n",
            "Training\n",
            "Step: 17067; Training Loss: 2.9949939250946045; Gradient Norm 0.2841782569885254; Learning Rate: 0.0001475607786820227; Time: 3239.0575408935547 ms; Tokens/Sec : 161864.36745281325\n",
            "\n",
            "Training\n",
            "Step: 17068; Training Loss: 2.9686715602874756; Gradient Norm 0.2950287461280823; Learning Rate: 0.0001475278121561998; Time: 3236.6855144500732 ms; Tokens/Sec : 161982.99082791142\n",
            "\n",
            "Training\n",
            "Step: 17069; Training Loss: 2.989219903945923; Gradient Norm 0.2923840582370758; Learning Rate: 0.00014749485642262723; Time: 3242.661237716675 ms; Tokens/Sec : 161684.48122233644\n",
            "\n",
            "Training\n",
            "Step: 17070; Training Loss: 3.01092529296875; Gradient Norm 0.29143020510673523; Learning Rate: 0.00014746191148218865; Time: 3241.2238121032715 ms; Tokens/Sec : 161756.18543903725\n",
            "\n",
            "Training\n",
            "Step: 17071; Training Loss: 3.010066270828247; Gradient Norm 0.31068184971809387; Learning Rate: 0.00014742897733576777; Time: 3242.2268390655518 ms; Tokens/Sec : 161706.14396342053\n",
            "\n",
            "Training\n",
            "Step: 17072; Training Loss: 3.0049479007720947; Gradient Norm 0.2913963794708252; Learning Rate: 0.00014739605398424743; Time: 3240.8652305603027 ms; Tokens/Sec : 161774.0827530053\n",
            "\n",
            "Training\n",
            "Step: 17073; Training Loss: 3.0416805744171143; Gradient Norm 0.35297101736068726; Learning Rate: 0.0001473631414285105; Time: 3240.4675483703613 ms; Tokens/Sec : 161793.93626813687\n",
            "\n",
            "Training\n",
            "Step: 17074; Training Loss: 2.984107255935669; Gradient Norm 0.27650004625320435; Learning Rate: 0.00014733023966943955; Time: 3240.648031234741 ms; Tokens/Sec : 161784.92540587243\n",
            "\n",
            "Training\n",
            "Step: 17075; Training Loss: 2.9637327194213867; Gradient Norm 0.32710009813308716; Learning Rate: 0.00014729734870791688; Time: 3240.5707836151123 ms; Tokens/Sec : 161788.7819796719\n",
            "\n",
            "Training\n",
            "Step: 17076; Training Loss: 3.095237970352173; Gradient Norm 0.28714799880981445; Learning Rate: 0.00014726446854482442; Time: 3238.2235527038574 ms; Tokens/Sec : 161906.05480657108\n",
            "\n",
            "Training\n",
            "Step: 17077; Training Loss: 3.0273170471191406; Gradient Norm 0.3275482952594757; Learning Rate: 0.0001472315991810439; Time: 3239.9346828460693 ms; Tokens/Sec : 161820.54619059403\n",
            "\n",
            "Training\n",
            "Step: 17078; Training Loss: 3.1224048137664795; Gradient Norm 0.32723674178123474; Learning Rate: 0.00014719874061745663; Time: 3240.2710914611816 ms; Tokens/Sec : 161803.74579818733\n",
            "\n",
            "Training\n",
            "Step: 17079; Training Loss: 3.078325033187866; Gradient Norm 0.2952362596988678; Learning Rate: 0.00014716589285494372; Time: 3237.9584312438965 ms; Tokens/Sec : 161919.3115455127\n",
            "\n",
            "Training\n",
            "Step: 17080; Training Loss: 3.1155333518981934; Gradient Norm 0.29400870203971863; Learning Rate: 0.00014713305589438605; Time: 3236.704111099243 ms; Tokens/Sec : 161982.0601463451\n",
            "\n",
            "Training\n",
            "Step: 17081; Training Loss: 3.0787060260772705; Gradient Norm 0.3056148588657379; Learning Rate: 0.0001471002297366641; Time: 3243.7832355499268 ms; Tokens/Sec : 161628.55589551013\n",
            "\n",
            "Training\n",
            "Step: 17082; Training Loss: 3.039818286895752; Gradient Norm 0.33201029896736145; Learning Rate: 0.0001470674143826581; Time: 3239.974021911621 ms; Tokens/Sec : 161818.58140043486\n",
            "\n",
            "Training\n",
            "Step: 17083; Training Loss: 3.0691897869110107; Gradient Norm 0.2860501706600189; Learning Rate: 0.00014703460983324793; Time: 3239.207983016968 ms; Tokens/Sec : 161856.8498067491\n",
            "\n",
            "Training\n",
            "Step: 17084; Training Loss: 2.975825071334839; Gradient Norm 0.3277547061443329; Learning Rate: 0.00014700181608931343; Time: 3237.1175289154053 ms; Tokens/Sec : 161961.37314040077\n",
            "\n",
            "Training\n",
            "Step: 17085; Training Loss: 3.0729403495788574; Gradient Norm 0.37327954173088074; Learning Rate: 0.00014696903315173366; Time: 3240.720748901367 ms; Tokens/Sec : 161781.29515717708\n",
            "\n",
            "Training\n",
            "Step: 17086; Training Loss: 3.063293933868408; Gradient Norm 0.3600165545940399; Learning Rate: 0.00014693626102138795; Time: 3240.2682304382324 ms; Tokens/Sec : 161803.8886642086\n",
            "\n",
            "Training\n",
            "Step: 17087; Training Loss: 3.0740766525268555; Gradient Norm 0.421708881855011; Learning Rate: 0.00014690349969915498; Time: 3239.7565841674805 ms; Tokens/Sec : 161829.4419284979\n",
            "\n",
            "Training\n",
            "Step: 17088; Training Loss: 3.026735544204712; Gradient Norm 0.41000208258628845; Learning Rate: 0.00014687074918591332; Time: 3239.0661239624023 ms; Tokens/Sec : 161863.938535046\n",
            "\n",
            "Training\n",
            "Step: 17089; Training Loss: 3.0514962673187256; Gradient Norm 0.3093288540840149; Learning Rate: 0.00014683800948254094; Time: 3238.0499839782715 ms; Tokens/Sec : 161914.73343344108\n",
            "\n",
            "Training\n",
            "Step: 17090; Training Loss: 3.0328805446624756; Gradient Norm 0.33123135566711426; Learning Rate: 0.00014680528058991604; Time: 3236.6814613342285 ms; Tokens/Sec : 161983.19367018508\n",
            "\n",
            "Training\n",
            "Step: 17091; Training Loss: 3.035248279571533; Gradient Norm 0.2944697141647339; Learning Rate: 0.00014677256250891617; Time: 3248.436450958252 ms; Tokens/Sec : 161397.03143810647\n",
            "\n",
            "Training\n",
            "Step: 17092; Training Loss: 3.099801778793335; Gradient Norm 0.32146963477134705; Learning Rate: 0.00014673985524041848; Time: 3241.4259910583496 ms; Tokens/Sec : 161746.09614604097\n",
            "\n",
            "Training\n",
            "Step: 17093; Training Loss: 3.0590803623199463; Gradient Norm 0.36121174693107605; Learning Rate: 0.00014670715878530008; Time: 3240.3037548065186 ms; Tokens/Sec : 161802.11476232595\n",
            "\n",
            "Training\n",
            "Step: 17094; Training Loss: 3.0888750553131104; Gradient Norm 0.31184151768684387; Learning Rate: 0.000146674473144438; Time: 3241.7399883270264 ms; Tokens/Sec : 161730.4293027433\n",
            "\n",
            "Training\n",
            "Step: 17095; Training Loss: 3.0488648414611816; Gradient Norm 0.3213096857070923; Learning Rate: 0.0001466417983187084; Time: 3236.3264560699463 ms; Tokens/Sec : 162000.96223811503\n",
            "\n",
            "Training\n",
            "Step: 17096; Training Loss: 3.0561389923095703; Gradient Norm 0.28410494327545166; Learning Rate: 0.00014660913430898748; Time: 3238.764524459839 ms; Tokens/Sec : 161879.0115923728\n",
            "\n",
            "Training\n",
            "Step: 17097; Training Loss: 3.0611629486083984; Gradient Norm 0.3156834542751312; Learning Rate: 0.00014657648111615114; Time: 3238.2426261901855 ms; Tokens/Sec : 161905.10116804572\n",
            "\n",
            "Training\n",
            "Step: 17098; Training Loss: 3.022078275680542; Gradient Norm 0.3011815845966339; Learning Rate: 0.00014654383874107514; Time: 3242.158889770508 ms; Tokens/Sec : 161709.53300722133\n",
            "\n",
            "Training\n",
            "Step: 17099; Training Loss: 2.9585397243499756; Gradient Norm 0.30087506771087646; Learning Rate: 0.0001465112071846346; Time: 3240.21577835083 ms; Tokens/Sec : 161806.507919311\n",
            "\n",
            "Training\n",
            "Step: 17100; Training Loss: 3.0799851417541504; Gradient Norm 0.33020567893981934; Learning Rate: 0.0001464785864477046; Time: 3238.557815551758 ms; Tokens/Sec : 161889.34391794278\n",
            "\n",
            "Training\n",
            "Step: 17101; Training Loss: 2.95603084564209; Gradient Norm 0.28143519163131714; Learning Rate: 0.00014644597653115985; Time: 3243.93630027771 ms; Tokens/Sec : 161620.92947235625\n",
            "\n",
            "Training\n",
            "Step: 17102; Training Loss: 2.980334758758545; Gradient Norm 0.30428171157836914; Learning Rate: 0.00014641337743587476; Time: 3241.6133880615234 ms; Tokens/Sec : 161736.74563749347\n",
            "\n",
            "Training\n",
            "Step: 17103; Training Loss: 2.978178024291992; Gradient Norm 0.3229525089263916; Learning Rate: 0.00014638078916272347; Time: 3238.07430267334 ms; Tokens/Sec : 161913.51741593765\n",
            "\n",
            "Training\n",
            "Step: 17104; Training Loss: 3.007303237915039; Gradient Norm 0.27223238348960876; Learning Rate: 0.00014634821171257984; Time: 3238.8672828674316 ms; Tokens/Sec : 161873.87571368398\n",
            "\n",
            "Training\n",
            "Step: 17105; Training Loss: 2.9419631958007812; Gradient Norm 0.2935760021209717; Learning Rate: 0.00014631564508631748; Time: 3238.119602203369 ms; Tokens/Sec : 161911.25233399338\n",
            "\n",
            "Training\n",
            "Step: 17106; Training Loss: 2.9769887924194336; Gradient Norm 0.3605837821960449; Learning Rate: 0.00014628308928480964; Time: 3236.5736961364746 ms; Tokens/Sec : 161988.58707461134\n",
            "\n",
            "Training\n",
            "Step: 17107; Training Loss: 3.0052170753479004; Gradient Norm 0.393609881401062; Learning Rate: 0.00014625054430892926; Time: 3243.0009841918945 ms; Tokens/Sec : 161667.54267286923\n",
            "\n",
            "Training\n",
            "Step: 17108; Training Loss: 3.027090072631836; Gradient Norm 0.2953401505947113; Learning Rate: 0.00014621801015954903; Time: 3236.475944519043 ms; Tokens/Sec : 161993.47963264777\n",
            "\n",
            "Training\n",
            "Step: 17109; Training Loss: 3.0198471546173096; Gradient Norm 0.3147828280925751; Learning Rate: 0.0001461854868375414; Time: 3242.522716522217 ms; Tokens/Sec : 161691.3884144897\n",
            "\n",
            "Training\n",
            "Step: 17110; Training Loss: 2.972538709640503; Gradient Norm 0.3856988549232483; Learning Rate: 0.00014615297434377846; Time: 3243.2539463043213 ms; Tokens/Sec : 161654.93318752444\n",
            "\n",
            "Training\n",
            "Step: 17111; Training Loss: 2.9934959411621094; Gradient Norm 0.3943054676055908; Learning Rate: 0.00014612047267913213; Time: 3240.7116889953613 ms; Tokens/Sec : 161781.7474415727\n",
            "\n",
            "Training\n",
            "Step: 17112; Training Loss: 3.140998601913452; Gradient Norm 0.3692241609096527; Learning Rate: 0.0001460879818444736; Time: 3243.6845302581787 ms; Tokens/Sec : 161633.47425104553\n",
            "\n",
            "Training\n",
            "Step: 17113; Training Loss: 3.1277213096618652; Gradient Norm 0.3682495951652527; Learning Rate: 0.00014605550184067443; Time: 3243.0260181427 ms; Tokens/Sec : 161666.29470961285\n",
            "\n",
            "Training\n",
            "Step: 17114; Training Loss: 3.118119239807129; Gradient Norm 0.28405144810676575; Learning Rate: 0.0001460230326686055; Time: 3242.2280311584473 ms; Tokens/Sec : 161706.08450777968\n",
            "\n",
            "Training\n",
            "Step: 17115; Training Loss: 3.030860424041748; Gradient Norm 0.30049657821655273; Learning Rate: 0.00014599057432913753; Time: 3241.771459579468 ms; Tokens/Sec : 161728.85921699496\n",
            "\n",
            "Training\n",
            "Step: 17116; Training Loss: 3.1075639724731445; Gradient Norm 0.4513646066188812; Learning Rate: 0.00014595812682314065; Time: 3239.1767501831055 ms; Tokens/Sec : 161858.410465055\n",
            "\n",
            "Training\n",
            "Step: 17117; Training Loss: 3.0617313385009766; Gradient Norm 0.35437312722206116; Learning Rate: 0.00014592569015148518; Time: 3243.6954975128174 ms; Tokens/Sec : 161632.92775231542\n",
            "\n",
            "Training\n",
            "Step: 17118; Training Loss: 3.0402629375457764; Gradient Norm 0.4045696258544922; Learning Rate: 0.00014589326431504085; Time: 3238.3713722229004 ms; Tokens/Sec : 161898.6644018272\n",
            "\n",
            "Training\n",
            "Step: 17119; Training Loss: 3.1174354553222656; Gradient Norm 0.3720080256462097; Learning Rate: 0.00014586084931467695; Time: 3239.4754886627197 ms; Tokens/Sec : 161843.48417972753\n",
            "\n",
            "Training\n",
            "Step: 17120; Training Loss: 3.07596492767334; Gradient Norm 0.3801569938659668; Learning Rate: 0.00014582844515126297; Time: 3244.600772857666 ms; Tokens/Sec : 161587.83058485066\n",
            "\n",
            "Training\n",
            "Step: 17121; Training Loss: 3.131683588027954; Gradient Norm 0.42744171619415283; Learning Rate: 0.0001457960518256677; Time: 3239.0942573547363 ms; Tokens/Sec : 161862.53265385647\n",
            "\n",
            "Training\n",
            "Step: 17122; Training Loss: 3.0389838218688965; Gradient Norm 0.3022812008857727; Learning Rate: 0.0001457636693387599; Time: 3239.5198345184326 ms; Tokens/Sec : 161841.2687008405\n",
            "\n",
            "Training\n",
            "Step: 17123; Training Loss: 3.1231839656829834; Gradient Norm 0.38982656598091125; Learning Rate: 0.00014573129769140757; Time: 3243.4182167053223 ms; Tokens/Sec : 161646.74580035315\n",
            "\n",
            "Training\n",
            "Step: 17124; Training Loss: 3.1565375328063965; Gradient Norm 0.4234842360019684; Learning Rate: 0.000145698936884479; Time: 3237.2803688049316 ms; Tokens/Sec : 161953.22624884208\n",
            "\n",
            "Training\n",
            "Step: 17125; Training Loss: 3.044637441635132; Gradient Norm 0.3282585144042969; Learning Rate: 0.00014566658691884208; Time: 3242.3393726348877 ms; Tokens/Sec : 161700.53154366047\n",
            "\n",
            "Training\n",
            "Step: 17126; Training Loss: 3.0407886505126953; Gradient Norm 0.3326539993286133; Learning Rate: 0.00014563424779536393; Time: 3238.3956909179688 ms; Tokens/Sec : 161897.44862567526\n",
            "\n",
            "Training\n",
            "Step: 17127; Training Loss: 3.029196262359619; Gradient Norm 0.35977959632873535; Learning Rate: 0.00014560191951491183; Time: 3237.446069717407 ms; Tokens/Sec : 161944.93706138074\n",
            "\n",
            "Training\n",
            "Step: 17128; Training Loss: 3.0136637687683105; Gradient Norm 0.35985830426216125; Learning Rate: 0.00014556960207835285; Time: 3239.3195629119873 ms; Tokens/Sec : 161851.27457097537\n",
            "\n",
            "Training\n",
            "Step: 17129; Training Loss: 3.05079984664917; Gradient Norm 0.29800310730934143; Learning Rate: 0.00014553729548655338; Time: 3240.6675815582275 ms; Tokens/Sec : 161783.9493885713\n",
            "\n",
            "Training\n",
            "Step: 17130; Training Loss: 3.0396409034729004; Gradient Norm 0.30888622999191284; Learning Rate: 0.00014550499974037978; Time: 3241.8675422668457 ms; Tokens/Sec : 161724.0658862319\n",
            "\n",
            "Training\n",
            "Step: 17131; Training Loss: 3.0816361904144287; Gradient Norm 0.3388574719429016; Learning Rate: 0.00014547271484069798; Time: 3236.750841140747 ms; Tokens/Sec : 161979.72155781448\n",
            "\n",
            "Training\n",
            "Step: 17132; Training Loss: 3.0315043926239014; Gradient Norm 0.27727797627449036; Learning Rate: 0.00014544044078837394; Time: 3234.529972076416 ms; Tokens/Sec : 162090.93887710423\n",
            "\n",
            "Training\n",
            "Step: 17133; Training Loss: 3.0464088916778564; Gradient Norm 0.3182060122489929; Learning Rate: 0.0001454081775842728; Time: 3242.4654960632324 ms; Tokens/Sec : 161694.24181585052\n",
            "\n",
            "Training\n",
            "Step: 17134; Training Loss: 3.075620174407959; Gradient Norm 0.29523301124572754; Learning Rate: 0.0001453759252292598; Time: 3245.718479156494 ms; Tokens/Sec : 161532.18566764094\n",
            "\n",
            "Training\n",
            "Step: 17135; Training Loss: 3.0080976486206055; Gradient Norm 0.2707098424434662; Learning Rate: 0.00014534368372419985; Time: 3237.5757694244385 ms; Tokens/Sec : 161938.44942606718\n",
            "\n",
            "Training\n",
            "Step: 17136; Training Loss: 2.9856009483337402; Gradient Norm 0.26863202452659607; Learning Rate: 0.00014531145306995738; Time: 3235.879898071289 ms; Tokens/Sec : 162023.31869996045\n",
            "\n",
            "Training\n",
            "Step: 17137; Training Loss: 3.063480854034424; Gradient Norm 0.4668886959552765; Learning Rate: 0.00014527923326739678; Time: 3240.7612800598145 ms; Tokens/Sec : 161779.27181058004\n",
            "\n",
            "Training\n",
            "Step: 17138; Training Loss: 3.0054080486297607; Gradient Norm 0.3424782156944275; Learning Rate: 0.00014524702431738192; Time: 3235.1202964782715 ms; Tokens/Sec : 162061.361542177\n",
            "\n",
            "Training\n",
            "Step: 17139; Training Loss: 2.993706703186035; Gradient Norm 0.33465254306793213; Learning Rate: 0.00014521482622077654; Time: 3242.23256111145 ms; Tokens/Sec : 161705.85857674317\n",
            "\n",
            "Training\n",
            "Step: 17140; Training Loss: 2.9599733352661133; Gradient Norm 0.3342025578022003; Learning Rate: 0.00014518263897844404; Time: 3236.020803451538 ms; Tokens/Sec : 162016.26375232034\n",
            "\n",
            "Training\n",
            "Step: 17141; Training Loss: 3.011305332183838; Gradient Norm 0.33007732033729553; Learning Rate: 0.00014515046259124743; Time: 3239.1421794891357 ms; Tokens/Sec : 161860.13794636473\n",
            "\n",
            "Training\n",
            "Step: 17142; Training Loss: 3.0288591384887695; Gradient Norm 0.3316526710987091; Learning Rate: 0.0001451182970600496; Time: 3236.9892597198486 ms; Tokens/Sec : 161967.79103473932\n",
            "\n",
            "Training\n",
            "Step: 17143; Training Loss: 2.9570610523223877; Gradient Norm 0.3526546359062195; Learning Rate: 0.00014508614238571303; Time: 3236.112594604492 ms; Tokens/Sec : 162011.66822011548\n",
            "\n",
            "Training\n",
            "Step: 17144; Training Loss: 2.9542911052703857; Gradient Norm 0.293211966753006; Learning Rate: 0.00014505399856909993; Time: 3240.640878677368 ms; Tokens/Sec : 161785.2824883152\n",
            "\n",
            "Training\n",
            "Step: 17145; Training Loss: 3.029193162918091; Gradient Norm 0.3168131411075592; Learning Rate: 0.00014502186561107236; Time: 3235.656499862671 ms; Tokens/Sec : 162034.50521470746\n",
            "\n",
            "Training\n",
            "Step: 17146; Training Loss: 3.032057046890259; Gradient Norm 0.33578386902809143; Learning Rate: 0.0001449897435124917; Time: 3240.330457687378 ms; Tokens/Sec : 161800.78138517516\n",
            "\n",
            "Training\n",
            "Step: 17147; Training Loss: 3.014585494995117; Gradient Norm 0.32066014409065247; Learning Rate: 0.00014495763227421955; Time: 3236.685037612915 ms; Tokens/Sec : 161983.01469168195\n",
            "\n",
            "Training\n",
            "Step: 17148; Training Loss: 3.105386972427368; Gradient Norm 0.34470057487487793; Learning Rate: 0.00014492553189711686; Time: 3239.607572555542 ms; Tokens/Sec : 161836.88556648823\n",
            "\n",
            "Training\n",
            "Step: 17149; Training Loss: 3.1285805702209473; Gradient Norm 0.3660568594932556; Learning Rate: 0.00014489344238204449; Time: 3238.865852355957 ms; Tokens/Sec : 161873.9472085983\n",
            "\n",
            "Training\n",
            "Step: 17150; Training Loss: 3.005190134048462; Gradient Norm 0.38881662487983704; Learning Rate: 0.00014486136372986267; Time: 3240.3271198272705 ms; Tokens/Sec : 161800.9480561172\n",
            "\n",
            "Training\n",
            "Step: 17151; Training Loss: 3.059628963470459; Gradient Norm 0.31734347343444824; Learning Rate: 0.00014482929594143183; Time: 3238.5175228118896 ms; Tokens/Sec : 161891.35810041236\n",
            "\n",
            "Training\n",
            "Step: 17152; Training Loss: 3.0364837646484375; Gradient Norm 0.36396554112434387; Learning Rate: 0.0001447972390176119; Time: 3261.4355087280273 ms; Tokens/Sec : 160753.7535532856\n",
            "\n",
            "Training\n",
            "Step: 17153; Training Loss: 3.1358463764190674; Gradient Norm 0.3222236931324005; Learning Rate: 0.00014476519295926232; Time: 3237.6677989959717 ms; Tokens/Sec : 161933.84638244423\n",
            "\n",
            "Training\n",
            "Step: 17154; Training Loss: 3.033998489379883; Gradient Norm 0.36088067293167114; Learning Rate: 0.00014473315776724233; Time: 3242.185354232788 ms; Tokens/Sec : 161708.21304695716\n",
            "\n",
            "Training\n",
            "Step: 17155; Training Loss: 3.0895824432373047; Gradient Norm 0.3664354085922241; Learning Rate: 0.00014470113344241126; Time: 3242.9916858673096 ms; Tokens/Sec : 161668.00620698594\n",
            "\n",
            "Training\n",
            "Step: 17156; Training Loss: 3.160653591156006; Gradient Norm 0.28655344247817993; Learning Rate: 0.0001446691199856276; Time: 3237.238645553589 ms; Tokens/Sec : 161955.3135880544\n",
            "\n",
            "Training\n",
            "Step: 17157; Training Loss: 3.117034912109375; Gradient Norm 0.3220481872558594; Learning Rate: 0.00014463711739774978; Time: 3240.635871887207 ms; Tokens/Sec : 161785.532446963\n",
            "\n",
            "Training\n",
            "Step: 17158; Training Loss: 3.066469430923462; Gradient Norm 0.33132651448249817; Learning Rate: 0.000144605125679636; Time: 3236.2425327301025 ms; Tokens/Sec : 162005.16330205614\n",
            "\n",
            "Training\n",
            "Step: 17159; Training Loss: 3.048551321029663; Gradient Norm 0.3010556697845459; Learning Rate: 0.00014457314483214427; Time: 3237.887144088745 ms; Tokens/Sec : 161922.87645267913\n",
            "\n",
            "Training\n",
            "Step: 17160; Training Loss: 3.091737747192383; Gradient Norm 0.2805420756340027; Learning Rate: 0.00014454117485613192; Time: 3239.0005588531494 ms; Tokens/Sec : 161867.2150478534\n",
            "\n",
            "Training\n",
            "Step: 17161; Training Loss: 3.0125741958618164; Gradient Norm 0.31128889322280884; Learning Rate: 0.0001445092157524563; Time: 3238.753080368042 ms; Tokens/Sec : 161879.58358975037\n",
            "\n",
            "Training\n",
            "Step: 17162; Training Loss: 3.0665061473846436; Gradient Norm 0.3080700635910034; Learning Rate: 0.0001444772675219744; Time: 3243.962049484253 ms; Tokens/Sec : 161619.64659338567\n",
            "\n",
            "Training\n",
            "Step: 17163; Training Loss: 3.040734052658081; Gradient Norm 0.31009307503700256; Learning Rate: 0.0001444453301655429; Time: 3244.311571121216 ms; Tokens/Sec : 161602.23471348314\n",
            "\n",
            "Training\n",
            "Step: 17164; Training Loss: 3.048875331878662; Gradient Norm 0.3121694028377533; Learning Rate: 0.0001444134036840182; Time: 3241.4002418518066 ms; Tokens/Sec : 161747.38103322752\n",
            "\n",
            "Training\n",
            "Step: 17165; Training Loss: 3.0434937477111816; Gradient Norm 0.33424603939056396; Learning Rate: 0.00014438148807825634; Time: 3241.7843341827393 ms; Tokens/Sec : 161728.21691797525\n",
            "\n",
            "Training\n",
            "Step: 17166; Training Loss: 3.0725584030151367; Gradient Norm 0.3081039488315582; Learning Rate: 0.0001443495833491134; Time: 3242.1183586120605 ms; Tokens/Sec : 161711.5546097601\n",
            "\n",
            "Training\n",
            "Step: 17167; Training Loss: 3.0031261444091797; Gradient Norm 0.32754021883010864; Learning Rate: 0.0001443176894974446; Time: 3238.4159564971924 ms; Tokens/Sec : 161896.4354928303\n",
            "\n",
            "Training\n",
            "Step: 17168; Training Loss: 3.018738031387329; Gradient Norm 0.2747022211551666; Learning Rate: 0.00014428580652410525; Time: 3235.833168029785 ms; Tokens/Sec : 162025.65854754043\n",
            "\n",
            "Training\n",
            "Step: 17169; Training Loss: 3.049237012863159; Gradient Norm 0.2908550500869751; Learning Rate: 0.00014425393442995036; Time: 3241.122007369995 ms; Tokens/Sec : 161761.26625527217\n",
            "\n",
            "Training\n",
            "Step: 17170; Training Loss: 3.0703885555267334; Gradient Norm 0.2762484848499298; Learning Rate: 0.00014422207321583455; Time: 3242.5696849823 ms; Tokens/Sec : 161689.0463227969\n",
            "\n",
            "Training\n",
            "Step: 17171; Training Loss: 3.040339946746826; Gradient Norm 0.28630778193473816; Learning Rate: 0.0001441902228826121; Time: 3254.0857791900635 ms; Tokens/Sec : 161116.83452010734\n",
            "\n",
            "Training\n",
            "Step: 17172; Training Loss: 2.9977314472198486; Gradient Norm 0.292493611574173; Learning Rate: 0.0001441583834311372; Time: 3237.466335296631 ms; Tokens/Sec : 161943.92333409775\n",
            "\n",
            "Training\n",
            "Step: 17173; Training Loss: 2.964324951171875; Gradient Norm 0.2757203280925751; Learning Rate: 0.00014412655486226346; Time: 3235.6104850769043 ms; Tokens/Sec : 162036.8095659508\n",
            "\n",
            "Training\n",
            "Step: 17174; Training Loss: 2.9793860912323; Gradient Norm 0.2962650656700134; Learning Rate: 0.00014409473717684446; Time: 3238.798141479492 ms; Tokens/Sec : 161877.33137345317\n",
            "\n",
            "Training\n",
            "Step: 17175; Training Loss: 3.010038375854492; Gradient Norm 0.3106051981449127; Learning Rate: 0.00014406293037573346; Time: 3244.3935871124268 ms; Tokens/Sec : 161598.1495224895\n",
            "\n",
            "Training\n",
            "Step: 17176; Training Loss: 2.994553565979004; Gradient Norm 0.2762741446495056; Learning Rate: 0.00014403113445978324; Time: 3237.478256225586 ms; Tokens/Sec : 161943.3270298597\n",
            "\n",
            "Training\n",
            "Step: 17177; Training Loss: 3.0115418434143066; Gradient Norm 0.28422242403030396; Learning Rate: 0.00014399934942984646; Time: 3239.219903945923 ms; Tokens/Sec : 161856.25414357564\n",
            "\n",
            "Training\n",
            "Step: 17178; Training Loss: 2.9710581302642822; Gradient Norm 0.2709116041660309; Learning Rate: 0.00014396757528677533; Time: 3239.011764526367 ms; Tokens/Sec : 161866.65505263003\n",
            "\n",
            "Training\n",
            "Step: 17179; Training Loss: 2.9924657344818115; Gradient Norm 0.2964016795158386; Learning Rate: 0.00014393581203142212; Time: 3237.435817718506 ms; Tokens/Sec : 161945.44989295807\n",
            "\n",
            "Training\n",
            "Step: 17180; Training Loss: 3.048612117767334; Gradient Norm 0.29196593165397644; Learning Rate: 0.00014390405966463818; Time: 3240.633010864258 ms; Tokens/Sec : 161785.67528082282\n",
            "\n",
            "Training\n",
            "Step: 17181; Training Loss: 2.986740827560425; Gradient Norm 0.27174848318099976; Learning Rate: 0.00014387231818727527; Time: 3235.203266143799 ms; Tokens/Sec : 162057.205334404\n",
            "\n",
            "Training\n",
            "Step: 17182; Training Loss: 2.9900238513946533; Gradient Norm 0.3007816970348358; Learning Rate: 0.0001438405876001845; Time: 3240.342855453491 ms; Tokens/Sec : 161800.1623246825\n",
            "\n",
            "Training\n",
            "Step: 17183; Training Loss: 3.109880208969116; Gradient Norm 0.30685555934906006; Learning Rate: 0.00014380886790421658; Time: 3242.3152923583984 ms; Tokens/Sec : 161701.73247360002\n",
            "\n",
            "Training\n",
            "Step: 17184; Training Loss: 3.1574907302856445; Gradient Norm 0.32151174545288086; Learning Rate: 0.00014377715910022203; Time: 3238.109350204468 ms; Tokens/Sec : 161911.76495225346\n",
            "\n",
            "Training\n",
            "Step: 17185; Training Loss: 3.0330634117126465; Gradient Norm 0.27368563413619995; Learning Rate: 0.0001437454611890513; Time: 3241.405725479126 ms; Tokens/Sec : 161747.10739813442\n",
            "\n",
            "Training\n",
            "Step: 17186; Training Loss: 3.1317026615142822; Gradient Norm 0.3200404644012451; Learning Rate: 0.0001437137741715544; Time: 3241.483449935913 ms; Tokens/Sec : 161743.22901767882\n",
            "\n",
            "Training\n",
            "Step: 17187; Training Loss: 3.1113123893737793; Gradient Norm 0.3214947581291199; Learning Rate: 0.00014368209804858082; Time: 3237.7307415008545 ms; Tokens/Sec : 161930.69833749227\n",
            "\n",
            "Training\n",
            "Step: 17188; Training Loss: 3.0355474948883057; Gradient Norm 0.2929620146751404; Learning Rate: 0.00014365043282097995; Time: 3239.5272254943848 ms; Tokens/Sec : 161840.89946025636\n",
            "\n",
            "Training\n",
            "Step: 17189; Training Loss: 3.0885515213012695; Gradient Norm 0.32884079217910767; Learning Rate: 0.0001436187784896011; Time: 3238.2750511169434 ms; Tokens/Sec : 161903.48000833436\n",
            "\n",
            "Training\n",
            "Step: 17190; Training Loss: 3.042141914367676; Gradient Norm 0.3276044726371765; Learning Rate: 0.00014358713505529294; Time: 3247.1868991851807 ms; Tokens/Sec : 161459.13871836575\n",
            "\n",
            "Training\n",
            "Step: 17191; Training Loss: 3.110039472579956; Gradient Norm 0.2988920211791992; Learning Rate: 0.00014355550251890395; Time: 3239.7398948669434 ms; Tokens/Sec : 161830.2755819021\n",
            "\n",
            "Training\n",
            "Step: 17192; Training Loss: 3.083420991897583; Gradient Norm 0.3124358355998993; Learning Rate: 0.00014352388088128233; Time: 3240.966796875 ms; Tokens/Sec : 161769.01303201506\n",
            "\n",
            "Training\n",
            "Step: 17193; Training Loss: 3.0745034217834473; Gradient Norm 0.29785171151161194; Learning Rate: 0.00014349227014327624; Time: 3237.720251083374 ms; Tokens/Sec : 161931.22300315104\n",
            "\n",
            "Training\n",
            "Step: 17194; Training Loss: 2.9817705154418945; Gradient Norm 0.2992166578769684; Learning Rate: 0.0001434606703057331; Time: 3241.689920425415 ms; Tokens/Sec : 161732.9272292633\n",
            "\n",
            "Training\n",
            "Step: 17195; Training Loss: 3.053605556488037; Gradient Norm 0.33210721611976624; Learning Rate: 0.0001434290813695003; Time: 3239.8011684417725 ms; Tokens/Sec : 161827.2149250948\n",
            "\n",
            "Training\n",
            "Step: 17196; Training Loss: 3.0278122425079346; Gradient Norm 0.2849765121936798; Learning Rate: 0.00014339750333542493; Time: 3240.4496669769287 ms; Tokens/Sec : 161794.82907664395\n",
            "\n",
            "Training\n",
            "Step: 17197; Training Loss: 3.096510887145996; Gradient Norm 0.34052574634552; Learning Rate: 0.0001433659362043537; Time: 3243.732690811157 ms; Tokens/Sec : 161631.074436313\n",
            "\n",
            "Training\n",
            "Step: 17198; Training Loss: 3.022831916809082; Gradient Norm 0.2840869128704071; Learning Rate: 0.00014333437997713318; Time: 3240.2400970458984 ms; Tokens/Sec : 161805.293526856\n",
            "\n",
            "Training\n",
            "Step: 17199; Training Loss: 3.037693500518799; Gradient Norm 0.28793564438819885; Learning Rate: 0.00014330283465460944; Time: 3239.708662033081 ms; Tokens/Sec : 161831.8357277789\n",
            "\n",
            "Training\n",
            "Step: 17200; Training Loss: 3.098724365234375; Gradient Norm 0.27917516231536865; Learning Rate: 0.00014327130023762837; Time: 3239.4771575927734 ms; Tokens/Sec : 161843.40080039142\n",
            "\n",
            "Training\n",
            "Step: 17201; Training Loss: 3.061094045639038; Gradient Norm 0.2854820191860199; Learning Rate: 0.00014323977672703564; Time: 3240.292549133301 ms; Tokens/Sec : 161802.6743110693\n",
            "\n",
            "Training\n",
            "Step: 17202; Training Loss: 3.0146231651306152; Gradient Norm 0.26704657077789307; Learning Rate: 0.00014320826412367652; Time: 3240.720748901367 ms; Tokens/Sec : 161781.29515717708\n",
            "\n",
            "Training\n",
            "Step: 17203; Training Loss: 3.092444658279419; Gradient Norm 0.28739452362060547; Learning Rate: 0.000143176762428396; Time: 3243.328094482422 ms; Tokens/Sec : 161651.23747175728\n",
            "\n",
            "Training\n",
            "Step: 17204; Training Loss: 3.0746407508850098; Gradient Norm 0.28773564100265503; Learning Rate: 0.00014314527164203874; Time: 3240.783452987671 ms; Tokens/Sec : 161778.16494238764\n",
            "\n",
            "Training\n",
            "Step: 17205; Training Loss: 3.0329859256744385; Gradient Norm 0.2549056112766266; Learning Rate: 0.00014311379176544926; Time: 3241.7211532592773 ms; Tokens/Sec : 161731.3689898567\n",
            "\n",
            "Training\n",
            "Step: 17206; Training Loss: 2.994412422180176; Gradient Norm 0.29339492321014404; Learning Rate: 0.00014308232279947177; Time: 3236.1438274383545 ms; Tokens/Sec : 162010.1046049651\n",
            "\n",
            "Training\n",
            "Step: 17207; Training Loss: 3.0105161666870117; Gradient Norm 0.2987706959247589; Learning Rate: 0.0001430508647449498; Time: 3240.638256072998 ms; Tokens/Sec : 161785.41341893916\n",
            "\n",
            "Training\n",
            "Step: 17208; Training Loss: 2.9897074699401855; Gradient Norm 0.2607717514038086; Learning Rate: 0.00014301941760272715; Time: 3239.342451095581 ms; Tokens/Sec : 161850.1309803414\n",
            "\n",
            "Training\n",
            "Step: 17209; Training Loss: 3.0021586418151855; Gradient Norm 0.32936257123947144; Learning Rate: 0.000142987981373647; Time: 3257.38787651062 ms; Tokens/Sec : 160953.5062682274\n",
            "\n",
            "Training\n",
            "Step: 17210; Training Loss: 3.0272715091705322; Gradient Norm 0.26037052273750305; Learning Rate: 0.0001429565560585524; Time: 3240.2660846710205 ms; Tokens/Sec : 161803.99581389013\n",
            "\n",
            "Training\n",
            "Step: 17211; Training Loss: 2.988459587097168; Gradient Norm 0.25907185673713684; Learning Rate: 0.0001429251416582858; Time: 3238.577365875244 ms; Tokens/Sec : 161888.3666403653\n",
            "\n",
            "Training\n",
            "Step: 17212; Training Loss: 2.9713973999023438; Gradient Norm 0.29483169317245483; Learning Rate: 0.00014289373817368977; Time: 3241.1530017852783 ms; Tokens/Sec : 161759.71936875978\n",
            "\n",
            "Training\n",
            "Step: 17213; Training Loss: 3.0177645683288574; Gradient Norm 0.2829947769641876; Learning Rate: 0.00014286234560560636; Time: 3239.6085262298584 ms; Tokens/Sec : 161836.83792502788\n",
            "\n",
            "Training\n",
            "Step: 17214; Training Loss: 2.9856197834014893; Gradient Norm 0.27440592646598816; Learning Rate: 0.00014283096395487728; Time: 3241.9042587280273 ms; Tokens/Sec : 161722.23426663014\n",
            "\n",
            "Training\n",
            "Step: 17215; Training Loss: 2.975080966949463; Gradient Norm 0.30549973249435425; Learning Rate: 0.000142799593222344; Time: 3240.5846118927 ms; Tokens/Sec : 161788.09159183892\n",
            "\n",
            "Training\n",
            "Step: 17216; Training Loss: 3.0250372886657715; Gradient Norm 0.2523530125617981; Learning Rate: 0.00014276823340884795; Time: 3238.114595413208 ms; Tokens/Sec : 161911.50268204047\n",
            "\n",
            "Training\n",
            "Step: 17217; Training Loss: 3.0694832801818848; Gradient Norm 0.3543207347393036; Learning Rate: 0.0001427368845152298; Time: 3242.685556411743 ms; Tokens/Sec : 161683.26866085688\n",
            "\n",
            "Training\n",
            "Step: 17218; Training Loss: 3.0333549976348877; Gradient Norm 0.3792412579059601; Learning Rate: 0.00014270554654233013; Time: 3240.902900695801 ms; Tokens/Sec : 161772.20239688107\n",
            "\n",
            "Training\n",
            "Step: 17219; Training Loss: 3.064405918121338; Gradient Norm 0.3224799931049347; Learning Rate: 0.00014267421949098953; Time: 3238.156795501709 ms; Tokens/Sec : 161909.39262987993\n",
            "\n",
            "Training\n",
            "Step: 17220; Training Loss: 3.0632717609405518; Gradient Norm 0.3112291693687439; Learning Rate: 0.00014264290336204793; Time: 3242.288112640381 ms; Tokens/Sec : 161703.08800011058\n",
            "\n",
            "Training\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = GPT2.load_pretrained_weights()\n",
        "model.to(device)\n",
        "model.eval()\n",
        "\n",
        "print(\"Evaluating HellaSwag with HuggingFace GPT2\")\n",
        "with torch.no_grad():\n",
        "    hls_acc = 0\n",
        "    hls_seen = 0\n",
        "    for hls_idx in tqdm(range(len(hls_inputs))):\n",
        "        # with torch.autocast(device_type = device, dtype = torch.bfloat16):\n",
        "        output, _ = model(hls_inputs[hls_idx].to(device))\n",
        "        hls_targets_device = hls_targets[hls_idx].view(-1).to(device)\n",
        "        hls_loss = F.cross_entropy(output.view(-1, default_vocab_size), hls_targets_device, reduce = False)\n",
        "        hls_loss = hls_loss.masked_fill(hls_targets_device == -100, 0.0)\n",
        "        hls_sums = hls_loss.view(4,-1).sum(dim = - 1)\n",
        "        hls_divisors = torch.count_nonzero(hls_loss.view(4,-1), -1)\n",
        "        hls_means = hls_sums/hls_divisors\n",
        "        if torch.argmin(hls_means) == int(hls_labels[hls_idx]):\n",
        "            hls_acc +=1\n",
        "        hls_seen +=1\n",
        "    hls_acc = hls_acc/hls_seen"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206,
          "referenced_widgets": [
            "7956602f739a428c8420a09977d275e1",
            "54100a0ddcfd4c5a9539732944eeb0cd",
            "cf14a236fc9e47fa9ee8288dc4d398c2",
            "998b643d109d4b30892835e02eb697d6",
            "0531ccba3e42461ab332cdd3a95ec991",
            "c8d011d55c464517871dce55dd1a6f7f",
            "6d8c7974bc2c47f796e39a49b9b9d2e9",
            "b6cd1b6b34604ca7b6fc6cbffeed8db5",
            "918d333ff3a2481d9808db8aaa60ac57",
            "312dd90fa47c41f5a9e0cd27817f948a",
            "fce13f13b48e459995d3c4d0c0d4f03a",
            "6ab84a536dc645a0ba72f85ac3e215a0",
            "2f1ed1d63c1440368a84216334873b43",
            "06d1d1e9e0cd49b4b170ceeb63f6b679",
            "08d06f2fbbef457eb5e9105f95eda81e",
            "1acafbad0f7f4b23b6e47a7c27e0771e",
            "e3cbbd7ce3ab44dcb70d996f0cdd2ab8",
            "972a8fa8f75248b4af4e7424ec7c238c",
            "9c622a6bc98646d792d228fe2f63ca61",
            "7267f5399a4f43fe8b769dabfe382545",
            "668da48e307042e3b4c0ae834b790954",
            "8d58827c33d846ea82684091cd46e3c2",
            "55794d2ba121409aa6f560aa12e8d17b",
            "6ba62013cd294ff89eb5a480114bb3c3",
            "70d718cdcdab41939296aaf07a01b837",
            "f1cb7e7314bc498e981bda3d920eb7df",
            "0f1aaf748cc541a893df4253789307aa",
            "d3924a3feb6b478abbc854328f314014",
            "1547bbc3e1eb44b4808d29e500f6b834",
            "17d4e9d1ff344be0b64e8702f53f64db",
            "40342558bbc3415f8fdfda700b674631",
            "7b90dfb5e6334e9490b19a5fc5e5db8d",
            "19193e10dc044681b48dd1b2ce873e44"
          ]
        },
        "id": "qcut0zdUWUFh",
        "outputId": "8a5cea63-14ff-4cf6-ecb4-72c5ae6cb241"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/665 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "7956602f739a428c8420a09977d275e1"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/548M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "6ab84a536dc645a0ba72f85ac3e215a0"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "generation_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "55794d2ba121409aa6f560aa12e8d17b"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Evaluating HellaSwag with HuggingFace GPT2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  0%|          | 0/10042 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/torch/nn/_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.\n",
            "  warnings.warn(warning.format(ret))\n",
            "100%|██████████| 10042/10042 [05:39<00:00, 29.58it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "hls_acc"
      ],
      "metadata": {
        "id": "tCByoN1gXuH5",
        "outputId": "0b485806-430f-48b9-e3b9-1543519b222e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.2954590718980283"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "config = GPT2Config(new_vocab_size,1024,768,12,12)\n",
        "model = GPT2(config)\n",
        "model_checkpoint = torch.load(f\"/content/gdrive/My Drive/logs/checkpoint_17000.pt\")\n",
        "model_weight_dict = {}\n",
        "\n",
        "for k,v in model_checkpoint[\"model_weights\"].items():\n",
        "  model_weight_dict[k.split(\"_orig_mod.\")[1]] = v\n",
        "\n",
        "model.load_state_dict(model_weight_dict)\n",
        "model.to(device)\n",
        "\n",
        "print(\"Evaluating HellaSwag with custom trained GPT2\")\n",
        "with torch.no_grad():\n",
        "    hls_acc = 0\n",
        "    hls_seen = 0\n",
        "    for hls_idx in tqdm(range(len(hls_inputs))):\n",
        "        # with torch.autocast(device_type = device, dtype = torch.bfloat16):\n",
        "        output, _ = model(hls_inputs[hls_idx].to(device))\n",
        "        hls_targets_device = hls_targets[hls_idx].view(-1).to(device)\n",
        "        hls_loss = F.cross_entropy(output.view(-1, new_vocab_size), hls_targets_device, reduce = False)\n",
        "        hls_loss = hls_loss.masked_fill(hls_targets_device == -100, 0.0)\n",
        "        hls_sums = hls_loss.view(4,-1).sum(dim = - 1)\n",
        "        hls_divisors = torch.count_nonzero(hls_loss.view(4,-1), -1)\n",
        "        hls_means = hls_sums/hls_divisors\n",
        "        if torch.argmin(hls_means) == int(hls_labels[hls_idx]):\n",
        "            hls_acc +=1\n",
        "        hls_seen +=1\n",
        "    hls_acc = hls_acc/hls_seen"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jumkVpPvURqn",
        "outputId": "17e41e92-215d-4abd-f84b-3af7348f0a65"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Evaluating HellaSwag with custom trained GPT2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  0%|          | 0/10042 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/torch/nn/_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.\n",
            "  warnings.warn(warning.format(ret))\n",
            "100%|██████████| 10042/10042 [05:38<00:00, 29.71it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "hls_acc"
      ],
      "metadata": {
        "id": "QqjguKRjZRSc",
        "outputId": "7055138d-7672-4ef5-8488-87a5ec89c733",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.3016331408086039"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "btVy6Tjbb5dz"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}